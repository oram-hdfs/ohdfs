// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: inotify.proto

package org.apache.hadoop.hdfs.protocol.proto;

public final class InotifyProtos {
  private InotifyProtos() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  /**
   * Protobuf enum {@code hadoop.hdfs.EventType}
   */
  public enum EventType
      implements com.google.protobuf.ProtocolMessageEnum {
    /**
     * <code>EVENT_CREATE = 0;</code>
     */
    EVENT_CREATE(0, 0),
    /**
     * <code>EVENT_CLOSE = 1;</code>
     */
    EVENT_CLOSE(1, 1),
    /**
     * <code>EVENT_APPEND = 2;</code>
     */
    EVENT_APPEND(2, 2),
    /**
     * <code>EVENT_RENAME = 3;</code>
     */
    EVENT_RENAME(3, 3),
    /**
     * <code>EVENT_METADATA = 4;</code>
     */
    EVENT_METADATA(4, 4),
    /**
     * <code>EVENT_UNLINK = 5;</code>
     */
    EVENT_UNLINK(5, 5),
    /**
     * <code>EVENT_TRUNCATE = 6;</code>
     */
    EVENT_TRUNCATE(6, 6),
    ;

    /**
     * <code>EVENT_CREATE = 0;</code>
     */
    public static final int EVENT_CREATE_VALUE = 0;
    /**
     * <code>EVENT_CLOSE = 1;</code>
     */
    public static final int EVENT_CLOSE_VALUE = 1;
    /**
     * <code>EVENT_APPEND = 2;</code>
     */
    public static final int EVENT_APPEND_VALUE = 2;
    /**
     * <code>EVENT_RENAME = 3;</code>
     */
    public static final int EVENT_RENAME_VALUE = 3;
    /**
     * <code>EVENT_METADATA = 4;</code>
     */
    public static final int EVENT_METADATA_VALUE = 4;
    /**
     * <code>EVENT_UNLINK = 5;</code>
     */
    public static final int EVENT_UNLINK_VALUE = 5;
    /**
     * <code>EVENT_TRUNCATE = 6;</code>
     */
    public static final int EVENT_TRUNCATE_VALUE = 6;


    public final int getNumber() { return value; }

    public static EventType valueOf(int value) {
      switch (value) {
        case 0: return EVENT_CREATE;
        case 1: return EVENT_CLOSE;
        case 2: return EVENT_APPEND;
        case 3: return EVENT_RENAME;
        case 4: return EVENT_METADATA;
        case 5: return EVENT_UNLINK;
        case 6: return EVENT_TRUNCATE;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<EventType>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static com.google.protobuf.Internal.EnumLiteMap<EventType>
        internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<EventType>() {
            public EventType findValueByNumber(int number) {
              return EventType.valueOf(number);
            }
          };

    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(index);
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.getDescriptor().getEnumTypes().get(0);
    }

    private static final EventType[] VALUES = values();

    public static EventType valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }

    private final int index;
    private final int value;

    private EventType(int index, int value) {
      this.index = index;
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:hadoop.hdfs.EventType)
  }

  /**
   * Protobuf enum {@code hadoop.hdfs.INodeType}
   */
  public enum INodeType
      implements com.google.protobuf.ProtocolMessageEnum {
    /**
     * <code>I_TYPE_FILE = 0;</code>
     */
    I_TYPE_FILE(0, 0),
    /**
     * <code>I_TYPE_DIRECTORY = 1;</code>
     */
    I_TYPE_DIRECTORY(1, 1),
    /**
     * <code>I_TYPE_SYMLINK = 2;</code>
     */
    I_TYPE_SYMLINK(2, 2),
    ;

    /**
     * <code>I_TYPE_FILE = 0;</code>
     */
    public static final int I_TYPE_FILE_VALUE = 0;
    /**
     * <code>I_TYPE_DIRECTORY = 1;</code>
     */
    public static final int I_TYPE_DIRECTORY_VALUE = 1;
    /**
     * <code>I_TYPE_SYMLINK = 2;</code>
     */
    public static final int I_TYPE_SYMLINK_VALUE = 2;


    public final int getNumber() { return value; }

    public static INodeType valueOf(int value) {
      switch (value) {
        case 0: return I_TYPE_FILE;
        case 1: return I_TYPE_DIRECTORY;
        case 2: return I_TYPE_SYMLINK;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<INodeType>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static com.google.protobuf.Internal.EnumLiteMap<INodeType>
        internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<INodeType>() {
            public INodeType findValueByNumber(int number) {
              return INodeType.valueOf(number);
            }
          };

    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(index);
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.getDescriptor().getEnumTypes().get(1);
    }

    private static final INodeType[] VALUES = values();

    public static INodeType valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }

    private final int index;
    private final int value;

    private INodeType(int index, int value) {
      this.index = index;
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:hadoop.hdfs.INodeType)
  }

  /**
   * Protobuf enum {@code hadoop.hdfs.MetadataUpdateType}
   */
  public enum MetadataUpdateType
      implements com.google.protobuf.ProtocolMessageEnum {
    /**
     * <code>META_TYPE_TIMES = 0;</code>
     */
    META_TYPE_TIMES(0, 0),
    /**
     * <code>META_TYPE_REPLICATION = 1;</code>
     */
    META_TYPE_REPLICATION(1, 1),
    /**
     * <code>META_TYPE_OWNER = 2;</code>
     */
    META_TYPE_OWNER(2, 2),
    /**
     * <code>META_TYPE_PERMS = 3;</code>
     */
    META_TYPE_PERMS(3, 3),
    /**
     * <code>META_TYPE_ACLS = 4;</code>
     */
    META_TYPE_ACLS(4, 4),
    /**
     * <code>META_TYPE_XATTRS = 5;</code>
     */
    META_TYPE_XATTRS(5, 5),
    ;

    /**
     * <code>META_TYPE_TIMES = 0;</code>
     */
    public static final int META_TYPE_TIMES_VALUE = 0;
    /**
     * <code>META_TYPE_REPLICATION = 1;</code>
     */
    public static final int META_TYPE_REPLICATION_VALUE = 1;
    /**
     * <code>META_TYPE_OWNER = 2;</code>
     */
    public static final int META_TYPE_OWNER_VALUE = 2;
    /**
     * <code>META_TYPE_PERMS = 3;</code>
     */
    public static final int META_TYPE_PERMS_VALUE = 3;
    /**
     * <code>META_TYPE_ACLS = 4;</code>
     */
    public static final int META_TYPE_ACLS_VALUE = 4;
    /**
     * <code>META_TYPE_XATTRS = 5;</code>
     */
    public static final int META_TYPE_XATTRS_VALUE = 5;


    public final int getNumber() { return value; }

    public static MetadataUpdateType valueOf(int value) {
      switch (value) {
        case 0: return META_TYPE_TIMES;
        case 1: return META_TYPE_REPLICATION;
        case 2: return META_TYPE_OWNER;
        case 3: return META_TYPE_PERMS;
        case 4: return META_TYPE_ACLS;
        case 5: return META_TYPE_XATTRS;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<MetadataUpdateType>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static com.google.protobuf.Internal.EnumLiteMap<MetadataUpdateType>
        internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<MetadataUpdateType>() {
            public MetadataUpdateType findValueByNumber(int number) {
              return MetadataUpdateType.valueOf(number);
            }
          };

    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(index);
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.getDescriptor().getEnumTypes().get(2);
    }

    private static final MetadataUpdateType[] VALUES = values();

    public static MetadataUpdateType valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }

    private final int index;
    private final int value;

    private MetadataUpdateType(int index, int value) {
      this.index = index;
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:hadoop.hdfs.MetadataUpdateType)
  }

  public interface EventProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required .hadoop.hdfs.EventType type = 1;
    /**
     * <code>required .hadoop.hdfs.EventType type = 1;</code>
     */
    boolean hasType();
    /**
     * <code>required .hadoop.hdfs.EventType type = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventType getType();

    // required bytes contents = 2;
    /**
     * <code>required bytes contents = 2;</code>
     */
    boolean hasContents();
    /**
     * <code>required bytes contents = 2;</code>
     */
    com.google.protobuf.ByteString getContents();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.EventProto}
   */
  public static final class EventProto extends
      com.google.protobuf.GeneratedMessage
      implements EventProtoOrBuilder {
    // Use EventProto.newBuilder() to construct.
    private EventProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private EventProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final EventProto defaultInstance;
    public static EventProto getDefaultInstance() {
      return defaultInstance;
    }

    public EventProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private EventProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              int rawValue = input.readEnum();
              org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventType value = org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventType.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(1, rawValue);
              } else {
                bitField0_ |= 0x00000001;
                type_ = value;
              }
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              contents_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_EventProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_EventProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.class, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.Builder.class);
    }

    public static com.google.protobuf.Parser<EventProto> PARSER =
        new com.google.protobuf.AbstractParser<EventProto>() {
      public EventProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new EventProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<EventProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required .hadoop.hdfs.EventType type = 1;
    public static final int TYPE_FIELD_NUMBER = 1;
    private org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventType type_;
    /**
     * <code>required .hadoop.hdfs.EventType type = 1;</code>
     */
    public boolean hasType() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required .hadoop.hdfs.EventType type = 1;</code>
     */
    public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventType getType() {
      return type_;
    }

    // required bytes contents = 2;
    public static final int CONTENTS_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString contents_;
    /**
     * <code>required bytes contents = 2;</code>
     */
    public boolean hasContents() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required bytes contents = 2;</code>
     */
    public com.google.protobuf.ByteString getContents() {
      return contents_;
    }

    private void initFields() {
      type_ = org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventType.EVENT_CREATE;
      contents_ = com.google.protobuf.ByteString.EMPTY;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasType()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasContents()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeEnum(1, type_.getNumber());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, contents_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(1, type_.getNumber());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, contents_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto other = (org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto) obj;

      boolean result = true;
      result = result && (hasType() == other.hasType());
      if (hasType()) {
        result = result &&
            (getType() == other.getType());
      }
      result = result && (hasContents() == other.hasContents());
      if (hasContents()) {
        result = result && getContents()
            .equals(other.getContents());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasType()) {
        hash = (37 * hash) + TYPE_FIELD_NUMBER;
        hash = (53 * hash) + hashEnum(getType());
      }
      if (hasContents()) {
        hash = (37 * hash) + CONTENTS_FIELD_NUMBER;
        hash = (53 * hash) + getContents().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.EventProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_EventProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_EventProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.class, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        type_ = org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventType.EVENT_CREATE;
        bitField0_ = (bitField0_ & ~0x00000001);
        contents_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_EventProto_descriptor;
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto build() {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto result = new org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.type_ = type_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.contents_ = contents_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.getDefaultInstance()) return this;
        if (other.hasType()) {
          setType(other.getType());
        }
        if (other.hasContents()) {
          setContents(other.getContents());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasType()) {
          
          return false;
        }
        if (!hasContents()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required .hadoop.hdfs.EventType type = 1;
      private org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventType type_ = org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventType.EVENT_CREATE;
      /**
       * <code>required .hadoop.hdfs.EventType type = 1;</code>
       */
      public boolean hasType() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required .hadoop.hdfs.EventType type = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventType getType() {
        return type_;
      }
      /**
       * <code>required .hadoop.hdfs.EventType type = 1;</code>
       */
      public Builder setType(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventType value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000001;
        type_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required .hadoop.hdfs.EventType type = 1;</code>
       */
      public Builder clearType() {
        bitField0_ = (bitField0_ & ~0x00000001);
        type_ = org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventType.EVENT_CREATE;
        onChanged();
        return this;
      }

      // required bytes contents = 2;
      private com.google.protobuf.ByteString contents_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>required bytes contents = 2;</code>
       */
      public boolean hasContents() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required bytes contents = 2;</code>
       */
      public com.google.protobuf.ByteString getContents() {
        return contents_;
      }
      /**
       * <code>required bytes contents = 2;</code>
       */
      public Builder setContents(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        contents_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required bytes contents = 2;</code>
       */
      public Builder clearContents() {
        bitField0_ = (bitField0_ & ~0x00000002);
        contents_ = getDefaultInstance().getContents();
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.EventProto)
    }

    static {
      defaultInstance = new EventProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.EventProto)
  }

  public interface EventBatchProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required int64 txid = 1;
    /**
     * <code>required int64 txid = 1;</code>
     */
    boolean hasTxid();
    /**
     * <code>required int64 txid = 1;</code>
     */
    long getTxid();

    // repeated .hadoop.hdfs.EventProto events = 2;
    /**
     * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto> 
        getEventsList();
    /**
     * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
     */
    org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto getEvents(int index);
    /**
     * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
     */
    int getEventsCount();
    /**
     * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProtoOrBuilder> 
        getEventsOrBuilderList();
    /**
     * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
     */
    org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProtoOrBuilder getEventsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.hdfs.EventBatchProto}
   */
  public static final class EventBatchProto extends
      com.google.protobuf.GeneratedMessage
      implements EventBatchProtoOrBuilder {
    // Use EventBatchProto.newBuilder() to construct.
    private EventBatchProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private EventBatchProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final EventBatchProto defaultInstance;
    public static EventBatchProto getDefaultInstance() {
      return defaultInstance;
    }

    public EventBatchProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private EventBatchProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              txid_ = input.readInt64();
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                events_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto>();
                mutable_bitField0_ |= 0x00000002;
              }
              events_.add(input.readMessage(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
          events_ = java.util.Collections.unmodifiableList(events_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_EventBatchProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_EventBatchProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto.class, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto.Builder.class);
    }

    public static com.google.protobuf.Parser<EventBatchProto> PARSER =
        new com.google.protobuf.AbstractParser<EventBatchProto>() {
      public EventBatchProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new EventBatchProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<EventBatchProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required int64 txid = 1;
    public static final int TXID_FIELD_NUMBER = 1;
    private long txid_;
    /**
     * <code>required int64 txid = 1;</code>
     */
    public boolean hasTxid() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required int64 txid = 1;</code>
     */
    public long getTxid() {
      return txid_;
    }

    // repeated .hadoop.hdfs.EventProto events = 2;
    public static final int EVENTS_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto> events_;
    /**
     * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
     */
    public java.util.List<org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto> getEventsList() {
      return events_;
    }
    /**
     * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProtoOrBuilder> 
        getEventsOrBuilderList() {
      return events_;
    }
    /**
     * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
     */
    public int getEventsCount() {
      return events_.size();
    }
    /**
     * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
     */
    public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto getEvents(int index) {
      return events_.get(index);
    }
    /**
     * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
     */
    public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProtoOrBuilder getEventsOrBuilder(
        int index) {
      return events_.get(index);
    }

    private void initFields() {
      txid_ = 0L;
      events_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasTxid()) {
        memoizedIsInitialized = 0;
        return false;
      }
      for (int i = 0; i < getEventsCount(); i++) {
        if (!getEvents(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeInt64(1, txid_);
      }
      for (int i = 0; i < events_.size(); i++) {
        output.writeMessage(2, events_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, txid_);
      }
      for (int i = 0; i < events_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, events_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto other = (org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto) obj;

      boolean result = true;
      result = result && (hasTxid() == other.hasTxid());
      if (hasTxid()) {
        result = result && (getTxid()
            == other.getTxid());
      }
      result = result && getEventsList()
          .equals(other.getEventsList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasTxid()) {
        hash = (37 * hash) + TXID_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getTxid());
      }
      if (getEventsCount() > 0) {
        hash = (37 * hash) + EVENTS_FIELD_NUMBER;
        hash = (53 * hash) + getEventsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.EventBatchProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_EventBatchProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_EventBatchProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto.class, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getEventsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        txid_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000001);
        if (eventsBuilder_ == null) {
          events_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          eventsBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_EventBatchProto_descriptor;
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto build() {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto result = new org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.txid_ = txid_;
        if (eventsBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002)) {
            events_ = java.util.Collections.unmodifiableList(events_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.events_ = events_;
        } else {
          result.events_ = eventsBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto.getDefaultInstance()) return this;
        if (other.hasTxid()) {
          setTxid(other.getTxid());
        }
        if (eventsBuilder_ == null) {
          if (!other.events_.isEmpty()) {
            if (events_.isEmpty()) {
              events_ = other.events_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureEventsIsMutable();
              events_.addAll(other.events_);
            }
            onChanged();
          }
        } else {
          if (!other.events_.isEmpty()) {
            if (eventsBuilder_.isEmpty()) {
              eventsBuilder_.dispose();
              eventsBuilder_ = null;
              events_ = other.events_;
              bitField0_ = (bitField0_ & ~0x00000002);
              eventsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getEventsFieldBuilder() : null;
            } else {
              eventsBuilder_.addAllMessages(other.events_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasTxid()) {
          
          return false;
        }
        for (int i = 0; i < getEventsCount(); i++) {
          if (!getEvents(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required int64 txid = 1;
      private long txid_ ;
      /**
       * <code>required int64 txid = 1;</code>
       */
      public boolean hasTxid() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required int64 txid = 1;</code>
       */
      public long getTxid() {
        return txid_;
      }
      /**
       * <code>required int64 txid = 1;</code>
       */
      public Builder setTxid(long value) {
        bitField0_ |= 0x00000001;
        txid_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required int64 txid = 1;</code>
       */
      public Builder clearTxid() {
        bitField0_ = (bitField0_ & ~0x00000001);
        txid_ = 0L;
        onChanged();
        return this;
      }

      // repeated .hadoop.hdfs.EventProto events = 2;
      private java.util.List<org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto> events_ =
        java.util.Collections.emptyList();
      private void ensureEventsIsMutable() {
        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
          events_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto>(events_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.Builder, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProtoOrBuilder> eventsBuilder_;

      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto> getEventsList() {
        if (eventsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(events_);
        } else {
          return eventsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
       */
      public int getEventsCount() {
        if (eventsBuilder_ == null) {
          return events_.size();
        } else {
          return eventsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto getEvents(int index) {
        if (eventsBuilder_ == null) {
          return events_.get(index);
        } else {
          return eventsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
       */
      public Builder setEvents(
          int index, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto value) {
        if (eventsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureEventsIsMutable();
          events_.set(index, value);
          onChanged();
        } else {
          eventsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
       */
      public Builder setEvents(
          int index, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.Builder builderForValue) {
        if (eventsBuilder_ == null) {
          ensureEventsIsMutable();
          events_.set(index, builderForValue.build());
          onChanged();
        } else {
          eventsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
       */
      public Builder addEvents(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto value) {
        if (eventsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureEventsIsMutable();
          events_.add(value);
          onChanged();
        } else {
          eventsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
       */
      public Builder addEvents(
          int index, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto value) {
        if (eventsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureEventsIsMutable();
          events_.add(index, value);
          onChanged();
        } else {
          eventsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
       */
      public Builder addEvents(
          org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.Builder builderForValue) {
        if (eventsBuilder_ == null) {
          ensureEventsIsMutable();
          events_.add(builderForValue.build());
          onChanged();
        } else {
          eventsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
       */
      public Builder addEvents(
          int index, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.Builder builderForValue) {
        if (eventsBuilder_ == null) {
          ensureEventsIsMutable();
          events_.add(index, builderForValue.build());
          onChanged();
        } else {
          eventsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
       */
      public Builder addAllEvents(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto> values) {
        if (eventsBuilder_ == null) {
          ensureEventsIsMutable();
          super.addAll(values, events_);
          onChanged();
        } else {
          eventsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
       */
      public Builder clearEvents() {
        if (eventsBuilder_ == null) {
          events_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          eventsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
       */
      public Builder removeEvents(int index) {
        if (eventsBuilder_ == null) {
          ensureEventsIsMutable();
          events_.remove(index);
          onChanged();
        } else {
          eventsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.Builder getEventsBuilder(
          int index) {
        return getEventsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProtoOrBuilder getEventsOrBuilder(
          int index) {
        if (eventsBuilder_ == null) {
          return events_.get(index);  } else {
          return eventsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProtoOrBuilder> 
           getEventsOrBuilderList() {
        if (eventsBuilder_ != null) {
          return eventsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(events_);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.Builder addEventsBuilder() {
        return getEventsFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.Builder addEventsBuilder(
          int index) {
        return getEventsFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 2;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.Builder> 
           getEventsBuilderList() {
        return getEventsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.Builder, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProtoOrBuilder> 
          getEventsFieldBuilder() {
        if (eventsBuilder_ == null) {
          eventsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.Builder, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProtoOrBuilder>(
                  events_,
                  ((bitField0_ & 0x00000002) == 0x00000002),
                  getParentForChildren(),
                  isClean());
          events_ = null;
        }
        return eventsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.EventBatchProto)
    }

    static {
      defaultInstance = new EventBatchProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.EventBatchProto)
  }

  public interface CreateEventProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required .hadoop.hdfs.INodeType type = 1;
    /**
     * <code>required .hadoop.hdfs.INodeType type = 1;</code>
     */
    boolean hasType();
    /**
     * <code>required .hadoop.hdfs.INodeType type = 1;</code>
     */
    org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.INodeType getType();

    // required string path = 2;
    /**
     * <code>required string path = 2;</code>
     */
    boolean hasPath();
    /**
     * <code>required string path = 2;</code>
     */
    java.lang.String getPath();
    /**
     * <code>required string path = 2;</code>
     */
    com.google.protobuf.ByteString
        getPathBytes();

    // required int64 ctime = 3;
    /**
     * <code>required int64 ctime = 3;</code>
     */
    boolean hasCtime();
    /**
     * <code>required int64 ctime = 3;</code>
     */
    long getCtime();

    // required string ownerName = 4;
    /**
     * <code>required string ownerName = 4;</code>
     */
    boolean hasOwnerName();
    /**
     * <code>required string ownerName = 4;</code>
     */
    java.lang.String getOwnerName();
    /**
     * <code>required string ownerName = 4;</code>
     */
    com.google.protobuf.ByteString
        getOwnerNameBytes();

    // required string groupName = 5;
    /**
     * <code>required string groupName = 5;</code>
     */
    boolean hasGroupName();
    /**
     * <code>required string groupName = 5;</code>
     */
    java.lang.String getGroupName();
    /**
     * <code>required string groupName = 5;</code>
     */
    com.google.protobuf.ByteString
        getGroupNameBytes();

    // required .hadoop.hdfs.FsPermissionProto perms = 6;
    /**
     * <code>required .hadoop.hdfs.FsPermissionProto perms = 6;</code>
     */
    boolean hasPerms();
    /**
     * <code>required .hadoop.hdfs.FsPermissionProto perms = 6;</code>
     */
    org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto getPerms();
    /**
     * <code>required .hadoop.hdfs.FsPermissionProto perms = 6;</code>
     */
    org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProtoOrBuilder getPermsOrBuilder();

    // optional int32 replication = 7;
    /**
     * <code>optional int32 replication = 7;</code>
     */
    boolean hasReplication();
    /**
     * <code>optional int32 replication = 7;</code>
     */
    int getReplication();

    // optional string symlinkTarget = 8;
    /**
     * <code>optional string symlinkTarget = 8;</code>
     */
    boolean hasSymlinkTarget();
    /**
     * <code>optional string symlinkTarget = 8;</code>
     */
    java.lang.String getSymlinkTarget();
    /**
     * <code>optional string symlinkTarget = 8;</code>
     */
    com.google.protobuf.ByteString
        getSymlinkTargetBytes();

    // optional bool overwrite = 9;
    /**
     * <code>optional bool overwrite = 9;</code>
     */
    boolean hasOverwrite();
    /**
     * <code>optional bool overwrite = 9;</code>
     */
    boolean getOverwrite();

    // optional int64 defaultBlockSize = 10 [default = 0];
    /**
     * <code>optional int64 defaultBlockSize = 10 [default = 0];</code>
     */
    boolean hasDefaultBlockSize();
    /**
     * <code>optional int64 defaultBlockSize = 10 [default = 0];</code>
     */
    long getDefaultBlockSize();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.CreateEventProto}
   */
  public static final class CreateEventProto extends
      com.google.protobuf.GeneratedMessage
      implements CreateEventProtoOrBuilder {
    // Use CreateEventProto.newBuilder() to construct.
    private CreateEventProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private CreateEventProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final CreateEventProto defaultInstance;
    public static CreateEventProto getDefaultInstance() {
      return defaultInstance;
    }

    public CreateEventProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private CreateEventProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              int rawValue = input.readEnum();
              org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.INodeType value = org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.INodeType.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(1, rawValue);
              } else {
                bitField0_ |= 0x00000001;
                type_ = value;
              }
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              path_ = input.readBytes();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              ctime_ = input.readInt64();
              break;
            }
            case 34: {
              bitField0_ |= 0x00000008;
              ownerName_ = input.readBytes();
              break;
            }
            case 42: {
              bitField0_ |= 0x00000010;
              groupName_ = input.readBytes();
              break;
            }
            case 50: {
              org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000020) == 0x00000020)) {
                subBuilder = perms_.toBuilder();
              }
              perms_ = input.readMessage(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(perms_);
                perms_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000020;
              break;
            }
            case 56: {
              bitField0_ |= 0x00000040;
              replication_ = input.readInt32();
              break;
            }
            case 66: {
              bitField0_ |= 0x00000080;
              symlinkTarget_ = input.readBytes();
              break;
            }
            case 72: {
              bitField0_ |= 0x00000100;
              overwrite_ = input.readBool();
              break;
            }
            case 80: {
              bitField0_ |= 0x00000200;
              defaultBlockSize_ = input.readInt64();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_CreateEventProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_CreateEventProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto.class, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto.Builder.class);
    }

    public static com.google.protobuf.Parser<CreateEventProto> PARSER =
        new com.google.protobuf.AbstractParser<CreateEventProto>() {
      public CreateEventProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new CreateEventProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<CreateEventProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required .hadoop.hdfs.INodeType type = 1;
    public static final int TYPE_FIELD_NUMBER = 1;
    private org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.INodeType type_;
    /**
     * <code>required .hadoop.hdfs.INodeType type = 1;</code>
     */
    public boolean hasType() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required .hadoop.hdfs.INodeType type = 1;</code>
     */
    public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.INodeType getType() {
      return type_;
    }

    // required string path = 2;
    public static final int PATH_FIELD_NUMBER = 2;
    private java.lang.Object path_;
    /**
     * <code>required string path = 2;</code>
     */
    public boolean hasPath() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required string path = 2;</code>
     */
    public java.lang.String getPath() {
      java.lang.Object ref = path_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          path_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string path = 2;</code>
     */
    public com.google.protobuf.ByteString
        getPathBytes() {
      java.lang.Object ref = path_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        path_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // required int64 ctime = 3;
    public static final int CTIME_FIELD_NUMBER = 3;
    private long ctime_;
    /**
     * <code>required int64 ctime = 3;</code>
     */
    public boolean hasCtime() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>required int64 ctime = 3;</code>
     */
    public long getCtime() {
      return ctime_;
    }

    // required string ownerName = 4;
    public static final int OWNERNAME_FIELD_NUMBER = 4;
    private java.lang.Object ownerName_;
    /**
     * <code>required string ownerName = 4;</code>
     */
    public boolean hasOwnerName() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>required string ownerName = 4;</code>
     */
    public java.lang.String getOwnerName() {
      java.lang.Object ref = ownerName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          ownerName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string ownerName = 4;</code>
     */
    public com.google.protobuf.ByteString
        getOwnerNameBytes() {
      java.lang.Object ref = ownerName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        ownerName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // required string groupName = 5;
    public static final int GROUPNAME_FIELD_NUMBER = 5;
    private java.lang.Object groupName_;
    /**
     * <code>required string groupName = 5;</code>
     */
    public boolean hasGroupName() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>required string groupName = 5;</code>
     */
    public java.lang.String getGroupName() {
      java.lang.Object ref = groupName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          groupName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string groupName = 5;</code>
     */
    public com.google.protobuf.ByteString
        getGroupNameBytes() {
      java.lang.Object ref = groupName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        groupName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // required .hadoop.hdfs.FsPermissionProto perms = 6;
    public static final int PERMS_FIELD_NUMBER = 6;
    private org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto perms_;
    /**
     * <code>required .hadoop.hdfs.FsPermissionProto perms = 6;</code>
     */
    public boolean hasPerms() {
      return ((bitField0_ & 0x00000020) == 0x00000020);
    }
    /**
     * <code>required .hadoop.hdfs.FsPermissionProto perms = 6;</code>
     */
    public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto getPerms() {
      return perms_;
    }
    /**
     * <code>required .hadoop.hdfs.FsPermissionProto perms = 6;</code>
     */
    public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProtoOrBuilder getPermsOrBuilder() {
      return perms_;
    }

    // optional int32 replication = 7;
    public static final int REPLICATION_FIELD_NUMBER = 7;
    private int replication_;
    /**
     * <code>optional int32 replication = 7;</code>
     */
    public boolean hasReplication() {
      return ((bitField0_ & 0x00000040) == 0x00000040);
    }
    /**
     * <code>optional int32 replication = 7;</code>
     */
    public int getReplication() {
      return replication_;
    }

    // optional string symlinkTarget = 8;
    public static final int SYMLINKTARGET_FIELD_NUMBER = 8;
    private java.lang.Object symlinkTarget_;
    /**
     * <code>optional string symlinkTarget = 8;</code>
     */
    public boolean hasSymlinkTarget() {
      return ((bitField0_ & 0x00000080) == 0x00000080);
    }
    /**
     * <code>optional string symlinkTarget = 8;</code>
     */
    public java.lang.String getSymlinkTarget() {
      java.lang.Object ref = symlinkTarget_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          symlinkTarget_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string symlinkTarget = 8;</code>
     */
    public com.google.protobuf.ByteString
        getSymlinkTargetBytes() {
      java.lang.Object ref = symlinkTarget_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        symlinkTarget_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional bool overwrite = 9;
    public static final int OVERWRITE_FIELD_NUMBER = 9;
    private boolean overwrite_;
    /**
     * <code>optional bool overwrite = 9;</code>
     */
    public boolean hasOverwrite() {
      return ((bitField0_ & 0x00000100) == 0x00000100);
    }
    /**
     * <code>optional bool overwrite = 9;</code>
     */
    public boolean getOverwrite() {
      return overwrite_;
    }

    // optional int64 defaultBlockSize = 10 [default = 0];
    public static final int DEFAULTBLOCKSIZE_FIELD_NUMBER = 10;
    private long defaultBlockSize_;
    /**
     * <code>optional int64 defaultBlockSize = 10 [default = 0];</code>
     */
    public boolean hasDefaultBlockSize() {
      return ((bitField0_ & 0x00000200) == 0x00000200);
    }
    /**
     * <code>optional int64 defaultBlockSize = 10 [default = 0];</code>
     */
    public long getDefaultBlockSize() {
      return defaultBlockSize_;
    }

    private void initFields() {
      type_ = org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.INodeType.I_TYPE_FILE;
      path_ = "";
      ctime_ = 0L;
      ownerName_ = "";
      groupName_ = "";
      perms_ = org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.getDefaultInstance();
      replication_ = 0;
      symlinkTarget_ = "";
      overwrite_ = false;
      defaultBlockSize_ = 0L;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasType()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasPath()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasCtime()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasOwnerName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasGroupName()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasPerms()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getPerms().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeEnum(1, type_.getNumber());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, getPathBytes());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeInt64(3, ctime_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeBytes(4, getOwnerNameBytes());
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeBytes(5, getGroupNameBytes());
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        output.writeMessage(6, perms_);
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        output.writeInt32(7, replication_);
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        output.writeBytes(8, getSymlinkTargetBytes());
      }
      if (((bitField0_ & 0x00000100) == 0x00000100)) {
        output.writeBool(9, overwrite_);
      }
      if (((bitField0_ & 0x00000200) == 0x00000200)) {
        output.writeInt64(10, defaultBlockSize_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(1, type_.getNumber());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, getPathBytes());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(3, ctime_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(4, getOwnerNameBytes());
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(5, getGroupNameBytes());
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, perms_);
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(7, replication_);
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(8, getSymlinkTargetBytes());
      }
      if (((bitField0_ & 0x00000100) == 0x00000100)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(9, overwrite_);
      }
      if (((bitField0_ & 0x00000200) == 0x00000200)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(10, defaultBlockSize_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto other = (org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto) obj;

      boolean result = true;
      result = result && (hasType() == other.hasType());
      if (hasType()) {
        result = result &&
            (getType() == other.getType());
      }
      result = result && (hasPath() == other.hasPath());
      if (hasPath()) {
        result = result && getPath()
            .equals(other.getPath());
      }
      result = result && (hasCtime() == other.hasCtime());
      if (hasCtime()) {
        result = result && (getCtime()
            == other.getCtime());
      }
      result = result && (hasOwnerName() == other.hasOwnerName());
      if (hasOwnerName()) {
        result = result && getOwnerName()
            .equals(other.getOwnerName());
      }
      result = result && (hasGroupName() == other.hasGroupName());
      if (hasGroupName()) {
        result = result && getGroupName()
            .equals(other.getGroupName());
      }
      result = result && (hasPerms() == other.hasPerms());
      if (hasPerms()) {
        result = result && getPerms()
            .equals(other.getPerms());
      }
      result = result && (hasReplication() == other.hasReplication());
      if (hasReplication()) {
        result = result && (getReplication()
            == other.getReplication());
      }
      result = result && (hasSymlinkTarget() == other.hasSymlinkTarget());
      if (hasSymlinkTarget()) {
        result = result && getSymlinkTarget()
            .equals(other.getSymlinkTarget());
      }
      result = result && (hasOverwrite() == other.hasOverwrite());
      if (hasOverwrite()) {
        result = result && (getOverwrite()
            == other.getOverwrite());
      }
      result = result && (hasDefaultBlockSize() == other.hasDefaultBlockSize());
      if (hasDefaultBlockSize()) {
        result = result && (getDefaultBlockSize()
            == other.getDefaultBlockSize());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasType()) {
        hash = (37 * hash) + TYPE_FIELD_NUMBER;
        hash = (53 * hash) + hashEnum(getType());
      }
      if (hasPath()) {
        hash = (37 * hash) + PATH_FIELD_NUMBER;
        hash = (53 * hash) + getPath().hashCode();
      }
      if (hasCtime()) {
        hash = (37 * hash) + CTIME_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getCtime());
      }
      if (hasOwnerName()) {
        hash = (37 * hash) + OWNERNAME_FIELD_NUMBER;
        hash = (53 * hash) + getOwnerName().hashCode();
      }
      if (hasGroupName()) {
        hash = (37 * hash) + GROUPNAME_FIELD_NUMBER;
        hash = (53 * hash) + getGroupName().hashCode();
      }
      if (hasPerms()) {
        hash = (37 * hash) + PERMS_FIELD_NUMBER;
        hash = (53 * hash) + getPerms().hashCode();
      }
      if (hasReplication()) {
        hash = (37 * hash) + REPLICATION_FIELD_NUMBER;
        hash = (53 * hash) + getReplication();
      }
      if (hasSymlinkTarget()) {
        hash = (37 * hash) + SYMLINKTARGET_FIELD_NUMBER;
        hash = (53 * hash) + getSymlinkTarget().hashCode();
      }
      if (hasOverwrite()) {
        hash = (37 * hash) + OVERWRITE_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getOverwrite());
      }
      if (hasDefaultBlockSize()) {
        hash = (37 * hash) + DEFAULTBLOCKSIZE_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getDefaultBlockSize());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.CreateEventProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_CreateEventProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_CreateEventProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto.class, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getPermsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        type_ = org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.INodeType.I_TYPE_FILE;
        bitField0_ = (bitField0_ & ~0x00000001);
        path_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        ctime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        ownerName_ = "";
        bitField0_ = (bitField0_ & ~0x00000008);
        groupName_ = "";
        bitField0_ = (bitField0_ & ~0x00000010);
        if (permsBuilder_ == null) {
          perms_ = org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.getDefaultInstance();
        } else {
          permsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000020);
        replication_ = 0;
        bitField0_ = (bitField0_ & ~0x00000040);
        symlinkTarget_ = "";
        bitField0_ = (bitField0_ & ~0x00000080);
        overwrite_ = false;
        bitField0_ = (bitField0_ & ~0x00000100);
        defaultBlockSize_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000200);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_CreateEventProto_descriptor;
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto build() {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto result = new org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.type_ = type_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.path_ = path_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.ctime_ = ctime_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.ownerName_ = ownerName_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000010;
        }
        result.groupName_ = groupName_;
        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
          to_bitField0_ |= 0x00000020;
        }
        if (permsBuilder_ == null) {
          result.perms_ = perms_;
        } else {
          result.perms_ = permsBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000040;
        }
        result.replication_ = replication_;
        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
          to_bitField0_ |= 0x00000080;
        }
        result.symlinkTarget_ = symlinkTarget_;
        if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
          to_bitField0_ |= 0x00000100;
        }
        result.overwrite_ = overwrite_;
        if (((from_bitField0_ & 0x00000200) == 0x00000200)) {
          to_bitField0_ |= 0x00000200;
        }
        result.defaultBlockSize_ = defaultBlockSize_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto.getDefaultInstance()) return this;
        if (other.hasType()) {
          setType(other.getType());
        }
        if (other.hasPath()) {
          bitField0_ |= 0x00000002;
          path_ = other.path_;
          onChanged();
        }
        if (other.hasCtime()) {
          setCtime(other.getCtime());
        }
        if (other.hasOwnerName()) {
          bitField0_ |= 0x00000008;
          ownerName_ = other.ownerName_;
          onChanged();
        }
        if (other.hasGroupName()) {
          bitField0_ |= 0x00000010;
          groupName_ = other.groupName_;
          onChanged();
        }
        if (other.hasPerms()) {
          mergePerms(other.getPerms());
        }
        if (other.hasReplication()) {
          setReplication(other.getReplication());
        }
        if (other.hasSymlinkTarget()) {
          bitField0_ |= 0x00000080;
          symlinkTarget_ = other.symlinkTarget_;
          onChanged();
        }
        if (other.hasOverwrite()) {
          setOverwrite(other.getOverwrite());
        }
        if (other.hasDefaultBlockSize()) {
          setDefaultBlockSize(other.getDefaultBlockSize());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasType()) {
          
          return false;
        }
        if (!hasPath()) {
          
          return false;
        }
        if (!hasCtime()) {
          
          return false;
        }
        if (!hasOwnerName()) {
          
          return false;
        }
        if (!hasGroupName()) {
          
          return false;
        }
        if (!hasPerms()) {
          
          return false;
        }
        if (!getPerms().isInitialized()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CreateEventProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required .hadoop.hdfs.INodeType type = 1;
      private org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.INodeType type_ = org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.INodeType.I_TYPE_FILE;
      /**
       * <code>required .hadoop.hdfs.INodeType type = 1;</code>
       */
      public boolean hasType() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required .hadoop.hdfs.INodeType type = 1;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.INodeType getType() {
        return type_;
      }
      /**
       * <code>required .hadoop.hdfs.INodeType type = 1;</code>
       */
      public Builder setType(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.INodeType value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000001;
        type_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required .hadoop.hdfs.INodeType type = 1;</code>
       */
      public Builder clearType() {
        bitField0_ = (bitField0_ & ~0x00000001);
        type_ = org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.INodeType.I_TYPE_FILE;
        onChanged();
        return this;
      }

      // required string path = 2;
      private java.lang.Object path_ = "";
      /**
       * <code>required string path = 2;</code>
       */
      public boolean hasPath() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required string path = 2;</code>
       */
      public java.lang.String getPath() {
        java.lang.Object ref = path_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          path_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string path = 2;</code>
       */
      public com.google.protobuf.ByteString
          getPathBytes() {
        java.lang.Object ref = path_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          path_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string path = 2;</code>
       */
      public Builder setPath(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        path_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string path = 2;</code>
       */
      public Builder clearPath() {
        bitField0_ = (bitField0_ & ~0x00000002);
        path_ = getDefaultInstance().getPath();
        onChanged();
        return this;
      }
      /**
       * <code>required string path = 2;</code>
       */
      public Builder setPathBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        path_ = value;
        onChanged();
        return this;
      }

      // required int64 ctime = 3;
      private long ctime_ ;
      /**
       * <code>required int64 ctime = 3;</code>
       */
      public boolean hasCtime() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>required int64 ctime = 3;</code>
       */
      public long getCtime() {
        return ctime_;
      }
      /**
       * <code>required int64 ctime = 3;</code>
       */
      public Builder setCtime(long value) {
        bitField0_ |= 0x00000004;
        ctime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required int64 ctime = 3;</code>
       */
      public Builder clearCtime() {
        bitField0_ = (bitField0_ & ~0x00000004);
        ctime_ = 0L;
        onChanged();
        return this;
      }

      // required string ownerName = 4;
      private java.lang.Object ownerName_ = "";
      /**
       * <code>required string ownerName = 4;</code>
       */
      public boolean hasOwnerName() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>required string ownerName = 4;</code>
       */
      public java.lang.String getOwnerName() {
        java.lang.Object ref = ownerName_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          ownerName_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string ownerName = 4;</code>
       */
      public com.google.protobuf.ByteString
          getOwnerNameBytes() {
        java.lang.Object ref = ownerName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          ownerName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string ownerName = 4;</code>
       */
      public Builder setOwnerName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        ownerName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string ownerName = 4;</code>
       */
      public Builder clearOwnerName() {
        bitField0_ = (bitField0_ & ~0x00000008);
        ownerName_ = getDefaultInstance().getOwnerName();
        onChanged();
        return this;
      }
      /**
       * <code>required string ownerName = 4;</code>
       */
      public Builder setOwnerNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        ownerName_ = value;
        onChanged();
        return this;
      }

      // required string groupName = 5;
      private java.lang.Object groupName_ = "";
      /**
       * <code>required string groupName = 5;</code>
       */
      public boolean hasGroupName() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>required string groupName = 5;</code>
       */
      public java.lang.String getGroupName() {
        java.lang.Object ref = groupName_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          groupName_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string groupName = 5;</code>
       */
      public com.google.protobuf.ByteString
          getGroupNameBytes() {
        java.lang.Object ref = groupName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          groupName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string groupName = 5;</code>
       */
      public Builder setGroupName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        groupName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string groupName = 5;</code>
       */
      public Builder clearGroupName() {
        bitField0_ = (bitField0_ & ~0x00000010);
        groupName_ = getDefaultInstance().getGroupName();
        onChanged();
        return this;
      }
      /**
       * <code>required string groupName = 5;</code>
       */
      public Builder setGroupNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        groupName_ = value;
        onChanged();
        return this;
      }

      // required .hadoop.hdfs.FsPermissionProto perms = 6;
      private org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto perms_ = org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.Builder, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProtoOrBuilder> permsBuilder_;
      /**
       * <code>required .hadoop.hdfs.FsPermissionProto perms = 6;</code>
       */
      public boolean hasPerms() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      /**
       * <code>required .hadoop.hdfs.FsPermissionProto perms = 6;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto getPerms() {
        if (permsBuilder_ == null) {
          return perms_;
        } else {
          return permsBuilder_.getMessage();
        }
      }
      /**
       * <code>required .hadoop.hdfs.FsPermissionProto perms = 6;</code>
       */
      public Builder setPerms(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto value) {
        if (permsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          perms_ = value;
          onChanged();
        } else {
          permsBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000020;
        return this;
      }
      /**
       * <code>required .hadoop.hdfs.FsPermissionProto perms = 6;</code>
       */
      public Builder setPerms(
          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.Builder builderForValue) {
        if (permsBuilder_ == null) {
          perms_ = builderForValue.build();
          onChanged();
        } else {
          permsBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000020;
        return this;
      }
      /**
       * <code>required .hadoop.hdfs.FsPermissionProto perms = 6;</code>
       */
      public Builder mergePerms(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto value) {
        if (permsBuilder_ == null) {
          if (((bitField0_ & 0x00000020) == 0x00000020) &&
              perms_ != org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.getDefaultInstance()) {
            perms_ =
              org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.newBuilder(perms_).mergeFrom(value).buildPartial();
          } else {
            perms_ = value;
          }
          onChanged();
        } else {
          permsBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000020;
        return this;
      }
      /**
       * <code>required .hadoop.hdfs.FsPermissionProto perms = 6;</code>
       */
      public Builder clearPerms() {
        if (permsBuilder_ == null) {
          perms_ = org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.getDefaultInstance();
          onChanged();
        } else {
          permsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000020);
        return this;
      }
      /**
       * <code>required .hadoop.hdfs.FsPermissionProto perms = 6;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.Builder getPermsBuilder() {
        bitField0_ |= 0x00000020;
        onChanged();
        return getPermsFieldBuilder().getBuilder();
      }
      /**
       * <code>required .hadoop.hdfs.FsPermissionProto perms = 6;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProtoOrBuilder getPermsOrBuilder() {
        if (permsBuilder_ != null) {
          return permsBuilder_.getMessageOrBuilder();
        } else {
          return perms_;
        }
      }
      /**
       * <code>required .hadoop.hdfs.FsPermissionProto perms = 6;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.Builder, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProtoOrBuilder> 
          getPermsFieldBuilder() {
        if (permsBuilder_ == null) {
          permsBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.Builder, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProtoOrBuilder>(
                  perms_,
                  getParentForChildren(),
                  isClean());
          perms_ = null;
        }
        return permsBuilder_;
      }

      // optional int32 replication = 7;
      private int replication_ ;
      /**
       * <code>optional int32 replication = 7;</code>
       */
      public boolean hasReplication() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      /**
       * <code>optional int32 replication = 7;</code>
       */
      public int getReplication() {
        return replication_;
      }
      /**
       * <code>optional int32 replication = 7;</code>
       */
      public Builder setReplication(int value) {
        bitField0_ |= 0x00000040;
        replication_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 replication = 7;</code>
       */
      public Builder clearReplication() {
        bitField0_ = (bitField0_ & ~0x00000040);
        replication_ = 0;
        onChanged();
        return this;
      }

      // optional string symlinkTarget = 8;
      private java.lang.Object symlinkTarget_ = "";
      /**
       * <code>optional string symlinkTarget = 8;</code>
       */
      public boolean hasSymlinkTarget() {
        return ((bitField0_ & 0x00000080) == 0x00000080);
      }
      /**
       * <code>optional string symlinkTarget = 8;</code>
       */
      public java.lang.String getSymlinkTarget() {
        java.lang.Object ref = symlinkTarget_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          symlinkTarget_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string symlinkTarget = 8;</code>
       */
      public com.google.protobuf.ByteString
          getSymlinkTargetBytes() {
        java.lang.Object ref = symlinkTarget_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          symlinkTarget_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string symlinkTarget = 8;</code>
       */
      public Builder setSymlinkTarget(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000080;
        symlinkTarget_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string symlinkTarget = 8;</code>
       */
      public Builder clearSymlinkTarget() {
        bitField0_ = (bitField0_ & ~0x00000080);
        symlinkTarget_ = getDefaultInstance().getSymlinkTarget();
        onChanged();
        return this;
      }
      /**
       * <code>optional string symlinkTarget = 8;</code>
       */
      public Builder setSymlinkTargetBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000080;
        symlinkTarget_ = value;
        onChanged();
        return this;
      }

      // optional bool overwrite = 9;
      private boolean overwrite_ ;
      /**
       * <code>optional bool overwrite = 9;</code>
       */
      public boolean hasOverwrite() {
        return ((bitField0_ & 0x00000100) == 0x00000100);
      }
      /**
       * <code>optional bool overwrite = 9;</code>
       */
      public boolean getOverwrite() {
        return overwrite_;
      }
      /**
       * <code>optional bool overwrite = 9;</code>
       */
      public Builder setOverwrite(boolean value) {
        bitField0_ |= 0x00000100;
        overwrite_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool overwrite = 9;</code>
       */
      public Builder clearOverwrite() {
        bitField0_ = (bitField0_ & ~0x00000100);
        overwrite_ = false;
        onChanged();
        return this;
      }

      // optional int64 defaultBlockSize = 10 [default = 0];
      private long defaultBlockSize_ ;
      /**
       * <code>optional int64 defaultBlockSize = 10 [default = 0];</code>
       */
      public boolean hasDefaultBlockSize() {
        return ((bitField0_ & 0x00000200) == 0x00000200);
      }
      /**
       * <code>optional int64 defaultBlockSize = 10 [default = 0];</code>
       */
      public long getDefaultBlockSize() {
        return defaultBlockSize_;
      }
      /**
       * <code>optional int64 defaultBlockSize = 10 [default = 0];</code>
       */
      public Builder setDefaultBlockSize(long value) {
        bitField0_ |= 0x00000200;
        defaultBlockSize_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 defaultBlockSize = 10 [default = 0];</code>
       */
      public Builder clearDefaultBlockSize() {
        bitField0_ = (bitField0_ & ~0x00000200);
        defaultBlockSize_ = 0L;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.CreateEventProto)
    }

    static {
      defaultInstance = new CreateEventProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.CreateEventProto)
  }

  public interface CloseEventProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required string path = 1;
    /**
     * <code>required string path = 1;</code>
     */
    boolean hasPath();
    /**
     * <code>required string path = 1;</code>
     */
    java.lang.String getPath();
    /**
     * <code>required string path = 1;</code>
     */
    com.google.protobuf.ByteString
        getPathBytes();

    // required int64 fileSize = 2;
    /**
     * <code>required int64 fileSize = 2;</code>
     */
    boolean hasFileSize();
    /**
     * <code>required int64 fileSize = 2;</code>
     */
    long getFileSize();

    // required int64 timestamp = 3;
    /**
     * <code>required int64 timestamp = 3;</code>
     */
    boolean hasTimestamp();
    /**
     * <code>required int64 timestamp = 3;</code>
     */
    long getTimestamp();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.CloseEventProto}
   */
  public static final class CloseEventProto extends
      com.google.protobuf.GeneratedMessage
      implements CloseEventProtoOrBuilder {
    // Use CloseEventProto.newBuilder() to construct.
    private CloseEventProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private CloseEventProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final CloseEventProto defaultInstance;
    public static CloseEventProto getDefaultInstance() {
      return defaultInstance;
    }

    public CloseEventProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private CloseEventProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              path_ = input.readBytes();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              fileSize_ = input.readInt64();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              timestamp_ = input.readInt64();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_CloseEventProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_CloseEventProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto.class, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto.Builder.class);
    }

    public static com.google.protobuf.Parser<CloseEventProto> PARSER =
        new com.google.protobuf.AbstractParser<CloseEventProto>() {
      public CloseEventProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new CloseEventProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<CloseEventProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required string path = 1;
    public static final int PATH_FIELD_NUMBER = 1;
    private java.lang.Object path_;
    /**
     * <code>required string path = 1;</code>
     */
    public boolean hasPath() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required string path = 1;</code>
     */
    public java.lang.String getPath() {
      java.lang.Object ref = path_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          path_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string path = 1;</code>
     */
    public com.google.protobuf.ByteString
        getPathBytes() {
      java.lang.Object ref = path_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        path_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // required int64 fileSize = 2;
    public static final int FILESIZE_FIELD_NUMBER = 2;
    private long fileSize_;
    /**
     * <code>required int64 fileSize = 2;</code>
     */
    public boolean hasFileSize() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required int64 fileSize = 2;</code>
     */
    public long getFileSize() {
      return fileSize_;
    }

    // required int64 timestamp = 3;
    public static final int TIMESTAMP_FIELD_NUMBER = 3;
    private long timestamp_;
    /**
     * <code>required int64 timestamp = 3;</code>
     */
    public boolean hasTimestamp() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>required int64 timestamp = 3;</code>
     */
    public long getTimestamp() {
      return timestamp_;
    }

    private void initFields() {
      path_ = "";
      fileSize_ = 0L;
      timestamp_ = 0L;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasPath()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasFileSize()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasTimestamp()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getPathBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeInt64(2, fileSize_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeInt64(3, timestamp_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getPathBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, fileSize_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(3, timestamp_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto other = (org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto) obj;

      boolean result = true;
      result = result && (hasPath() == other.hasPath());
      if (hasPath()) {
        result = result && getPath()
            .equals(other.getPath());
      }
      result = result && (hasFileSize() == other.hasFileSize());
      if (hasFileSize()) {
        result = result && (getFileSize()
            == other.getFileSize());
      }
      result = result && (hasTimestamp() == other.hasTimestamp());
      if (hasTimestamp()) {
        result = result && (getTimestamp()
            == other.getTimestamp());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasPath()) {
        hash = (37 * hash) + PATH_FIELD_NUMBER;
        hash = (53 * hash) + getPath().hashCode();
      }
      if (hasFileSize()) {
        hash = (37 * hash) + FILESIZE_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getFileSize());
      }
      if (hasTimestamp()) {
        hash = (37 * hash) + TIMESTAMP_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getTimestamp());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.CloseEventProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_CloseEventProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_CloseEventProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto.class, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        path_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        fileSize_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        timestamp_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_CloseEventProto_descriptor;
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto build() {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto result = new org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.path_ = path_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.fileSize_ = fileSize_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.timestamp_ = timestamp_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto.getDefaultInstance()) return this;
        if (other.hasPath()) {
          bitField0_ |= 0x00000001;
          path_ = other.path_;
          onChanged();
        }
        if (other.hasFileSize()) {
          setFileSize(other.getFileSize());
        }
        if (other.hasTimestamp()) {
          setTimestamp(other.getTimestamp());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasPath()) {
          
          return false;
        }
        if (!hasFileSize()) {
          
          return false;
        }
        if (!hasTimestamp()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.CloseEventProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required string path = 1;
      private java.lang.Object path_ = "";
      /**
       * <code>required string path = 1;</code>
       */
      public boolean hasPath() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required string path = 1;</code>
       */
      public java.lang.String getPath() {
        java.lang.Object ref = path_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          path_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string path = 1;</code>
       */
      public com.google.protobuf.ByteString
          getPathBytes() {
        java.lang.Object ref = path_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          path_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string path = 1;</code>
       */
      public Builder setPath(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        path_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string path = 1;</code>
       */
      public Builder clearPath() {
        bitField0_ = (bitField0_ & ~0x00000001);
        path_ = getDefaultInstance().getPath();
        onChanged();
        return this;
      }
      /**
       * <code>required string path = 1;</code>
       */
      public Builder setPathBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        path_ = value;
        onChanged();
        return this;
      }

      // required int64 fileSize = 2;
      private long fileSize_ ;
      /**
       * <code>required int64 fileSize = 2;</code>
       */
      public boolean hasFileSize() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required int64 fileSize = 2;</code>
       */
      public long getFileSize() {
        return fileSize_;
      }
      /**
       * <code>required int64 fileSize = 2;</code>
       */
      public Builder setFileSize(long value) {
        bitField0_ |= 0x00000002;
        fileSize_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required int64 fileSize = 2;</code>
       */
      public Builder clearFileSize() {
        bitField0_ = (bitField0_ & ~0x00000002);
        fileSize_ = 0L;
        onChanged();
        return this;
      }

      // required int64 timestamp = 3;
      private long timestamp_ ;
      /**
       * <code>required int64 timestamp = 3;</code>
       */
      public boolean hasTimestamp() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>required int64 timestamp = 3;</code>
       */
      public long getTimestamp() {
        return timestamp_;
      }
      /**
       * <code>required int64 timestamp = 3;</code>
       */
      public Builder setTimestamp(long value) {
        bitField0_ |= 0x00000004;
        timestamp_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required int64 timestamp = 3;</code>
       */
      public Builder clearTimestamp() {
        bitField0_ = (bitField0_ & ~0x00000004);
        timestamp_ = 0L;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.CloseEventProto)
    }

    static {
      defaultInstance = new CloseEventProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.CloseEventProto)
  }

  public interface TruncateEventProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required string path = 1;
    /**
     * <code>required string path = 1;</code>
     */
    boolean hasPath();
    /**
     * <code>required string path = 1;</code>
     */
    java.lang.String getPath();
    /**
     * <code>required string path = 1;</code>
     */
    com.google.protobuf.ByteString
        getPathBytes();

    // required int64 fileSize = 2;
    /**
     * <code>required int64 fileSize = 2;</code>
     */
    boolean hasFileSize();
    /**
     * <code>required int64 fileSize = 2;</code>
     */
    long getFileSize();

    // required int64 timestamp = 3;
    /**
     * <code>required int64 timestamp = 3;</code>
     */
    boolean hasTimestamp();
    /**
     * <code>required int64 timestamp = 3;</code>
     */
    long getTimestamp();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.TruncateEventProto}
   */
  public static final class TruncateEventProto extends
      com.google.protobuf.GeneratedMessage
      implements TruncateEventProtoOrBuilder {
    // Use TruncateEventProto.newBuilder() to construct.
    private TruncateEventProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private TruncateEventProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final TruncateEventProto defaultInstance;
    public static TruncateEventProto getDefaultInstance() {
      return defaultInstance;
    }

    public TruncateEventProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private TruncateEventProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              path_ = input.readBytes();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              fileSize_ = input.readInt64();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              timestamp_ = input.readInt64();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_TruncateEventProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_TruncateEventProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto.class, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto.Builder.class);
    }

    public static com.google.protobuf.Parser<TruncateEventProto> PARSER =
        new com.google.protobuf.AbstractParser<TruncateEventProto>() {
      public TruncateEventProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new TruncateEventProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<TruncateEventProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required string path = 1;
    public static final int PATH_FIELD_NUMBER = 1;
    private java.lang.Object path_;
    /**
     * <code>required string path = 1;</code>
     */
    public boolean hasPath() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required string path = 1;</code>
     */
    public java.lang.String getPath() {
      java.lang.Object ref = path_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          path_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string path = 1;</code>
     */
    public com.google.protobuf.ByteString
        getPathBytes() {
      java.lang.Object ref = path_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        path_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // required int64 fileSize = 2;
    public static final int FILESIZE_FIELD_NUMBER = 2;
    private long fileSize_;
    /**
     * <code>required int64 fileSize = 2;</code>
     */
    public boolean hasFileSize() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required int64 fileSize = 2;</code>
     */
    public long getFileSize() {
      return fileSize_;
    }

    // required int64 timestamp = 3;
    public static final int TIMESTAMP_FIELD_NUMBER = 3;
    private long timestamp_;
    /**
     * <code>required int64 timestamp = 3;</code>
     */
    public boolean hasTimestamp() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>required int64 timestamp = 3;</code>
     */
    public long getTimestamp() {
      return timestamp_;
    }

    private void initFields() {
      path_ = "";
      fileSize_ = 0L;
      timestamp_ = 0L;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasPath()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasFileSize()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasTimestamp()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getPathBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeInt64(2, fileSize_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeInt64(3, timestamp_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getPathBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, fileSize_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(3, timestamp_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto other = (org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto) obj;

      boolean result = true;
      result = result && (hasPath() == other.hasPath());
      if (hasPath()) {
        result = result && getPath()
            .equals(other.getPath());
      }
      result = result && (hasFileSize() == other.hasFileSize());
      if (hasFileSize()) {
        result = result && (getFileSize()
            == other.getFileSize());
      }
      result = result && (hasTimestamp() == other.hasTimestamp());
      if (hasTimestamp()) {
        result = result && (getTimestamp()
            == other.getTimestamp());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasPath()) {
        hash = (37 * hash) + PATH_FIELD_NUMBER;
        hash = (53 * hash) + getPath().hashCode();
      }
      if (hasFileSize()) {
        hash = (37 * hash) + FILESIZE_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getFileSize());
      }
      if (hasTimestamp()) {
        hash = (37 * hash) + TIMESTAMP_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getTimestamp());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.TruncateEventProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_TruncateEventProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_TruncateEventProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto.class, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        path_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        fileSize_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        timestamp_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_TruncateEventProto_descriptor;
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto build() {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto result = new org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.path_ = path_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.fileSize_ = fileSize_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.timestamp_ = timestamp_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto.getDefaultInstance()) return this;
        if (other.hasPath()) {
          bitField0_ |= 0x00000001;
          path_ = other.path_;
          onChanged();
        }
        if (other.hasFileSize()) {
          setFileSize(other.getFileSize());
        }
        if (other.hasTimestamp()) {
          setTimestamp(other.getTimestamp());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasPath()) {
          
          return false;
        }
        if (!hasFileSize()) {
          
          return false;
        }
        if (!hasTimestamp()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.TruncateEventProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required string path = 1;
      private java.lang.Object path_ = "";
      /**
       * <code>required string path = 1;</code>
       */
      public boolean hasPath() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required string path = 1;</code>
       */
      public java.lang.String getPath() {
        java.lang.Object ref = path_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          path_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string path = 1;</code>
       */
      public com.google.protobuf.ByteString
          getPathBytes() {
        java.lang.Object ref = path_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          path_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string path = 1;</code>
       */
      public Builder setPath(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        path_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string path = 1;</code>
       */
      public Builder clearPath() {
        bitField0_ = (bitField0_ & ~0x00000001);
        path_ = getDefaultInstance().getPath();
        onChanged();
        return this;
      }
      /**
       * <code>required string path = 1;</code>
       */
      public Builder setPathBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        path_ = value;
        onChanged();
        return this;
      }

      // required int64 fileSize = 2;
      private long fileSize_ ;
      /**
       * <code>required int64 fileSize = 2;</code>
       */
      public boolean hasFileSize() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required int64 fileSize = 2;</code>
       */
      public long getFileSize() {
        return fileSize_;
      }
      /**
       * <code>required int64 fileSize = 2;</code>
       */
      public Builder setFileSize(long value) {
        bitField0_ |= 0x00000002;
        fileSize_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required int64 fileSize = 2;</code>
       */
      public Builder clearFileSize() {
        bitField0_ = (bitField0_ & ~0x00000002);
        fileSize_ = 0L;
        onChanged();
        return this;
      }

      // required int64 timestamp = 3;
      private long timestamp_ ;
      /**
       * <code>required int64 timestamp = 3;</code>
       */
      public boolean hasTimestamp() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>required int64 timestamp = 3;</code>
       */
      public long getTimestamp() {
        return timestamp_;
      }
      /**
       * <code>required int64 timestamp = 3;</code>
       */
      public Builder setTimestamp(long value) {
        bitField0_ |= 0x00000004;
        timestamp_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required int64 timestamp = 3;</code>
       */
      public Builder clearTimestamp() {
        bitField0_ = (bitField0_ & ~0x00000004);
        timestamp_ = 0L;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.TruncateEventProto)
    }

    static {
      defaultInstance = new TruncateEventProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.TruncateEventProto)
  }

  public interface AppendEventProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required string path = 1;
    /**
     * <code>required string path = 1;</code>
     */
    boolean hasPath();
    /**
     * <code>required string path = 1;</code>
     */
    java.lang.String getPath();
    /**
     * <code>required string path = 1;</code>
     */
    com.google.protobuf.ByteString
        getPathBytes();

    // optional bool newBlock = 2 [default = false];
    /**
     * <code>optional bool newBlock = 2 [default = false];</code>
     */
    boolean hasNewBlock();
    /**
     * <code>optional bool newBlock = 2 [default = false];</code>
     */
    boolean getNewBlock();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.AppendEventProto}
   */
  public static final class AppendEventProto extends
      com.google.protobuf.GeneratedMessage
      implements AppendEventProtoOrBuilder {
    // Use AppendEventProto.newBuilder() to construct.
    private AppendEventProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private AppendEventProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final AppendEventProto defaultInstance;
    public static AppendEventProto getDefaultInstance() {
      return defaultInstance;
    }

    public AppendEventProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private AppendEventProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              path_ = input.readBytes();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              newBlock_ = input.readBool();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_AppendEventProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_AppendEventProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto.class, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto.Builder.class);
    }

    public static com.google.protobuf.Parser<AppendEventProto> PARSER =
        new com.google.protobuf.AbstractParser<AppendEventProto>() {
      public AppendEventProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new AppendEventProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<AppendEventProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required string path = 1;
    public static final int PATH_FIELD_NUMBER = 1;
    private java.lang.Object path_;
    /**
     * <code>required string path = 1;</code>
     */
    public boolean hasPath() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required string path = 1;</code>
     */
    public java.lang.String getPath() {
      java.lang.Object ref = path_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          path_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string path = 1;</code>
     */
    public com.google.protobuf.ByteString
        getPathBytes() {
      java.lang.Object ref = path_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        path_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional bool newBlock = 2 [default = false];
    public static final int NEWBLOCK_FIELD_NUMBER = 2;
    private boolean newBlock_;
    /**
     * <code>optional bool newBlock = 2 [default = false];</code>
     */
    public boolean hasNewBlock() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional bool newBlock = 2 [default = false];</code>
     */
    public boolean getNewBlock() {
      return newBlock_;
    }

    private void initFields() {
      path_ = "";
      newBlock_ = false;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasPath()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getPathBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBool(2, newBlock_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getPathBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(2, newBlock_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto other = (org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto) obj;

      boolean result = true;
      result = result && (hasPath() == other.hasPath());
      if (hasPath()) {
        result = result && getPath()
            .equals(other.getPath());
      }
      result = result && (hasNewBlock() == other.hasNewBlock());
      if (hasNewBlock()) {
        result = result && (getNewBlock()
            == other.getNewBlock());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasPath()) {
        hash = (37 * hash) + PATH_FIELD_NUMBER;
        hash = (53 * hash) + getPath().hashCode();
      }
      if (hasNewBlock()) {
        hash = (37 * hash) + NEWBLOCK_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getNewBlock());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.AppendEventProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_AppendEventProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_AppendEventProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto.class, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        path_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        newBlock_ = false;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_AppendEventProto_descriptor;
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto build() {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto result = new org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.path_ = path_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.newBlock_ = newBlock_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto.getDefaultInstance()) return this;
        if (other.hasPath()) {
          bitField0_ |= 0x00000001;
          path_ = other.path_;
          onChanged();
        }
        if (other.hasNewBlock()) {
          setNewBlock(other.getNewBlock());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasPath()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.AppendEventProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required string path = 1;
      private java.lang.Object path_ = "";
      /**
       * <code>required string path = 1;</code>
       */
      public boolean hasPath() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required string path = 1;</code>
       */
      public java.lang.String getPath() {
        java.lang.Object ref = path_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          path_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string path = 1;</code>
       */
      public com.google.protobuf.ByteString
          getPathBytes() {
        java.lang.Object ref = path_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          path_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string path = 1;</code>
       */
      public Builder setPath(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        path_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string path = 1;</code>
       */
      public Builder clearPath() {
        bitField0_ = (bitField0_ & ~0x00000001);
        path_ = getDefaultInstance().getPath();
        onChanged();
        return this;
      }
      /**
       * <code>required string path = 1;</code>
       */
      public Builder setPathBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        path_ = value;
        onChanged();
        return this;
      }

      // optional bool newBlock = 2 [default = false];
      private boolean newBlock_ ;
      /**
       * <code>optional bool newBlock = 2 [default = false];</code>
       */
      public boolean hasNewBlock() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional bool newBlock = 2 [default = false];</code>
       */
      public boolean getNewBlock() {
        return newBlock_;
      }
      /**
       * <code>optional bool newBlock = 2 [default = false];</code>
       */
      public Builder setNewBlock(boolean value) {
        bitField0_ |= 0x00000002;
        newBlock_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool newBlock = 2 [default = false];</code>
       */
      public Builder clearNewBlock() {
        bitField0_ = (bitField0_ & ~0x00000002);
        newBlock_ = false;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.AppendEventProto)
    }

    static {
      defaultInstance = new AppendEventProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.AppendEventProto)
  }

  public interface RenameEventProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required string srcPath = 1;
    /**
     * <code>required string srcPath = 1;</code>
     */
    boolean hasSrcPath();
    /**
     * <code>required string srcPath = 1;</code>
     */
    java.lang.String getSrcPath();
    /**
     * <code>required string srcPath = 1;</code>
     */
    com.google.protobuf.ByteString
        getSrcPathBytes();

    // required string destPath = 2;
    /**
     * <code>required string destPath = 2;</code>
     */
    boolean hasDestPath();
    /**
     * <code>required string destPath = 2;</code>
     */
    java.lang.String getDestPath();
    /**
     * <code>required string destPath = 2;</code>
     */
    com.google.protobuf.ByteString
        getDestPathBytes();

    // required int64 timestamp = 3;
    /**
     * <code>required int64 timestamp = 3;</code>
     */
    boolean hasTimestamp();
    /**
     * <code>required int64 timestamp = 3;</code>
     */
    long getTimestamp();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.RenameEventProto}
   */
  public static final class RenameEventProto extends
      com.google.protobuf.GeneratedMessage
      implements RenameEventProtoOrBuilder {
    // Use RenameEventProto.newBuilder() to construct.
    private RenameEventProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private RenameEventProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final RenameEventProto defaultInstance;
    public static RenameEventProto getDefaultInstance() {
      return defaultInstance;
    }

    public RenameEventProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private RenameEventProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              srcPath_ = input.readBytes();
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              destPath_ = input.readBytes();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              timestamp_ = input.readInt64();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_RenameEventProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_RenameEventProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto.class, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto.Builder.class);
    }

    public static com.google.protobuf.Parser<RenameEventProto> PARSER =
        new com.google.protobuf.AbstractParser<RenameEventProto>() {
      public RenameEventProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RenameEventProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<RenameEventProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required string srcPath = 1;
    public static final int SRCPATH_FIELD_NUMBER = 1;
    private java.lang.Object srcPath_;
    /**
     * <code>required string srcPath = 1;</code>
     */
    public boolean hasSrcPath() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required string srcPath = 1;</code>
     */
    public java.lang.String getSrcPath() {
      java.lang.Object ref = srcPath_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          srcPath_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string srcPath = 1;</code>
     */
    public com.google.protobuf.ByteString
        getSrcPathBytes() {
      java.lang.Object ref = srcPath_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        srcPath_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // required string destPath = 2;
    public static final int DESTPATH_FIELD_NUMBER = 2;
    private java.lang.Object destPath_;
    /**
     * <code>required string destPath = 2;</code>
     */
    public boolean hasDestPath() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required string destPath = 2;</code>
     */
    public java.lang.String getDestPath() {
      java.lang.Object ref = destPath_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          destPath_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string destPath = 2;</code>
     */
    public com.google.protobuf.ByteString
        getDestPathBytes() {
      java.lang.Object ref = destPath_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        destPath_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // required int64 timestamp = 3;
    public static final int TIMESTAMP_FIELD_NUMBER = 3;
    private long timestamp_;
    /**
     * <code>required int64 timestamp = 3;</code>
     */
    public boolean hasTimestamp() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>required int64 timestamp = 3;</code>
     */
    public long getTimestamp() {
      return timestamp_;
    }

    private void initFields() {
      srcPath_ = "";
      destPath_ = "";
      timestamp_ = 0L;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasSrcPath()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasDestPath()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasTimestamp()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getSrcPathBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, getDestPathBytes());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeInt64(3, timestamp_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getSrcPathBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, getDestPathBytes());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(3, timestamp_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto other = (org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto) obj;

      boolean result = true;
      result = result && (hasSrcPath() == other.hasSrcPath());
      if (hasSrcPath()) {
        result = result && getSrcPath()
            .equals(other.getSrcPath());
      }
      result = result && (hasDestPath() == other.hasDestPath());
      if (hasDestPath()) {
        result = result && getDestPath()
            .equals(other.getDestPath());
      }
      result = result && (hasTimestamp() == other.hasTimestamp());
      if (hasTimestamp()) {
        result = result && (getTimestamp()
            == other.getTimestamp());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasSrcPath()) {
        hash = (37 * hash) + SRCPATH_FIELD_NUMBER;
        hash = (53 * hash) + getSrcPath().hashCode();
      }
      if (hasDestPath()) {
        hash = (37 * hash) + DESTPATH_FIELD_NUMBER;
        hash = (53 * hash) + getDestPath().hashCode();
      }
      if (hasTimestamp()) {
        hash = (37 * hash) + TIMESTAMP_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getTimestamp());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.RenameEventProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_RenameEventProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_RenameEventProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto.class, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        srcPath_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        destPath_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        timestamp_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_RenameEventProto_descriptor;
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto build() {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto result = new org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.srcPath_ = srcPath_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.destPath_ = destPath_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.timestamp_ = timestamp_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto.getDefaultInstance()) return this;
        if (other.hasSrcPath()) {
          bitField0_ |= 0x00000001;
          srcPath_ = other.srcPath_;
          onChanged();
        }
        if (other.hasDestPath()) {
          bitField0_ |= 0x00000002;
          destPath_ = other.destPath_;
          onChanged();
        }
        if (other.hasTimestamp()) {
          setTimestamp(other.getTimestamp());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasSrcPath()) {
          
          return false;
        }
        if (!hasDestPath()) {
          
          return false;
        }
        if (!hasTimestamp()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.RenameEventProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required string srcPath = 1;
      private java.lang.Object srcPath_ = "";
      /**
       * <code>required string srcPath = 1;</code>
       */
      public boolean hasSrcPath() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required string srcPath = 1;</code>
       */
      public java.lang.String getSrcPath() {
        java.lang.Object ref = srcPath_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          srcPath_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string srcPath = 1;</code>
       */
      public com.google.protobuf.ByteString
          getSrcPathBytes() {
        java.lang.Object ref = srcPath_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          srcPath_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string srcPath = 1;</code>
       */
      public Builder setSrcPath(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        srcPath_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string srcPath = 1;</code>
       */
      public Builder clearSrcPath() {
        bitField0_ = (bitField0_ & ~0x00000001);
        srcPath_ = getDefaultInstance().getSrcPath();
        onChanged();
        return this;
      }
      /**
       * <code>required string srcPath = 1;</code>
       */
      public Builder setSrcPathBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        srcPath_ = value;
        onChanged();
        return this;
      }

      // required string destPath = 2;
      private java.lang.Object destPath_ = "";
      /**
       * <code>required string destPath = 2;</code>
       */
      public boolean hasDestPath() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required string destPath = 2;</code>
       */
      public java.lang.String getDestPath() {
        java.lang.Object ref = destPath_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          destPath_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string destPath = 2;</code>
       */
      public com.google.protobuf.ByteString
          getDestPathBytes() {
        java.lang.Object ref = destPath_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          destPath_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string destPath = 2;</code>
       */
      public Builder setDestPath(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        destPath_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string destPath = 2;</code>
       */
      public Builder clearDestPath() {
        bitField0_ = (bitField0_ & ~0x00000002);
        destPath_ = getDefaultInstance().getDestPath();
        onChanged();
        return this;
      }
      /**
       * <code>required string destPath = 2;</code>
       */
      public Builder setDestPathBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        destPath_ = value;
        onChanged();
        return this;
      }

      // required int64 timestamp = 3;
      private long timestamp_ ;
      /**
       * <code>required int64 timestamp = 3;</code>
       */
      public boolean hasTimestamp() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>required int64 timestamp = 3;</code>
       */
      public long getTimestamp() {
        return timestamp_;
      }
      /**
       * <code>required int64 timestamp = 3;</code>
       */
      public Builder setTimestamp(long value) {
        bitField0_ |= 0x00000004;
        timestamp_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required int64 timestamp = 3;</code>
       */
      public Builder clearTimestamp() {
        bitField0_ = (bitField0_ & ~0x00000004);
        timestamp_ = 0L;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.RenameEventProto)
    }

    static {
      defaultInstance = new RenameEventProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.RenameEventProto)
  }

  public interface MetadataUpdateEventProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required string path = 1;
    /**
     * <code>required string path = 1;</code>
     */
    boolean hasPath();
    /**
     * <code>required string path = 1;</code>
     */
    java.lang.String getPath();
    /**
     * <code>required string path = 1;</code>
     */
    com.google.protobuf.ByteString
        getPathBytes();

    // required .hadoop.hdfs.MetadataUpdateType type = 2;
    /**
     * <code>required .hadoop.hdfs.MetadataUpdateType type = 2;</code>
     */
    boolean hasType();
    /**
     * <code>required .hadoop.hdfs.MetadataUpdateType type = 2;</code>
     */
    org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateType getType();

    // optional int64 mtime = 3;
    /**
     * <code>optional int64 mtime = 3;</code>
     */
    boolean hasMtime();
    /**
     * <code>optional int64 mtime = 3;</code>
     */
    long getMtime();

    // optional int64 atime = 4;
    /**
     * <code>optional int64 atime = 4;</code>
     */
    boolean hasAtime();
    /**
     * <code>optional int64 atime = 4;</code>
     */
    long getAtime();

    // optional int32 replication = 5;
    /**
     * <code>optional int32 replication = 5;</code>
     */
    boolean hasReplication();
    /**
     * <code>optional int32 replication = 5;</code>
     */
    int getReplication();

    // optional string ownerName = 6;
    /**
     * <code>optional string ownerName = 6;</code>
     */
    boolean hasOwnerName();
    /**
     * <code>optional string ownerName = 6;</code>
     */
    java.lang.String getOwnerName();
    /**
     * <code>optional string ownerName = 6;</code>
     */
    com.google.protobuf.ByteString
        getOwnerNameBytes();

    // optional string groupName = 7;
    /**
     * <code>optional string groupName = 7;</code>
     */
    boolean hasGroupName();
    /**
     * <code>optional string groupName = 7;</code>
     */
    java.lang.String getGroupName();
    /**
     * <code>optional string groupName = 7;</code>
     */
    com.google.protobuf.ByteString
        getGroupNameBytes();

    // optional .hadoop.hdfs.FsPermissionProto perms = 8;
    /**
     * <code>optional .hadoop.hdfs.FsPermissionProto perms = 8;</code>
     */
    boolean hasPerms();
    /**
     * <code>optional .hadoop.hdfs.FsPermissionProto perms = 8;</code>
     */
    org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto getPerms();
    /**
     * <code>optional .hadoop.hdfs.FsPermissionProto perms = 8;</code>
     */
    org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProtoOrBuilder getPermsOrBuilder();

    // repeated .hadoop.hdfs.AclEntryProto acls = 9;
    /**
     * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto> 
        getAclsList();
    /**
     * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
     */
    org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto getAcls(int index);
    /**
     * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
     */
    int getAclsCount();
    /**
     * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProtoOrBuilder> 
        getAclsOrBuilderList();
    /**
     * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
     */
    org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProtoOrBuilder getAclsOrBuilder(
        int index);

    // repeated .hadoop.hdfs.XAttrProto xAttrs = 10;
    /**
     * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto> 
        getXAttrsList();
    /**
     * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
     */
    org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto getXAttrs(int index);
    /**
     * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
     */
    int getXAttrsCount();
    /**
     * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProtoOrBuilder> 
        getXAttrsOrBuilderList();
    /**
     * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
     */
    org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProtoOrBuilder getXAttrsOrBuilder(
        int index);

    // optional bool xAttrsRemoved = 11;
    /**
     * <code>optional bool xAttrsRemoved = 11;</code>
     */
    boolean hasXAttrsRemoved();
    /**
     * <code>optional bool xAttrsRemoved = 11;</code>
     */
    boolean getXAttrsRemoved();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.MetadataUpdateEventProto}
   */
  public static final class MetadataUpdateEventProto extends
      com.google.protobuf.GeneratedMessage
      implements MetadataUpdateEventProtoOrBuilder {
    // Use MetadataUpdateEventProto.newBuilder() to construct.
    private MetadataUpdateEventProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private MetadataUpdateEventProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final MetadataUpdateEventProto defaultInstance;
    public static MetadataUpdateEventProto getDefaultInstance() {
      return defaultInstance;
    }

    public MetadataUpdateEventProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private MetadataUpdateEventProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              path_ = input.readBytes();
              break;
            }
            case 16: {
              int rawValue = input.readEnum();
              org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateType value = org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateType.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(2, rawValue);
              } else {
                bitField0_ |= 0x00000002;
                type_ = value;
              }
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              mtime_ = input.readInt64();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              atime_ = input.readInt64();
              break;
            }
            case 40: {
              bitField0_ |= 0x00000010;
              replication_ = input.readInt32();
              break;
            }
            case 50: {
              bitField0_ |= 0x00000020;
              ownerName_ = input.readBytes();
              break;
            }
            case 58: {
              bitField0_ |= 0x00000040;
              groupName_ = input.readBytes();
              break;
            }
            case 66: {
              org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000080) == 0x00000080)) {
                subBuilder = perms_.toBuilder();
              }
              perms_ = input.readMessage(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(perms_);
                perms_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000080;
              break;
            }
            case 74: {
              if (!((mutable_bitField0_ & 0x00000100) == 0x00000100)) {
                acls_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto>();
                mutable_bitField0_ |= 0x00000100;
              }
              acls_.add(input.readMessage(org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto.PARSER, extensionRegistry));
              break;
            }
            case 82: {
              if (!((mutable_bitField0_ & 0x00000200) == 0x00000200)) {
                xAttrs_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto>();
                mutable_bitField0_ |= 0x00000200;
              }
              xAttrs_.add(input.readMessage(org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto.PARSER, extensionRegistry));
              break;
            }
            case 88: {
              bitField0_ |= 0x00000100;
              xAttrsRemoved_ = input.readBool();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000100) == 0x00000100)) {
          acls_ = java.util.Collections.unmodifiableList(acls_);
        }
        if (((mutable_bitField0_ & 0x00000200) == 0x00000200)) {
          xAttrs_ = java.util.Collections.unmodifiableList(xAttrs_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_MetadataUpdateEventProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_MetadataUpdateEventProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto.class, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto.Builder.class);
    }

    public static com.google.protobuf.Parser<MetadataUpdateEventProto> PARSER =
        new com.google.protobuf.AbstractParser<MetadataUpdateEventProto>() {
      public MetadataUpdateEventProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new MetadataUpdateEventProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<MetadataUpdateEventProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required string path = 1;
    public static final int PATH_FIELD_NUMBER = 1;
    private java.lang.Object path_;
    /**
     * <code>required string path = 1;</code>
     */
    public boolean hasPath() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required string path = 1;</code>
     */
    public java.lang.String getPath() {
      java.lang.Object ref = path_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          path_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string path = 1;</code>
     */
    public com.google.protobuf.ByteString
        getPathBytes() {
      java.lang.Object ref = path_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        path_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // required .hadoop.hdfs.MetadataUpdateType type = 2;
    public static final int TYPE_FIELD_NUMBER = 2;
    private org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateType type_;
    /**
     * <code>required .hadoop.hdfs.MetadataUpdateType type = 2;</code>
     */
    public boolean hasType() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required .hadoop.hdfs.MetadataUpdateType type = 2;</code>
     */
    public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateType getType() {
      return type_;
    }

    // optional int64 mtime = 3;
    public static final int MTIME_FIELD_NUMBER = 3;
    private long mtime_;
    /**
     * <code>optional int64 mtime = 3;</code>
     */
    public boolean hasMtime() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional int64 mtime = 3;</code>
     */
    public long getMtime() {
      return mtime_;
    }

    // optional int64 atime = 4;
    public static final int ATIME_FIELD_NUMBER = 4;
    private long atime_;
    /**
     * <code>optional int64 atime = 4;</code>
     */
    public boolean hasAtime() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>optional int64 atime = 4;</code>
     */
    public long getAtime() {
      return atime_;
    }

    // optional int32 replication = 5;
    public static final int REPLICATION_FIELD_NUMBER = 5;
    private int replication_;
    /**
     * <code>optional int32 replication = 5;</code>
     */
    public boolean hasReplication() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>optional int32 replication = 5;</code>
     */
    public int getReplication() {
      return replication_;
    }

    // optional string ownerName = 6;
    public static final int OWNERNAME_FIELD_NUMBER = 6;
    private java.lang.Object ownerName_;
    /**
     * <code>optional string ownerName = 6;</code>
     */
    public boolean hasOwnerName() {
      return ((bitField0_ & 0x00000020) == 0x00000020);
    }
    /**
     * <code>optional string ownerName = 6;</code>
     */
    public java.lang.String getOwnerName() {
      java.lang.Object ref = ownerName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          ownerName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string ownerName = 6;</code>
     */
    public com.google.protobuf.ByteString
        getOwnerNameBytes() {
      java.lang.Object ref = ownerName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        ownerName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional string groupName = 7;
    public static final int GROUPNAME_FIELD_NUMBER = 7;
    private java.lang.Object groupName_;
    /**
     * <code>optional string groupName = 7;</code>
     */
    public boolean hasGroupName() {
      return ((bitField0_ & 0x00000040) == 0x00000040);
    }
    /**
     * <code>optional string groupName = 7;</code>
     */
    public java.lang.String getGroupName() {
      java.lang.Object ref = groupName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          groupName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string groupName = 7;</code>
     */
    public com.google.protobuf.ByteString
        getGroupNameBytes() {
      java.lang.Object ref = groupName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        groupName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional .hadoop.hdfs.FsPermissionProto perms = 8;
    public static final int PERMS_FIELD_NUMBER = 8;
    private org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto perms_;
    /**
     * <code>optional .hadoop.hdfs.FsPermissionProto perms = 8;</code>
     */
    public boolean hasPerms() {
      return ((bitField0_ & 0x00000080) == 0x00000080);
    }
    /**
     * <code>optional .hadoop.hdfs.FsPermissionProto perms = 8;</code>
     */
    public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto getPerms() {
      return perms_;
    }
    /**
     * <code>optional .hadoop.hdfs.FsPermissionProto perms = 8;</code>
     */
    public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProtoOrBuilder getPermsOrBuilder() {
      return perms_;
    }

    // repeated .hadoop.hdfs.AclEntryProto acls = 9;
    public static final int ACLS_FIELD_NUMBER = 9;
    private java.util.List<org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto> acls_;
    /**
     * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
     */
    public java.util.List<org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto> getAclsList() {
      return acls_;
    }
    /**
     * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProtoOrBuilder> 
        getAclsOrBuilderList() {
      return acls_;
    }
    /**
     * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
     */
    public int getAclsCount() {
      return acls_.size();
    }
    /**
     * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
     */
    public org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto getAcls(int index) {
      return acls_.get(index);
    }
    /**
     * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
     */
    public org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProtoOrBuilder getAclsOrBuilder(
        int index) {
      return acls_.get(index);
    }

    // repeated .hadoop.hdfs.XAttrProto xAttrs = 10;
    public static final int XATTRS_FIELD_NUMBER = 10;
    private java.util.List<org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto> xAttrs_;
    /**
     * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
     */
    public java.util.List<org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto> getXAttrsList() {
      return xAttrs_;
    }
    /**
     * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProtoOrBuilder> 
        getXAttrsOrBuilderList() {
      return xAttrs_;
    }
    /**
     * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
     */
    public int getXAttrsCount() {
      return xAttrs_.size();
    }
    /**
     * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
     */
    public org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto getXAttrs(int index) {
      return xAttrs_.get(index);
    }
    /**
     * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
     */
    public org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProtoOrBuilder getXAttrsOrBuilder(
        int index) {
      return xAttrs_.get(index);
    }

    // optional bool xAttrsRemoved = 11;
    public static final int XATTRSREMOVED_FIELD_NUMBER = 11;
    private boolean xAttrsRemoved_;
    /**
     * <code>optional bool xAttrsRemoved = 11;</code>
     */
    public boolean hasXAttrsRemoved() {
      return ((bitField0_ & 0x00000100) == 0x00000100);
    }
    /**
     * <code>optional bool xAttrsRemoved = 11;</code>
     */
    public boolean getXAttrsRemoved() {
      return xAttrsRemoved_;
    }

    private void initFields() {
      path_ = "";
      type_ = org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateType.META_TYPE_TIMES;
      mtime_ = 0L;
      atime_ = 0L;
      replication_ = 0;
      ownerName_ = "";
      groupName_ = "";
      perms_ = org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.getDefaultInstance();
      acls_ = java.util.Collections.emptyList();
      xAttrs_ = java.util.Collections.emptyList();
      xAttrsRemoved_ = false;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasPath()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasType()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (hasPerms()) {
        if (!getPerms().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getAclsCount(); i++) {
        if (!getAcls(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getXAttrsCount(); i++) {
        if (!getXAttrs(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getPathBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeEnum(2, type_.getNumber());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeInt64(3, mtime_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeInt64(4, atime_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeInt32(5, replication_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        output.writeBytes(6, getOwnerNameBytes());
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        output.writeBytes(7, getGroupNameBytes());
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        output.writeMessage(8, perms_);
      }
      for (int i = 0; i < acls_.size(); i++) {
        output.writeMessage(9, acls_.get(i));
      }
      for (int i = 0; i < xAttrs_.size(); i++) {
        output.writeMessage(10, xAttrs_.get(i));
      }
      if (((bitField0_ & 0x00000100) == 0x00000100)) {
        output.writeBool(11, xAttrsRemoved_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getPathBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(2, type_.getNumber());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(3, mtime_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(4, atime_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(5, replication_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(6, getOwnerNameBytes());
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(7, getGroupNameBytes());
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(8, perms_);
      }
      for (int i = 0; i < acls_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(9, acls_.get(i));
      }
      for (int i = 0; i < xAttrs_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(10, xAttrs_.get(i));
      }
      if (((bitField0_ & 0x00000100) == 0x00000100)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(11, xAttrsRemoved_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto other = (org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto) obj;

      boolean result = true;
      result = result && (hasPath() == other.hasPath());
      if (hasPath()) {
        result = result && getPath()
            .equals(other.getPath());
      }
      result = result && (hasType() == other.hasType());
      if (hasType()) {
        result = result &&
            (getType() == other.getType());
      }
      result = result && (hasMtime() == other.hasMtime());
      if (hasMtime()) {
        result = result && (getMtime()
            == other.getMtime());
      }
      result = result && (hasAtime() == other.hasAtime());
      if (hasAtime()) {
        result = result && (getAtime()
            == other.getAtime());
      }
      result = result && (hasReplication() == other.hasReplication());
      if (hasReplication()) {
        result = result && (getReplication()
            == other.getReplication());
      }
      result = result && (hasOwnerName() == other.hasOwnerName());
      if (hasOwnerName()) {
        result = result && getOwnerName()
            .equals(other.getOwnerName());
      }
      result = result && (hasGroupName() == other.hasGroupName());
      if (hasGroupName()) {
        result = result && getGroupName()
            .equals(other.getGroupName());
      }
      result = result && (hasPerms() == other.hasPerms());
      if (hasPerms()) {
        result = result && getPerms()
            .equals(other.getPerms());
      }
      result = result && getAclsList()
          .equals(other.getAclsList());
      result = result && getXAttrsList()
          .equals(other.getXAttrsList());
      result = result && (hasXAttrsRemoved() == other.hasXAttrsRemoved());
      if (hasXAttrsRemoved()) {
        result = result && (getXAttrsRemoved()
            == other.getXAttrsRemoved());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasPath()) {
        hash = (37 * hash) + PATH_FIELD_NUMBER;
        hash = (53 * hash) + getPath().hashCode();
      }
      if (hasType()) {
        hash = (37 * hash) + TYPE_FIELD_NUMBER;
        hash = (53 * hash) + hashEnum(getType());
      }
      if (hasMtime()) {
        hash = (37 * hash) + MTIME_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getMtime());
      }
      if (hasAtime()) {
        hash = (37 * hash) + ATIME_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getAtime());
      }
      if (hasReplication()) {
        hash = (37 * hash) + REPLICATION_FIELD_NUMBER;
        hash = (53 * hash) + getReplication();
      }
      if (hasOwnerName()) {
        hash = (37 * hash) + OWNERNAME_FIELD_NUMBER;
        hash = (53 * hash) + getOwnerName().hashCode();
      }
      if (hasGroupName()) {
        hash = (37 * hash) + GROUPNAME_FIELD_NUMBER;
        hash = (53 * hash) + getGroupName().hashCode();
      }
      if (hasPerms()) {
        hash = (37 * hash) + PERMS_FIELD_NUMBER;
        hash = (53 * hash) + getPerms().hashCode();
      }
      if (getAclsCount() > 0) {
        hash = (37 * hash) + ACLS_FIELD_NUMBER;
        hash = (53 * hash) + getAclsList().hashCode();
      }
      if (getXAttrsCount() > 0) {
        hash = (37 * hash) + XATTRS_FIELD_NUMBER;
        hash = (53 * hash) + getXAttrsList().hashCode();
      }
      if (hasXAttrsRemoved()) {
        hash = (37 * hash) + XATTRSREMOVED_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getXAttrsRemoved());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.MetadataUpdateEventProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_MetadataUpdateEventProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_MetadataUpdateEventProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto.class, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getPermsFieldBuilder();
          getAclsFieldBuilder();
          getXAttrsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        path_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        type_ = org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateType.META_TYPE_TIMES;
        bitField0_ = (bitField0_ & ~0x00000002);
        mtime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        atime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        replication_ = 0;
        bitField0_ = (bitField0_ & ~0x00000010);
        ownerName_ = "";
        bitField0_ = (bitField0_ & ~0x00000020);
        groupName_ = "";
        bitField0_ = (bitField0_ & ~0x00000040);
        if (permsBuilder_ == null) {
          perms_ = org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.getDefaultInstance();
        } else {
          permsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000080);
        if (aclsBuilder_ == null) {
          acls_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000100);
        } else {
          aclsBuilder_.clear();
        }
        if (xAttrsBuilder_ == null) {
          xAttrs_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000200);
        } else {
          xAttrsBuilder_.clear();
        }
        xAttrsRemoved_ = false;
        bitField0_ = (bitField0_ & ~0x00000400);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_MetadataUpdateEventProto_descriptor;
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto build() {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto result = new org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.path_ = path_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.type_ = type_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.mtime_ = mtime_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.atime_ = atime_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000010;
        }
        result.replication_ = replication_;
        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
          to_bitField0_ |= 0x00000020;
        }
        result.ownerName_ = ownerName_;
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000040;
        }
        result.groupName_ = groupName_;
        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
          to_bitField0_ |= 0x00000080;
        }
        if (permsBuilder_ == null) {
          result.perms_ = perms_;
        } else {
          result.perms_ = permsBuilder_.build();
        }
        if (aclsBuilder_ == null) {
          if (((bitField0_ & 0x00000100) == 0x00000100)) {
            acls_ = java.util.Collections.unmodifiableList(acls_);
            bitField0_ = (bitField0_ & ~0x00000100);
          }
          result.acls_ = acls_;
        } else {
          result.acls_ = aclsBuilder_.build();
        }
        if (xAttrsBuilder_ == null) {
          if (((bitField0_ & 0x00000200) == 0x00000200)) {
            xAttrs_ = java.util.Collections.unmodifiableList(xAttrs_);
            bitField0_ = (bitField0_ & ~0x00000200);
          }
          result.xAttrs_ = xAttrs_;
        } else {
          result.xAttrs_ = xAttrsBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000400) == 0x00000400)) {
          to_bitField0_ |= 0x00000100;
        }
        result.xAttrsRemoved_ = xAttrsRemoved_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto.getDefaultInstance()) return this;
        if (other.hasPath()) {
          bitField0_ |= 0x00000001;
          path_ = other.path_;
          onChanged();
        }
        if (other.hasType()) {
          setType(other.getType());
        }
        if (other.hasMtime()) {
          setMtime(other.getMtime());
        }
        if (other.hasAtime()) {
          setAtime(other.getAtime());
        }
        if (other.hasReplication()) {
          setReplication(other.getReplication());
        }
        if (other.hasOwnerName()) {
          bitField0_ |= 0x00000020;
          ownerName_ = other.ownerName_;
          onChanged();
        }
        if (other.hasGroupName()) {
          bitField0_ |= 0x00000040;
          groupName_ = other.groupName_;
          onChanged();
        }
        if (other.hasPerms()) {
          mergePerms(other.getPerms());
        }
        if (aclsBuilder_ == null) {
          if (!other.acls_.isEmpty()) {
            if (acls_.isEmpty()) {
              acls_ = other.acls_;
              bitField0_ = (bitField0_ & ~0x00000100);
            } else {
              ensureAclsIsMutable();
              acls_.addAll(other.acls_);
            }
            onChanged();
          }
        } else {
          if (!other.acls_.isEmpty()) {
            if (aclsBuilder_.isEmpty()) {
              aclsBuilder_.dispose();
              aclsBuilder_ = null;
              acls_ = other.acls_;
              bitField0_ = (bitField0_ & ~0x00000100);
              aclsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getAclsFieldBuilder() : null;
            } else {
              aclsBuilder_.addAllMessages(other.acls_);
            }
          }
        }
        if (xAttrsBuilder_ == null) {
          if (!other.xAttrs_.isEmpty()) {
            if (xAttrs_.isEmpty()) {
              xAttrs_ = other.xAttrs_;
              bitField0_ = (bitField0_ & ~0x00000200);
            } else {
              ensureXAttrsIsMutable();
              xAttrs_.addAll(other.xAttrs_);
            }
            onChanged();
          }
        } else {
          if (!other.xAttrs_.isEmpty()) {
            if (xAttrsBuilder_.isEmpty()) {
              xAttrsBuilder_.dispose();
              xAttrsBuilder_ = null;
              xAttrs_ = other.xAttrs_;
              bitField0_ = (bitField0_ & ~0x00000200);
              xAttrsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getXAttrsFieldBuilder() : null;
            } else {
              xAttrsBuilder_.addAllMessages(other.xAttrs_);
            }
          }
        }
        if (other.hasXAttrsRemoved()) {
          setXAttrsRemoved(other.getXAttrsRemoved());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasPath()) {
          
          return false;
        }
        if (!hasType()) {
          
          return false;
        }
        if (hasPerms()) {
          if (!getPerms().isInitialized()) {
            
            return false;
          }
        }
        for (int i = 0; i < getAclsCount(); i++) {
          if (!getAcls(i).isInitialized()) {
            
            return false;
          }
        }
        for (int i = 0; i < getXAttrsCount(); i++) {
          if (!getXAttrs(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateEventProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required string path = 1;
      private java.lang.Object path_ = "";
      /**
       * <code>required string path = 1;</code>
       */
      public boolean hasPath() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required string path = 1;</code>
       */
      public java.lang.String getPath() {
        java.lang.Object ref = path_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          path_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string path = 1;</code>
       */
      public com.google.protobuf.ByteString
          getPathBytes() {
        java.lang.Object ref = path_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          path_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string path = 1;</code>
       */
      public Builder setPath(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        path_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string path = 1;</code>
       */
      public Builder clearPath() {
        bitField0_ = (bitField0_ & ~0x00000001);
        path_ = getDefaultInstance().getPath();
        onChanged();
        return this;
      }
      /**
       * <code>required string path = 1;</code>
       */
      public Builder setPathBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        path_ = value;
        onChanged();
        return this;
      }

      // required .hadoop.hdfs.MetadataUpdateType type = 2;
      private org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateType type_ = org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateType.META_TYPE_TIMES;
      /**
       * <code>required .hadoop.hdfs.MetadataUpdateType type = 2;</code>
       */
      public boolean hasType() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required .hadoop.hdfs.MetadataUpdateType type = 2;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateType getType() {
        return type_;
      }
      /**
       * <code>required .hadoop.hdfs.MetadataUpdateType type = 2;</code>
       */
      public Builder setType(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateType value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000002;
        type_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required .hadoop.hdfs.MetadataUpdateType type = 2;</code>
       */
      public Builder clearType() {
        bitField0_ = (bitField0_ & ~0x00000002);
        type_ = org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.MetadataUpdateType.META_TYPE_TIMES;
        onChanged();
        return this;
      }

      // optional int64 mtime = 3;
      private long mtime_ ;
      /**
       * <code>optional int64 mtime = 3;</code>
       */
      public boolean hasMtime() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional int64 mtime = 3;</code>
       */
      public long getMtime() {
        return mtime_;
      }
      /**
       * <code>optional int64 mtime = 3;</code>
       */
      public Builder setMtime(long value) {
        bitField0_ |= 0x00000004;
        mtime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 mtime = 3;</code>
       */
      public Builder clearMtime() {
        bitField0_ = (bitField0_ & ~0x00000004);
        mtime_ = 0L;
        onChanged();
        return this;
      }

      // optional int64 atime = 4;
      private long atime_ ;
      /**
       * <code>optional int64 atime = 4;</code>
       */
      public boolean hasAtime() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional int64 atime = 4;</code>
       */
      public long getAtime() {
        return atime_;
      }
      /**
       * <code>optional int64 atime = 4;</code>
       */
      public Builder setAtime(long value) {
        bitField0_ |= 0x00000008;
        atime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 atime = 4;</code>
       */
      public Builder clearAtime() {
        bitField0_ = (bitField0_ & ~0x00000008);
        atime_ = 0L;
        onChanged();
        return this;
      }

      // optional int32 replication = 5;
      private int replication_ ;
      /**
       * <code>optional int32 replication = 5;</code>
       */
      public boolean hasReplication() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional int32 replication = 5;</code>
       */
      public int getReplication() {
        return replication_;
      }
      /**
       * <code>optional int32 replication = 5;</code>
       */
      public Builder setReplication(int value) {
        bitField0_ |= 0x00000010;
        replication_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 replication = 5;</code>
       */
      public Builder clearReplication() {
        bitField0_ = (bitField0_ & ~0x00000010);
        replication_ = 0;
        onChanged();
        return this;
      }

      // optional string ownerName = 6;
      private java.lang.Object ownerName_ = "";
      /**
       * <code>optional string ownerName = 6;</code>
       */
      public boolean hasOwnerName() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      /**
       * <code>optional string ownerName = 6;</code>
       */
      public java.lang.String getOwnerName() {
        java.lang.Object ref = ownerName_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          ownerName_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string ownerName = 6;</code>
       */
      public com.google.protobuf.ByteString
          getOwnerNameBytes() {
        java.lang.Object ref = ownerName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          ownerName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string ownerName = 6;</code>
       */
      public Builder setOwnerName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        ownerName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string ownerName = 6;</code>
       */
      public Builder clearOwnerName() {
        bitField0_ = (bitField0_ & ~0x00000020);
        ownerName_ = getDefaultInstance().getOwnerName();
        onChanged();
        return this;
      }
      /**
       * <code>optional string ownerName = 6;</code>
       */
      public Builder setOwnerNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        ownerName_ = value;
        onChanged();
        return this;
      }

      // optional string groupName = 7;
      private java.lang.Object groupName_ = "";
      /**
       * <code>optional string groupName = 7;</code>
       */
      public boolean hasGroupName() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      /**
       * <code>optional string groupName = 7;</code>
       */
      public java.lang.String getGroupName() {
        java.lang.Object ref = groupName_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          groupName_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string groupName = 7;</code>
       */
      public com.google.protobuf.ByteString
          getGroupNameBytes() {
        java.lang.Object ref = groupName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          groupName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string groupName = 7;</code>
       */
      public Builder setGroupName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000040;
        groupName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string groupName = 7;</code>
       */
      public Builder clearGroupName() {
        bitField0_ = (bitField0_ & ~0x00000040);
        groupName_ = getDefaultInstance().getGroupName();
        onChanged();
        return this;
      }
      /**
       * <code>optional string groupName = 7;</code>
       */
      public Builder setGroupNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000040;
        groupName_ = value;
        onChanged();
        return this;
      }

      // optional .hadoop.hdfs.FsPermissionProto perms = 8;
      private org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto perms_ = org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.Builder, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProtoOrBuilder> permsBuilder_;
      /**
       * <code>optional .hadoop.hdfs.FsPermissionProto perms = 8;</code>
       */
      public boolean hasPerms() {
        return ((bitField0_ & 0x00000080) == 0x00000080);
      }
      /**
       * <code>optional .hadoop.hdfs.FsPermissionProto perms = 8;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto getPerms() {
        if (permsBuilder_ == null) {
          return perms_;
        } else {
          return permsBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.hdfs.FsPermissionProto perms = 8;</code>
       */
      public Builder setPerms(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto value) {
        if (permsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          perms_ = value;
          onChanged();
        } else {
          permsBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.FsPermissionProto perms = 8;</code>
       */
      public Builder setPerms(
          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.Builder builderForValue) {
        if (permsBuilder_ == null) {
          perms_ = builderForValue.build();
          onChanged();
        } else {
          permsBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.FsPermissionProto perms = 8;</code>
       */
      public Builder mergePerms(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto value) {
        if (permsBuilder_ == null) {
          if (((bitField0_ & 0x00000080) == 0x00000080) &&
              perms_ != org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.getDefaultInstance()) {
            perms_ =
              org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.newBuilder(perms_).mergeFrom(value).buildPartial();
          } else {
            perms_ = value;
          }
          onChanged();
        } else {
          permsBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.FsPermissionProto perms = 8;</code>
       */
      public Builder clearPerms() {
        if (permsBuilder_ == null) {
          perms_ = org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.getDefaultInstance();
          onChanged();
        } else {
          permsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000080);
        return this;
      }
      /**
       * <code>optional .hadoop.hdfs.FsPermissionProto perms = 8;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.Builder getPermsBuilder() {
        bitField0_ |= 0x00000080;
        onChanged();
        return getPermsFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.hdfs.FsPermissionProto perms = 8;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProtoOrBuilder getPermsOrBuilder() {
        if (permsBuilder_ != null) {
          return permsBuilder_.getMessageOrBuilder();
        } else {
          return perms_;
        }
      }
      /**
       * <code>optional .hadoop.hdfs.FsPermissionProto perms = 8;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.Builder, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProtoOrBuilder> 
          getPermsFieldBuilder() {
        if (permsBuilder_ == null) {
          permsBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProto.Builder, org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.FsPermissionProtoOrBuilder>(
                  perms_,
                  getParentForChildren(),
                  isClean());
          perms_ = null;
        }
        return permsBuilder_;
      }

      // repeated .hadoop.hdfs.AclEntryProto acls = 9;
      private java.util.List<org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto> acls_ =
        java.util.Collections.emptyList();
      private void ensureAclsIsMutable() {
        if (!((bitField0_ & 0x00000100) == 0x00000100)) {
          acls_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto>(acls_);
          bitField0_ |= 0x00000100;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto, org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto.Builder, org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProtoOrBuilder> aclsBuilder_;

      /**
       * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto> getAclsList() {
        if (aclsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(acls_);
        } else {
          return aclsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
       */
      public int getAclsCount() {
        if (aclsBuilder_ == null) {
          return acls_.size();
        } else {
          return aclsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto getAcls(int index) {
        if (aclsBuilder_ == null) {
          return acls_.get(index);
        } else {
          return aclsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
       */
      public Builder setAcls(
          int index, org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto value) {
        if (aclsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAclsIsMutable();
          acls_.set(index, value);
          onChanged();
        } else {
          aclsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
       */
      public Builder setAcls(
          int index, org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto.Builder builderForValue) {
        if (aclsBuilder_ == null) {
          ensureAclsIsMutable();
          acls_.set(index, builderForValue.build());
          onChanged();
        } else {
          aclsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
       */
      public Builder addAcls(org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto value) {
        if (aclsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAclsIsMutable();
          acls_.add(value);
          onChanged();
        } else {
          aclsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
       */
      public Builder addAcls(
          int index, org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto value) {
        if (aclsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAclsIsMutable();
          acls_.add(index, value);
          onChanged();
        } else {
          aclsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
       */
      public Builder addAcls(
          org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto.Builder builderForValue) {
        if (aclsBuilder_ == null) {
          ensureAclsIsMutable();
          acls_.add(builderForValue.build());
          onChanged();
        } else {
          aclsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
       */
      public Builder addAcls(
          int index, org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto.Builder builderForValue) {
        if (aclsBuilder_ == null) {
          ensureAclsIsMutable();
          acls_.add(index, builderForValue.build());
          onChanged();
        } else {
          aclsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
       */
      public Builder addAllAcls(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto> values) {
        if (aclsBuilder_ == null) {
          ensureAclsIsMutable();
          super.addAll(values, acls_);
          onChanged();
        } else {
          aclsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
       */
      public Builder clearAcls() {
        if (aclsBuilder_ == null) {
          acls_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000100);
          onChanged();
        } else {
          aclsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
       */
      public Builder removeAcls(int index) {
        if (aclsBuilder_ == null) {
          ensureAclsIsMutable();
          acls_.remove(index);
          onChanged();
        } else {
          aclsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto.Builder getAclsBuilder(
          int index) {
        return getAclsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProtoOrBuilder getAclsOrBuilder(
          int index) {
        if (aclsBuilder_ == null) {
          return acls_.get(index);  } else {
          return aclsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProtoOrBuilder> 
           getAclsOrBuilderList() {
        if (aclsBuilder_ != null) {
          return aclsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(acls_);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto.Builder addAclsBuilder() {
        return getAclsFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto.Builder addAclsBuilder(
          int index) {
        return getAclsFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.hdfs.AclEntryProto acls = 9;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto.Builder> 
           getAclsBuilderList() {
        return getAclsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto, org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto.Builder, org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProtoOrBuilder> 
          getAclsFieldBuilder() {
        if (aclsBuilder_ == null) {
          aclsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto, org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProto.Builder, org.apache.hadoop.hdfs.protocol.proto.AclProtos.AclEntryProtoOrBuilder>(
                  acls_,
                  ((bitField0_ & 0x00000100) == 0x00000100),
                  getParentForChildren(),
                  isClean());
          acls_ = null;
        }
        return aclsBuilder_;
      }

      // repeated .hadoop.hdfs.XAttrProto xAttrs = 10;
      private java.util.List<org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto> xAttrs_ =
        java.util.Collections.emptyList();
      private void ensureXAttrsIsMutable() {
        if (!((bitField0_ & 0x00000200) == 0x00000200)) {
          xAttrs_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto>(xAttrs_);
          bitField0_ |= 0x00000200;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto, org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto.Builder, org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProtoOrBuilder> xAttrsBuilder_;

      /**
       * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto> getXAttrsList() {
        if (xAttrsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(xAttrs_);
        } else {
          return xAttrsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
       */
      public int getXAttrsCount() {
        if (xAttrsBuilder_ == null) {
          return xAttrs_.size();
        } else {
          return xAttrsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto getXAttrs(int index) {
        if (xAttrsBuilder_ == null) {
          return xAttrs_.get(index);
        } else {
          return xAttrsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
       */
      public Builder setXAttrs(
          int index, org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto value) {
        if (xAttrsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureXAttrsIsMutable();
          xAttrs_.set(index, value);
          onChanged();
        } else {
          xAttrsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
       */
      public Builder setXAttrs(
          int index, org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto.Builder builderForValue) {
        if (xAttrsBuilder_ == null) {
          ensureXAttrsIsMutable();
          xAttrs_.set(index, builderForValue.build());
          onChanged();
        } else {
          xAttrsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
       */
      public Builder addXAttrs(org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto value) {
        if (xAttrsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureXAttrsIsMutable();
          xAttrs_.add(value);
          onChanged();
        } else {
          xAttrsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
       */
      public Builder addXAttrs(
          int index, org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto value) {
        if (xAttrsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureXAttrsIsMutable();
          xAttrs_.add(index, value);
          onChanged();
        } else {
          xAttrsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
       */
      public Builder addXAttrs(
          org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto.Builder builderForValue) {
        if (xAttrsBuilder_ == null) {
          ensureXAttrsIsMutable();
          xAttrs_.add(builderForValue.build());
          onChanged();
        } else {
          xAttrsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
       */
      public Builder addXAttrs(
          int index, org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto.Builder builderForValue) {
        if (xAttrsBuilder_ == null) {
          ensureXAttrsIsMutable();
          xAttrs_.add(index, builderForValue.build());
          onChanged();
        } else {
          xAttrsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
       */
      public Builder addAllXAttrs(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto> values) {
        if (xAttrsBuilder_ == null) {
          ensureXAttrsIsMutable();
          super.addAll(values, xAttrs_);
          onChanged();
        } else {
          xAttrsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
       */
      public Builder clearXAttrs() {
        if (xAttrsBuilder_ == null) {
          xAttrs_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000200);
          onChanged();
        } else {
          xAttrsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
       */
      public Builder removeXAttrs(int index) {
        if (xAttrsBuilder_ == null) {
          ensureXAttrsIsMutable();
          xAttrs_.remove(index);
          onChanged();
        } else {
          xAttrsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto.Builder getXAttrsBuilder(
          int index) {
        return getXAttrsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProtoOrBuilder getXAttrsOrBuilder(
          int index) {
        if (xAttrsBuilder_ == null) {
          return xAttrs_.get(index);  } else {
          return xAttrsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProtoOrBuilder> 
           getXAttrsOrBuilderList() {
        if (xAttrsBuilder_ != null) {
          return xAttrsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(xAttrs_);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto.Builder addXAttrsBuilder() {
        return getXAttrsFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto.Builder addXAttrsBuilder(
          int index) {
        return getXAttrsFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.hdfs.XAttrProto xAttrs = 10;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto.Builder> 
           getXAttrsBuilderList() {
        return getXAttrsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto, org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto.Builder, org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProtoOrBuilder> 
          getXAttrsFieldBuilder() {
        if (xAttrsBuilder_ == null) {
          xAttrsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto, org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProto.Builder, org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.XAttrProtoOrBuilder>(
                  xAttrs_,
                  ((bitField0_ & 0x00000200) == 0x00000200),
                  getParentForChildren(),
                  isClean());
          xAttrs_ = null;
        }
        return xAttrsBuilder_;
      }

      // optional bool xAttrsRemoved = 11;
      private boolean xAttrsRemoved_ ;
      /**
       * <code>optional bool xAttrsRemoved = 11;</code>
       */
      public boolean hasXAttrsRemoved() {
        return ((bitField0_ & 0x00000400) == 0x00000400);
      }
      /**
       * <code>optional bool xAttrsRemoved = 11;</code>
       */
      public boolean getXAttrsRemoved() {
        return xAttrsRemoved_;
      }
      /**
       * <code>optional bool xAttrsRemoved = 11;</code>
       */
      public Builder setXAttrsRemoved(boolean value) {
        bitField0_ |= 0x00000400;
        xAttrsRemoved_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool xAttrsRemoved = 11;</code>
       */
      public Builder clearXAttrsRemoved() {
        bitField0_ = (bitField0_ & ~0x00000400);
        xAttrsRemoved_ = false;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.MetadataUpdateEventProto)
    }

    static {
      defaultInstance = new MetadataUpdateEventProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.MetadataUpdateEventProto)
  }

  public interface UnlinkEventProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required string path = 1;
    /**
     * <code>required string path = 1;</code>
     */
    boolean hasPath();
    /**
     * <code>required string path = 1;</code>
     */
    java.lang.String getPath();
    /**
     * <code>required string path = 1;</code>
     */
    com.google.protobuf.ByteString
        getPathBytes();

    // required int64 timestamp = 2;
    /**
     * <code>required int64 timestamp = 2;</code>
     */
    boolean hasTimestamp();
    /**
     * <code>required int64 timestamp = 2;</code>
     */
    long getTimestamp();
  }
  /**
   * Protobuf type {@code hadoop.hdfs.UnlinkEventProto}
   */
  public static final class UnlinkEventProto extends
      com.google.protobuf.GeneratedMessage
      implements UnlinkEventProtoOrBuilder {
    // Use UnlinkEventProto.newBuilder() to construct.
    private UnlinkEventProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private UnlinkEventProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final UnlinkEventProto defaultInstance;
    public static UnlinkEventProto getDefaultInstance() {
      return defaultInstance;
    }

    public UnlinkEventProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private UnlinkEventProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              path_ = input.readBytes();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              timestamp_ = input.readInt64();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_UnlinkEventProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_UnlinkEventProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto.class, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto.Builder.class);
    }

    public static com.google.protobuf.Parser<UnlinkEventProto> PARSER =
        new com.google.protobuf.AbstractParser<UnlinkEventProto>() {
      public UnlinkEventProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new UnlinkEventProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<UnlinkEventProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required string path = 1;
    public static final int PATH_FIELD_NUMBER = 1;
    private java.lang.Object path_;
    /**
     * <code>required string path = 1;</code>
     */
    public boolean hasPath() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required string path = 1;</code>
     */
    public java.lang.String getPath() {
      java.lang.Object ref = path_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          path_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string path = 1;</code>
     */
    public com.google.protobuf.ByteString
        getPathBytes() {
      java.lang.Object ref = path_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        path_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // required int64 timestamp = 2;
    public static final int TIMESTAMP_FIELD_NUMBER = 2;
    private long timestamp_;
    /**
     * <code>required int64 timestamp = 2;</code>
     */
    public boolean hasTimestamp() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required int64 timestamp = 2;</code>
     */
    public long getTimestamp() {
      return timestamp_;
    }

    private void initFields() {
      path_ = "";
      timestamp_ = 0L;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasPath()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasTimestamp()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getPathBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeInt64(2, timestamp_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getPathBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, timestamp_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto other = (org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto) obj;

      boolean result = true;
      result = result && (hasPath() == other.hasPath());
      if (hasPath()) {
        result = result && getPath()
            .equals(other.getPath());
      }
      result = result && (hasTimestamp() == other.hasTimestamp());
      if (hasTimestamp()) {
        result = result && (getTimestamp()
            == other.getTimestamp());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasPath()) {
        hash = (37 * hash) + PATH_FIELD_NUMBER;
        hash = (53 * hash) + getPath().hashCode();
      }
      if (hasTimestamp()) {
        hash = (37 * hash) + TIMESTAMP_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getTimestamp());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.UnlinkEventProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_UnlinkEventProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_UnlinkEventProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto.class, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        path_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        timestamp_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_UnlinkEventProto_descriptor;
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto build() {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto result = new org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.path_ = path_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.timestamp_ = timestamp_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto.getDefaultInstance()) return this;
        if (other.hasPath()) {
          bitField0_ |= 0x00000001;
          path_ = other.path_;
          onChanged();
        }
        if (other.hasTimestamp()) {
          setTimestamp(other.getTimestamp());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasPath()) {
          
          return false;
        }
        if (!hasTimestamp()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.UnlinkEventProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required string path = 1;
      private java.lang.Object path_ = "";
      /**
       * <code>required string path = 1;</code>
       */
      public boolean hasPath() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required string path = 1;</code>
       */
      public java.lang.String getPath() {
        java.lang.Object ref = path_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          path_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string path = 1;</code>
       */
      public com.google.protobuf.ByteString
          getPathBytes() {
        java.lang.Object ref = path_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          path_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string path = 1;</code>
       */
      public Builder setPath(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        path_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string path = 1;</code>
       */
      public Builder clearPath() {
        bitField0_ = (bitField0_ & ~0x00000001);
        path_ = getDefaultInstance().getPath();
        onChanged();
        return this;
      }
      /**
       * <code>required string path = 1;</code>
       */
      public Builder setPathBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        path_ = value;
        onChanged();
        return this;
      }

      // required int64 timestamp = 2;
      private long timestamp_ ;
      /**
       * <code>required int64 timestamp = 2;</code>
       */
      public boolean hasTimestamp() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required int64 timestamp = 2;</code>
       */
      public long getTimestamp() {
        return timestamp_;
      }
      /**
       * <code>required int64 timestamp = 2;</code>
       */
      public Builder setTimestamp(long value) {
        bitField0_ |= 0x00000002;
        timestamp_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required int64 timestamp = 2;</code>
       */
      public Builder clearTimestamp() {
        bitField0_ = (bitField0_ & ~0x00000002);
        timestamp_ = 0L;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.UnlinkEventProto)
    }

    static {
      defaultInstance = new UnlinkEventProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.UnlinkEventProto)
  }

  public interface EventsListProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.hdfs.EventProto events = 1;
    /**
     * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
     *
     * <pre>
     * deprecated
     * </pre>
     */
    java.util.List<org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto> 
        getEventsList();
    /**
     * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
     *
     * <pre>
     * deprecated
     * </pre>
     */
    org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto getEvents(int index);
    /**
     * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
     *
     * <pre>
     * deprecated
     * </pre>
     */
    int getEventsCount();
    /**
     * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
     *
     * <pre>
     * deprecated
     * </pre>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProtoOrBuilder> 
        getEventsOrBuilderList();
    /**
     * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
     *
     * <pre>
     * deprecated
     * </pre>
     */
    org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProtoOrBuilder getEventsOrBuilder(
        int index);

    // required int64 firstTxid = 2;
    /**
     * <code>required int64 firstTxid = 2;</code>
     */
    boolean hasFirstTxid();
    /**
     * <code>required int64 firstTxid = 2;</code>
     */
    long getFirstTxid();

    // required int64 lastTxid = 3;
    /**
     * <code>required int64 lastTxid = 3;</code>
     */
    boolean hasLastTxid();
    /**
     * <code>required int64 lastTxid = 3;</code>
     */
    long getLastTxid();

    // required int64 syncTxid = 4;
    /**
     * <code>required int64 syncTxid = 4;</code>
     */
    boolean hasSyncTxid();
    /**
     * <code>required int64 syncTxid = 4;</code>
     */
    long getSyncTxid();

    // repeated .hadoop.hdfs.EventBatchProto batch = 5;
    /**
     * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
     */
    java.util.List<org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto> 
        getBatchList();
    /**
     * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
     */
    org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto getBatch(int index);
    /**
     * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
     */
    int getBatchCount();
    /**
     * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
     */
    java.util.List<? extends org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProtoOrBuilder> 
        getBatchOrBuilderList();
    /**
     * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
     */
    org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProtoOrBuilder getBatchOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.hdfs.EventsListProto}
   */
  public static final class EventsListProto extends
      com.google.protobuf.GeneratedMessage
      implements EventsListProtoOrBuilder {
    // Use EventsListProto.newBuilder() to construct.
    private EventsListProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private EventsListProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final EventsListProto defaultInstance;
    public static EventsListProto getDefaultInstance() {
      return defaultInstance;
    }

    public EventsListProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private EventsListProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                events_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              events_.add(input.readMessage(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.PARSER, extensionRegistry));
              break;
            }
            case 16: {
              bitField0_ |= 0x00000001;
              firstTxid_ = input.readInt64();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000002;
              lastTxid_ = input.readInt64();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000004;
              syncTxid_ = input.readInt64();
              break;
            }
            case 42: {
              if (!((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
                batch_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto>();
                mutable_bitField0_ |= 0x00000010;
              }
              batch_.add(input.readMessage(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          events_ = java.util.Collections.unmodifiableList(events_);
        }
        if (((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
          batch_ = java.util.Collections.unmodifiableList(batch_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_EventsListProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_EventsListProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto.class, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto.Builder.class);
    }

    public static com.google.protobuf.Parser<EventsListProto> PARSER =
        new com.google.protobuf.AbstractParser<EventsListProto>() {
      public EventsListProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new EventsListProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<EventsListProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // repeated .hadoop.hdfs.EventProto events = 1;
    public static final int EVENTS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto> events_;
    /**
     * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
     *
     * <pre>
     * deprecated
     * </pre>
     */
    public java.util.List<org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto> getEventsList() {
      return events_;
    }
    /**
     * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
     *
     * <pre>
     * deprecated
     * </pre>
     */
    public java.util.List<? extends org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProtoOrBuilder> 
        getEventsOrBuilderList() {
      return events_;
    }
    /**
     * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
     *
     * <pre>
     * deprecated
     * </pre>
     */
    public int getEventsCount() {
      return events_.size();
    }
    /**
     * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
     *
     * <pre>
     * deprecated
     * </pre>
     */
    public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto getEvents(int index) {
      return events_.get(index);
    }
    /**
     * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
     *
     * <pre>
     * deprecated
     * </pre>
     */
    public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProtoOrBuilder getEventsOrBuilder(
        int index) {
      return events_.get(index);
    }

    // required int64 firstTxid = 2;
    public static final int FIRSTTXID_FIELD_NUMBER = 2;
    private long firstTxid_;
    /**
     * <code>required int64 firstTxid = 2;</code>
     */
    public boolean hasFirstTxid() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required int64 firstTxid = 2;</code>
     */
    public long getFirstTxid() {
      return firstTxid_;
    }

    // required int64 lastTxid = 3;
    public static final int LASTTXID_FIELD_NUMBER = 3;
    private long lastTxid_;
    /**
     * <code>required int64 lastTxid = 3;</code>
     */
    public boolean hasLastTxid() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required int64 lastTxid = 3;</code>
     */
    public long getLastTxid() {
      return lastTxid_;
    }

    // required int64 syncTxid = 4;
    public static final int SYNCTXID_FIELD_NUMBER = 4;
    private long syncTxid_;
    /**
     * <code>required int64 syncTxid = 4;</code>
     */
    public boolean hasSyncTxid() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>required int64 syncTxid = 4;</code>
     */
    public long getSyncTxid() {
      return syncTxid_;
    }

    // repeated .hadoop.hdfs.EventBatchProto batch = 5;
    public static final int BATCH_FIELD_NUMBER = 5;
    private java.util.List<org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto> batch_;
    /**
     * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
     */
    public java.util.List<org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto> getBatchList() {
      return batch_;
    }
    /**
     * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
     */
    public java.util.List<? extends org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProtoOrBuilder> 
        getBatchOrBuilderList() {
      return batch_;
    }
    /**
     * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
     */
    public int getBatchCount() {
      return batch_.size();
    }
    /**
     * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
     */
    public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto getBatch(int index) {
      return batch_.get(index);
    }
    /**
     * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
     */
    public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProtoOrBuilder getBatchOrBuilder(
        int index) {
      return batch_.get(index);
    }

    private void initFields() {
      events_ = java.util.Collections.emptyList();
      firstTxid_ = 0L;
      lastTxid_ = 0L;
      syncTxid_ = 0L;
      batch_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasFirstTxid()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasLastTxid()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasSyncTxid()) {
        memoizedIsInitialized = 0;
        return false;
      }
      for (int i = 0; i < getEventsCount(); i++) {
        if (!getEvents(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getBatchCount(); i++) {
        if (!getBatch(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < events_.size(); i++) {
        output.writeMessage(1, events_.get(i));
      }
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeInt64(2, firstTxid_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeInt64(3, lastTxid_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeInt64(4, syncTxid_);
      }
      for (int i = 0; i < batch_.size(); i++) {
        output.writeMessage(5, batch_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < events_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, events_.get(i));
      }
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(2, firstTxid_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(3, lastTxid_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(4, syncTxid_);
      }
      for (int i = 0; i < batch_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, batch_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto other = (org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto) obj;

      boolean result = true;
      result = result && getEventsList()
          .equals(other.getEventsList());
      result = result && (hasFirstTxid() == other.hasFirstTxid());
      if (hasFirstTxid()) {
        result = result && (getFirstTxid()
            == other.getFirstTxid());
      }
      result = result && (hasLastTxid() == other.hasLastTxid());
      if (hasLastTxid()) {
        result = result && (getLastTxid()
            == other.getLastTxid());
      }
      result = result && (hasSyncTxid() == other.hasSyncTxid());
      if (hasSyncTxid()) {
        result = result && (getSyncTxid()
            == other.getSyncTxid());
      }
      result = result && getBatchList()
          .equals(other.getBatchList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getEventsCount() > 0) {
        hash = (37 * hash) + EVENTS_FIELD_NUMBER;
        hash = (53 * hash) + getEventsList().hashCode();
      }
      if (hasFirstTxid()) {
        hash = (37 * hash) + FIRSTTXID_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getFirstTxid());
      }
      if (hasLastTxid()) {
        hash = (37 * hash) + LASTTXID_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getLastTxid());
      }
      if (hasSyncTxid()) {
        hash = (37 * hash) + SYNCTXID_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getSyncTxid());
      }
      if (getBatchCount() > 0) {
        hash = (37 * hash) + BATCH_FIELD_NUMBER;
        hash = (53 * hash) + getBatchList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.hdfs.EventsListProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_EventsListProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_EventsListProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto.class, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto.Builder.class);
      }

      // Construct using org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getEventsFieldBuilder();
          getBatchFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (eventsBuilder_ == null) {
          events_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          eventsBuilder_.clear();
        }
        firstTxid_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        lastTxid_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        syncTxid_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        if (batchBuilder_ == null) {
          batch_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
        } else {
          batchBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.internal_static_hadoop_hdfs_EventsListProto_descriptor;
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto getDefaultInstanceForType() {
        return org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto.getDefaultInstance();
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto build() {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto buildPartial() {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto result = new org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (eventsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            events_ = java.util.Collections.unmodifiableList(events_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.events_ = events_;
        } else {
          result.events_ = eventsBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000001;
        }
        result.firstTxid_ = firstTxid_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000002;
        }
        result.lastTxid_ = lastTxid_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000004;
        }
        result.syncTxid_ = syncTxid_;
        if (batchBuilder_ == null) {
          if (((bitField0_ & 0x00000010) == 0x00000010)) {
            batch_ = java.util.Collections.unmodifiableList(batch_);
            bitField0_ = (bitField0_ & ~0x00000010);
          }
          result.batch_ = batch_;
        } else {
          result.batch_ = batchBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto) {
          return mergeFrom((org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto other) {
        if (other == org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto.getDefaultInstance()) return this;
        if (eventsBuilder_ == null) {
          if (!other.events_.isEmpty()) {
            if (events_.isEmpty()) {
              events_ = other.events_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureEventsIsMutable();
              events_.addAll(other.events_);
            }
            onChanged();
          }
        } else {
          if (!other.events_.isEmpty()) {
            if (eventsBuilder_.isEmpty()) {
              eventsBuilder_.dispose();
              eventsBuilder_ = null;
              events_ = other.events_;
              bitField0_ = (bitField0_ & ~0x00000001);
              eventsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getEventsFieldBuilder() : null;
            } else {
              eventsBuilder_.addAllMessages(other.events_);
            }
          }
        }
        if (other.hasFirstTxid()) {
          setFirstTxid(other.getFirstTxid());
        }
        if (other.hasLastTxid()) {
          setLastTxid(other.getLastTxid());
        }
        if (other.hasSyncTxid()) {
          setSyncTxid(other.getSyncTxid());
        }
        if (batchBuilder_ == null) {
          if (!other.batch_.isEmpty()) {
            if (batch_.isEmpty()) {
              batch_ = other.batch_;
              bitField0_ = (bitField0_ & ~0x00000010);
            } else {
              ensureBatchIsMutable();
              batch_.addAll(other.batch_);
            }
            onChanged();
          }
        } else {
          if (!other.batch_.isEmpty()) {
            if (batchBuilder_.isEmpty()) {
              batchBuilder_.dispose();
              batchBuilder_ = null;
              batch_ = other.batch_;
              bitField0_ = (bitField0_ & ~0x00000010);
              batchBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getBatchFieldBuilder() : null;
            } else {
              batchBuilder_.addAllMessages(other.batch_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasFirstTxid()) {
          
          return false;
        }
        if (!hasLastTxid()) {
          
          return false;
        }
        if (!hasSyncTxid()) {
          
          return false;
        }
        for (int i = 0; i < getEventsCount(); i++) {
          if (!getEvents(i).isInitialized()) {
            
            return false;
          }
        }
        for (int i = 0; i < getBatchCount(); i++) {
          if (!getBatch(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventsListProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.hdfs.EventProto events = 1;
      private java.util.List<org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto> events_ =
        java.util.Collections.emptyList();
      private void ensureEventsIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          events_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto>(events_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.Builder, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProtoOrBuilder> eventsBuilder_;

      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
       *
       * <pre>
       * deprecated
       * </pre>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto> getEventsList() {
        if (eventsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(events_);
        } else {
          return eventsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
       *
       * <pre>
       * deprecated
       * </pre>
       */
      public int getEventsCount() {
        if (eventsBuilder_ == null) {
          return events_.size();
        } else {
          return eventsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
       *
       * <pre>
       * deprecated
       * </pre>
       */
      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto getEvents(int index) {
        if (eventsBuilder_ == null) {
          return events_.get(index);
        } else {
          return eventsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
       *
       * <pre>
       * deprecated
       * </pre>
       */
      public Builder setEvents(
          int index, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto value) {
        if (eventsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureEventsIsMutable();
          events_.set(index, value);
          onChanged();
        } else {
          eventsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
       *
       * <pre>
       * deprecated
       * </pre>
       */
      public Builder setEvents(
          int index, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.Builder builderForValue) {
        if (eventsBuilder_ == null) {
          ensureEventsIsMutable();
          events_.set(index, builderForValue.build());
          onChanged();
        } else {
          eventsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
       *
       * <pre>
       * deprecated
       * </pre>
       */
      public Builder addEvents(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto value) {
        if (eventsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureEventsIsMutable();
          events_.add(value);
          onChanged();
        } else {
          eventsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
       *
       * <pre>
       * deprecated
       * </pre>
       */
      public Builder addEvents(
          int index, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto value) {
        if (eventsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureEventsIsMutable();
          events_.add(index, value);
          onChanged();
        } else {
          eventsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
       *
       * <pre>
       * deprecated
       * </pre>
       */
      public Builder addEvents(
          org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.Builder builderForValue) {
        if (eventsBuilder_ == null) {
          ensureEventsIsMutable();
          events_.add(builderForValue.build());
          onChanged();
        } else {
          eventsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
       *
       * <pre>
       * deprecated
       * </pre>
       */
      public Builder addEvents(
          int index, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.Builder builderForValue) {
        if (eventsBuilder_ == null) {
          ensureEventsIsMutable();
          events_.add(index, builderForValue.build());
          onChanged();
        } else {
          eventsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
       *
       * <pre>
       * deprecated
       * </pre>
       */
      public Builder addAllEvents(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto> values) {
        if (eventsBuilder_ == null) {
          ensureEventsIsMutable();
          super.addAll(values, events_);
          onChanged();
        } else {
          eventsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
       *
       * <pre>
       * deprecated
       * </pre>
       */
      public Builder clearEvents() {
        if (eventsBuilder_ == null) {
          events_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          eventsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
       *
       * <pre>
       * deprecated
       * </pre>
       */
      public Builder removeEvents(int index) {
        if (eventsBuilder_ == null) {
          ensureEventsIsMutable();
          events_.remove(index);
          onChanged();
        } else {
          eventsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
       *
       * <pre>
       * deprecated
       * </pre>
       */
      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.Builder getEventsBuilder(
          int index) {
        return getEventsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
       *
       * <pre>
       * deprecated
       * </pre>
       */
      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProtoOrBuilder getEventsOrBuilder(
          int index) {
        if (eventsBuilder_ == null) {
          return events_.get(index);  } else {
          return eventsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
       *
       * <pre>
       * deprecated
       * </pre>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProtoOrBuilder> 
           getEventsOrBuilderList() {
        if (eventsBuilder_ != null) {
          return eventsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(events_);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
       *
       * <pre>
       * deprecated
       * </pre>
       */
      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.Builder addEventsBuilder() {
        return getEventsFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
       *
       * <pre>
       * deprecated
       * </pre>
       */
      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.Builder addEventsBuilder(
          int index) {
        return getEventsFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.hdfs.EventProto events = 1;</code>
       *
       * <pre>
       * deprecated
       * </pre>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.Builder> 
           getEventsBuilderList() {
        return getEventsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.Builder, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProtoOrBuilder> 
          getEventsFieldBuilder() {
        if (eventsBuilder_ == null) {
          eventsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProto.Builder, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventProtoOrBuilder>(
                  events_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          events_ = null;
        }
        return eventsBuilder_;
      }

      // required int64 firstTxid = 2;
      private long firstTxid_ ;
      /**
       * <code>required int64 firstTxid = 2;</code>
       */
      public boolean hasFirstTxid() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required int64 firstTxid = 2;</code>
       */
      public long getFirstTxid() {
        return firstTxid_;
      }
      /**
       * <code>required int64 firstTxid = 2;</code>
       */
      public Builder setFirstTxid(long value) {
        bitField0_ |= 0x00000002;
        firstTxid_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required int64 firstTxid = 2;</code>
       */
      public Builder clearFirstTxid() {
        bitField0_ = (bitField0_ & ~0x00000002);
        firstTxid_ = 0L;
        onChanged();
        return this;
      }

      // required int64 lastTxid = 3;
      private long lastTxid_ ;
      /**
       * <code>required int64 lastTxid = 3;</code>
       */
      public boolean hasLastTxid() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>required int64 lastTxid = 3;</code>
       */
      public long getLastTxid() {
        return lastTxid_;
      }
      /**
       * <code>required int64 lastTxid = 3;</code>
       */
      public Builder setLastTxid(long value) {
        bitField0_ |= 0x00000004;
        lastTxid_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required int64 lastTxid = 3;</code>
       */
      public Builder clearLastTxid() {
        bitField0_ = (bitField0_ & ~0x00000004);
        lastTxid_ = 0L;
        onChanged();
        return this;
      }

      // required int64 syncTxid = 4;
      private long syncTxid_ ;
      /**
       * <code>required int64 syncTxid = 4;</code>
       */
      public boolean hasSyncTxid() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>required int64 syncTxid = 4;</code>
       */
      public long getSyncTxid() {
        return syncTxid_;
      }
      /**
       * <code>required int64 syncTxid = 4;</code>
       */
      public Builder setSyncTxid(long value) {
        bitField0_ |= 0x00000008;
        syncTxid_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required int64 syncTxid = 4;</code>
       */
      public Builder clearSyncTxid() {
        bitField0_ = (bitField0_ & ~0x00000008);
        syncTxid_ = 0L;
        onChanged();
        return this;
      }

      // repeated .hadoop.hdfs.EventBatchProto batch = 5;
      private java.util.List<org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto> batch_ =
        java.util.Collections.emptyList();
      private void ensureBatchIsMutable() {
        if (!((bitField0_ & 0x00000010) == 0x00000010)) {
          batch_ = new java.util.ArrayList<org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto>(batch_);
          bitField0_ |= 0x00000010;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto.Builder, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProtoOrBuilder> batchBuilder_;

      /**
       * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto> getBatchList() {
        if (batchBuilder_ == null) {
          return java.util.Collections.unmodifiableList(batch_);
        } else {
          return batchBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
       */
      public int getBatchCount() {
        if (batchBuilder_ == null) {
          return batch_.size();
        } else {
          return batchBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto getBatch(int index) {
        if (batchBuilder_ == null) {
          return batch_.get(index);
        } else {
          return batchBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
       */
      public Builder setBatch(
          int index, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto value) {
        if (batchBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBatchIsMutable();
          batch_.set(index, value);
          onChanged();
        } else {
          batchBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
       */
      public Builder setBatch(
          int index, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto.Builder builderForValue) {
        if (batchBuilder_ == null) {
          ensureBatchIsMutable();
          batch_.set(index, builderForValue.build());
          onChanged();
        } else {
          batchBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
       */
      public Builder addBatch(org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto value) {
        if (batchBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBatchIsMutable();
          batch_.add(value);
          onChanged();
        } else {
          batchBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
       */
      public Builder addBatch(
          int index, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto value) {
        if (batchBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBatchIsMutable();
          batch_.add(index, value);
          onChanged();
        } else {
          batchBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
       */
      public Builder addBatch(
          org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto.Builder builderForValue) {
        if (batchBuilder_ == null) {
          ensureBatchIsMutable();
          batch_.add(builderForValue.build());
          onChanged();
        } else {
          batchBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
       */
      public Builder addBatch(
          int index, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto.Builder builderForValue) {
        if (batchBuilder_ == null) {
          ensureBatchIsMutable();
          batch_.add(index, builderForValue.build());
          onChanged();
        } else {
          batchBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
       */
      public Builder addAllBatch(
          java.lang.Iterable<? extends org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto> values) {
        if (batchBuilder_ == null) {
          ensureBatchIsMutable();
          super.addAll(values, batch_);
          onChanged();
        } else {
          batchBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
       */
      public Builder clearBatch() {
        if (batchBuilder_ == null) {
          batch_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
          onChanged();
        } else {
          batchBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
       */
      public Builder removeBatch(int index) {
        if (batchBuilder_ == null) {
          ensureBatchIsMutable();
          batch_.remove(index);
          onChanged();
        } else {
          batchBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto.Builder getBatchBuilder(
          int index) {
        return getBatchFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProtoOrBuilder getBatchOrBuilder(
          int index) {
        if (batchBuilder_ == null) {
          return batch_.get(index);  } else {
          return batchBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
       */
      public java.util.List<? extends org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProtoOrBuilder> 
           getBatchOrBuilderList() {
        if (batchBuilder_ != null) {
          return batchBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(batch_);
        }
      }
      /**
       * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto.Builder addBatchBuilder() {
        return getBatchFieldBuilder().addBuilder(
            org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
       */
      public org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto.Builder addBatchBuilder(
          int index) {
        return getBatchFieldBuilder().addBuilder(
            index, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.hdfs.EventBatchProto batch = 5;</code>
       */
      public java.util.List<org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto.Builder> 
           getBatchBuilderList() {
        return getBatchFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto.Builder, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProtoOrBuilder> 
          getBatchFieldBuilder() {
        if (batchBuilder_ == null) {
          batchBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProto.Builder, org.apache.hadoop.hdfs.protocol.proto.InotifyProtos.EventBatchProtoOrBuilder>(
                  batch_,
                  ((bitField0_ & 0x00000010) == 0x00000010),
                  getParentForChildren(),
                  isClean());
          batch_ = null;
        }
        return batchBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.hdfs.EventsListProto)
    }

    static {
      defaultInstance = new EventsListProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.hdfs.EventsListProto)
  }

  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_EventProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_EventProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_EventBatchProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_EventBatchProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_CreateEventProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_CreateEventProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_CloseEventProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_CloseEventProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_TruncateEventProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_TruncateEventProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_AppendEventProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_AppendEventProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_RenameEventProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_RenameEventProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_MetadataUpdateEventProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_MetadataUpdateEventProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_UnlinkEventProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_UnlinkEventProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_hdfs_EventsListProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_hdfs_EventsListProto_fieldAccessorTable;

  public static com.google.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static com.google.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n\rinotify.proto\022\013hadoop.hdfs\032\tacl.proto\032" +
      "\013xattr.proto\032\nhdfs.proto\"D\n\nEventProto\022$" +
      "\n\004type\030\001 \002(\0162\026.hadoop.hdfs.EventType\022\020\n\010" +
      "contents\030\002 \002(\014\"H\n\017EventBatchProto\022\014\n\004txi" +
      "d\030\001 \002(\003\022\'\n\006events\030\002 \003(\0132\027.hadoop.hdfs.Ev" +
      "entProto\"\206\002\n\020CreateEventProto\022$\n\004type\030\001 " +
      "\002(\0162\026.hadoop.hdfs.INodeType\022\014\n\004path\030\002 \002(" +
      "\t\022\r\n\005ctime\030\003 \002(\003\022\021\n\townerName\030\004 \002(\t\022\021\n\tg" +
      "roupName\030\005 \002(\t\022-\n\005perms\030\006 \002(\0132\036.hadoop.h" +
      "dfs.FsPermissionProto\022\023\n\013replication\030\007 \001",
      "(\005\022\025\n\rsymlinkTarget\030\010 \001(\t\022\021\n\toverwrite\030\t" +
      " \001(\010\022\033\n\020defaultBlockSize\030\n \001(\003:\0010\"D\n\017Clo" +
      "seEventProto\022\014\n\004path\030\001 \002(\t\022\020\n\010fileSize\030\002" +
      " \002(\003\022\021\n\ttimestamp\030\003 \002(\003\"G\n\022TruncateEvent" +
      "Proto\022\014\n\004path\030\001 \002(\t\022\020\n\010fileSize\030\002 \002(\003\022\021\n" +
      "\ttimestamp\030\003 \002(\003\"9\n\020AppendEventProto\022\014\n\004" +
      "path\030\001 \002(\t\022\027\n\010newBlock\030\002 \001(\010:\005false\"H\n\020R" +
      "enameEventProto\022\017\n\007srcPath\030\001 \002(\t\022\020\n\010dest" +
      "Path\030\002 \002(\t\022\021\n\ttimestamp\030\003 \002(\003\"\311\002\n\030Metada" +
      "taUpdateEventProto\022\014\n\004path\030\001 \002(\t\022-\n\004type",
      "\030\002 \002(\0162\037.hadoop.hdfs.MetadataUpdateType\022" +
      "\r\n\005mtime\030\003 \001(\003\022\r\n\005atime\030\004 \001(\003\022\023\n\013replica" +
      "tion\030\005 \001(\005\022\021\n\townerName\030\006 \001(\t\022\021\n\tgroupNa" +
      "me\030\007 \001(\t\022-\n\005perms\030\010 \001(\0132\036.hadoop.hdfs.Fs" +
      "PermissionProto\022(\n\004acls\030\t \003(\0132\032.hadoop.h" +
      "dfs.AclEntryProto\022\'\n\006xAttrs\030\n \003(\0132\027.hado" +
      "op.hdfs.XAttrProto\022\025\n\rxAttrsRemoved\030\013 \001(" +
      "\010\"3\n\020UnlinkEventProto\022\014\n\004path\030\001 \002(\t\022\021\n\tt" +
      "imestamp\030\002 \002(\003\"\236\001\n\017EventsListProto\022\'\n\006ev" +
      "ents\030\001 \003(\0132\027.hadoop.hdfs.EventProto\022\021\n\tf",
      "irstTxid\030\002 \002(\003\022\020\n\010lastTxid\030\003 \002(\003\022\020\n\010sync" +
      "Txid\030\004 \002(\003\022+\n\005batch\030\005 \003(\0132\034.hadoop.hdfs." +
      "EventBatchProto*\214\001\n\tEventType\022\020\n\014EVENT_C" +
      "REATE\020\000\022\017\n\013EVENT_CLOSE\020\001\022\020\n\014EVENT_APPEND" +
      "\020\002\022\020\n\014EVENT_RENAME\020\003\022\022\n\016EVENT_METADATA\020\004" +
      "\022\020\n\014EVENT_UNLINK\020\005\022\022\n\016EVENT_TRUNCATE\020\006*F" +
      "\n\tINodeType\022\017\n\013I_TYPE_FILE\020\000\022\024\n\020I_TYPE_D" +
      "IRECTORY\020\001\022\022\n\016I_TYPE_SYMLINK\020\002*\230\001\n\022Metad" +
      "ataUpdateType\022\023\n\017META_TYPE_TIMES\020\000\022\031\n\025ME" +
      "TA_TYPE_REPLICATION\020\001\022\023\n\017META_TYPE_OWNER",
      "\020\002\022\023\n\017META_TYPE_PERMS\020\003\022\022\n\016META_TYPE_ACL" +
      "S\020\004\022\024\n\020META_TYPE_XATTRS\020\005B9\n%org.apache." +
      "hadoop.hdfs.protocol.protoB\rInotifyProto" +
      "s\240\001\001"
    };
    com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
      new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
        public com.google.protobuf.ExtensionRegistry assignDescriptors(
            com.google.protobuf.Descriptors.FileDescriptor root) {
          descriptor = root;
          internal_static_hadoop_hdfs_EventProto_descriptor =
            getDescriptor().getMessageTypes().get(0);
          internal_static_hadoop_hdfs_EventProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_EventProto_descriptor,
              new java.lang.String[] { "Type", "Contents", });
          internal_static_hadoop_hdfs_EventBatchProto_descriptor =
            getDescriptor().getMessageTypes().get(1);
          internal_static_hadoop_hdfs_EventBatchProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_EventBatchProto_descriptor,
              new java.lang.String[] { "Txid", "Events", });
          internal_static_hadoop_hdfs_CreateEventProto_descriptor =
            getDescriptor().getMessageTypes().get(2);
          internal_static_hadoop_hdfs_CreateEventProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_CreateEventProto_descriptor,
              new java.lang.String[] { "Type", "Path", "Ctime", "OwnerName", "GroupName", "Perms", "Replication", "SymlinkTarget", "Overwrite", "DefaultBlockSize", });
          internal_static_hadoop_hdfs_CloseEventProto_descriptor =
            getDescriptor().getMessageTypes().get(3);
          internal_static_hadoop_hdfs_CloseEventProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_CloseEventProto_descriptor,
              new java.lang.String[] { "Path", "FileSize", "Timestamp", });
          internal_static_hadoop_hdfs_TruncateEventProto_descriptor =
            getDescriptor().getMessageTypes().get(4);
          internal_static_hadoop_hdfs_TruncateEventProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_TruncateEventProto_descriptor,
              new java.lang.String[] { "Path", "FileSize", "Timestamp", });
          internal_static_hadoop_hdfs_AppendEventProto_descriptor =
            getDescriptor().getMessageTypes().get(5);
          internal_static_hadoop_hdfs_AppendEventProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_AppendEventProto_descriptor,
              new java.lang.String[] { "Path", "NewBlock", });
          internal_static_hadoop_hdfs_RenameEventProto_descriptor =
            getDescriptor().getMessageTypes().get(6);
          internal_static_hadoop_hdfs_RenameEventProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_RenameEventProto_descriptor,
              new java.lang.String[] { "SrcPath", "DestPath", "Timestamp", });
          internal_static_hadoop_hdfs_MetadataUpdateEventProto_descriptor =
            getDescriptor().getMessageTypes().get(7);
          internal_static_hadoop_hdfs_MetadataUpdateEventProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_MetadataUpdateEventProto_descriptor,
              new java.lang.String[] { "Path", "Type", "Mtime", "Atime", "Replication", "OwnerName", "GroupName", "Perms", "Acls", "XAttrs", "XAttrsRemoved", });
          internal_static_hadoop_hdfs_UnlinkEventProto_descriptor =
            getDescriptor().getMessageTypes().get(8);
          internal_static_hadoop_hdfs_UnlinkEventProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_UnlinkEventProto_descriptor,
              new java.lang.String[] { "Path", "Timestamp", });
          internal_static_hadoop_hdfs_EventsListProto_descriptor =
            getDescriptor().getMessageTypes().get(9);
          internal_static_hadoop_hdfs_EventsListProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_hdfs_EventsListProto_descriptor,
              new java.lang.String[] { "Events", "FirstTxid", "LastTxid", "SyncTxid", "Batch", });
          return null;
        }
      };
    com.google.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new com.google.protobuf.Descriptors.FileDescriptor[] {
          org.apache.hadoop.hdfs.protocol.proto.AclProtos.getDescriptor(),
          org.apache.hadoop.hdfs.protocol.proto.XAttrProtos.getDescriptor(),
          org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.getDescriptor(),
        }, assigner);
  }

  // @@protoc_insertion_point(outer_class_scope)
}
