// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: yarn_service_protos.proto

package org.apache.hadoop.yarn.proto;

public final class YarnServiceProtos {
  private YarnServiceProtos() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  /**
   * Protobuf enum {@code hadoop.yarn.ContainerUpdateTypeProto}
   */
  public enum ContainerUpdateTypeProto
      implements com.google.protobuf.ProtocolMessageEnum {
    /**
     * <code>INCREASE_RESOURCE = 0;</code>
     */
    INCREASE_RESOURCE(0, 0),
    /**
     * <code>DECREASE_RESOURCE = 1;</code>
     */
    DECREASE_RESOURCE(1, 1),
    ;

    /**
     * <code>INCREASE_RESOURCE = 0;</code>
     */
    public static final int INCREASE_RESOURCE_VALUE = 0;
    /**
     * <code>DECREASE_RESOURCE = 1;</code>
     */
    public static final int DECREASE_RESOURCE_VALUE = 1;


    public final int getNumber() { return value; }

    public static ContainerUpdateTypeProto valueOf(int value) {
      switch (value) {
        case 0: return INCREASE_RESOURCE;
        case 1: return DECREASE_RESOURCE;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<ContainerUpdateTypeProto>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static com.google.protobuf.Internal.EnumLiteMap<ContainerUpdateTypeProto>
        internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<ContainerUpdateTypeProto>() {
            public ContainerUpdateTypeProto findValueByNumber(int number) {
              return ContainerUpdateTypeProto.valueOf(number);
            }
          };

    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(index);
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.getDescriptor().getEnumTypes().get(0);
    }

    private static final ContainerUpdateTypeProto[] VALUES = values();

    public static ContainerUpdateTypeProto valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }

    private final int index;
    private final int value;

    private ContainerUpdateTypeProto(int index, int value) {
      this.index = index;
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:hadoop.yarn.ContainerUpdateTypeProto)
  }

  /**
   * Protobuf enum {@code hadoop.yarn.SchedulerResourceTypes}
   */
  public enum SchedulerResourceTypes
      implements com.google.protobuf.ProtocolMessageEnum {
    /**
     * <code>MEMORY = 0;</code>
     */
    MEMORY(0, 0),
    /**
     * <code>CPU = 1;</code>
     */
    CPU(1, 1),
    ;

    /**
     * <code>MEMORY = 0;</code>
     */
    public static final int MEMORY_VALUE = 0;
    /**
     * <code>CPU = 1;</code>
     */
    public static final int CPU_VALUE = 1;


    public final int getNumber() { return value; }

    public static SchedulerResourceTypes valueOf(int value) {
      switch (value) {
        case 0: return MEMORY;
        case 1: return CPU;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<SchedulerResourceTypes>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static com.google.protobuf.Internal.EnumLiteMap<SchedulerResourceTypes>
        internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<SchedulerResourceTypes>() {
            public SchedulerResourceTypes findValueByNumber(int number) {
              return SchedulerResourceTypes.valueOf(number);
            }
          };

    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(index);
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.getDescriptor().getEnumTypes().get(1);
    }

    private static final SchedulerResourceTypes[] VALUES = values();

    public static SchedulerResourceTypes valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }

    private final int index;
    private final int value;

    private SchedulerResourceTypes(int index, int value) {
      this.index = index;
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:hadoop.yarn.SchedulerResourceTypes)
  }

  /**
   * Protobuf enum {@code hadoop.yarn.ApplicationsRequestScopeProto}
   */
  public enum ApplicationsRequestScopeProto
      implements com.google.protobuf.ProtocolMessageEnum {
    /**
     * <code>ALL = 0;</code>
     */
    ALL(0, 0),
    /**
     * <code>VIEWABLE = 1;</code>
     */
    VIEWABLE(1, 1),
    /**
     * <code>OWN = 2;</code>
     */
    OWN(2, 2),
    ;

    /**
     * <code>ALL = 0;</code>
     */
    public static final int ALL_VALUE = 0;
    /**
     * <code>VIEWABLE = 1;</code>
     */
    public static final int VIEWABLE_VALUE = 1;
    /**
     * <code>OWN = 2;</code>
     */
    public static final int OWN_VALUE = 2;


    public final int getNumber() { return value; }

    public static ApplicationsRequestScopeProto valueOf(int value) {
      switch (value) {
        case 0: return ALL;
        case 1: return VIEWABLE;
        case 2: return OWN;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<ApplicationsRequestScopeProto>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static com.google.protobuf.Internal.EnumLiteMap<ApplicationsRequestScopeProto>
        internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<ApplicationsRequestScopeProto>() {
            public ApplicationsRequestScopeProto findValueByNumber(int number) {
              return ApplicationsRequestScopeProto.valueOf(number);
            }
          };

    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(index);
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.getDescriptor().getEnumTypes().get(2);
    }

    private static final ApplicationsRequestScopeProto[] VALUES = values();

    public static ApplicationsRequestScopeProto valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }

    private final int index;
    private final int value;

    private ApplicationsRequestScopeProto(int index, int value) {
      this.index = index;
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:hadoop.yarn.ApplicationsRequestScopeProto)
  }

  public interface RegisterApplicationMasterRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional string host = 1;
    /**
     * <code>optional string host = 1;</code>
     */
    boolean hasHost();
    /**
     * <code>optional string host = 1;</code>
     */
    java.lang.String getHost();
    /**
     * <code>optional string host = 1;</code>
     */
    com.google.protobuf.ByteString
        getHostBytes();

    // optional int32 rpc_port = 2;
    /**
     * <code>optional int32 rpc_port = 2;</code>
     */
    boolean hasRpcPort();
    /**
     * <code>optional int32 rpc_port = 2;</code>
     */
    int getRpcPort();

    // optional string tracking_url = 3;
    /**
     * <code>optional string tracking_url = 3;</code>
     */
    boolean hasTrackingUrl();
    /**
     * <code>optional string tracking_url = 3;</code>
     */
    java.lang.String getTrackingUrl();
    /**
     * <code>optional string tracking_url = 3;</code>
     */
    com.google.protobuf.ByteString
        getTrackingUrlBytes();
  }
  /**
   * Protobuf type {@code hadoop.yarn.RegisterApplicationMasterRequestProto}
   *
   * <pre>
   *&#47;///////////////////////////////////////////////////
   * ///// AM_RM_Protocol ///////////////////////////////
   * ////////////////////////////////////////////////////
   * </pre>
   */
  public static final class RegisterApplicationMasterRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements RegisterApplicationMasterRequestProtoOrBuilder {
    // Use RegisterApplicationMasterRequestProto.newBuilder() to construct.
    private RegisterApplicationMasterRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private RegisterApplicationMasterRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final RegisterApplicationMasterRequestProto defaultInstance;
    public static RegisterApplicationMasterRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public RegisterApplicationMasterRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private RegisterApplicationMasterRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              host_ = input.readBytes();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              rpcPort_ = input.readInt32();
              break;
            }
            case 26: {
              bitField0_ |= 0x00000004;
              trackingUrl_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RegisterApplicationMasterRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RegisterApplicationMasterRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<RegisterApplicationMasterRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<RegisterApplicationMasterRequestProto>() {
      public RegisterApplicationMasterRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RegisterApplicationMasterRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<RegisterApplicationMasterRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional string host = 1;
    public static final int HOST_FIELD_NUMBER = 1;
    private java.lang.Object host_;
    /**
     * <code>optional string host = 1;</code>
     */
    public boolean hasHost() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional string host = 1;</code>
     */
    public java.lang.String getHost() {
      java.lang.Object ref = host_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          host_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string host = 1;</code>
     */
    public com.google.protobuf.ByteString
        getHostBytes() {
      java.lang.Object ref = host_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        host_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional int32 rpc_port = 2;
    public static final int RPC_PORT_FIELD_NUMBER = 2;
    private int rpcPort_;
    /**
     * <code>optional int32 rpc_port = 2;</code>
     */
    public boolean hasRpcPort() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional int32 rpc_port = 2;</code>
     */
    public int getRpcPort() {
      return rpcPort_;
    }

    // optional string tracking_url = 3;
    public static final int TRACKING_URL_FIELD_NUMBER = 3;
    private java.lang.Object trackingUrl_;
    /**
     * <code>optional string tracking_url = 3;</code>
     */
    public boolean hasTrackingUrl() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional string tracking_url = 3;</code>
     */
    public java.lang.String getTrackingUrl() {
      java.lang.Object ref = trackingUrl_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          trackingUrl_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string tracking_url = 3;</code>
     */
    public com.google.protobuf.ByteString
        getTrackingUrlBytes() {
      java.lang.Object ref = trackingUrl_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        trackingUrl_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private void initFields() {
      host_ = "";
      rpcPort_ = 0;
      trackingUrl_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getHostBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeInt32(2, rpcPort_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeBytes(3, getTrackingUrlBytes());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getHostBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, rpcPort_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(3, getTrackingUrlBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto) obj;

      boolean result = true;
      result = result && (hasHost() == other.hasHost());
      if (hasHost()) {
        result = result && getHost()
            .equals(other.getHost());
      }
      result = result && (hasRpcPort() == other.hasRpcPort());
      if (hasRpcPort()) {
        result = result && (getRpcPort()
            == other.getRpcPort());
      }
      result = result && (hasTrackingUrl() == other.hasTrackingUrl());
      if (hasTrackingUrl()) {
        result = result && getTrackingUrl()
            .equals(other.getTrackingUrl());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasHost()) {
        hash = (37 * hash) + HOST_FIELD_NUMBER;
        hash = (53 * hash) + getHost().hashCode();
      }
      if (hasRpcPort()) {
        hash = (37 * hash) + RPC_PORT_FIELD_NUMBER;
        hash = (53 * hash) + getRpcPort();
      }
      if (hasTrackingUrl()) {
        hash = (37 * hash) + TRACKING_URL_FIELD_NUMBER;
        hash = (53 * hash) + getTrackingUrl().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.RegisterApplicationMasterRequestProto}
     *
     * <pre>
     *&#47;///////////////////////////////////////////////////
     * ///// AM_RM_Protocol ///////////////////////////////
     * ////////////////////////////////////////////////////
     * </pre>
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RegisterApplicationMasterRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RegisterApplicationMasterRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        host_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        rpcPort_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        trackingUrl_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RegisterApplicationMasterRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.host_ = host_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.rpcPort_ = rpcPort_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.trackingUrl_ = trackingUrl_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto.getDefaultInstance()) return this;
        if (other.hasHost()) {
          bitField0_ |= 0x00000001;
          host_ = other.host_;
          onChanged();
        }
        if (other.hasRpcPort()) {
          setRpcPort(other.getRpcPort());
        }
        if (other.hasTrackingUrl()) {
          bitField0_ |= 0x00000004;
          trackingUrl_ = other.trackingUrl_;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional string host = 1;
      private java.lang.Object host_ = "";
      /**
       * <code>optional string host = 1;</code>
       */
      public boolean hasHost() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional string host = 1;</code>
       */
      public java.lang.String getHost() {
        java.lang.Object ref = host_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          host_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string host = 1;</code>
       */
      public com.google.protobuf.ByteString
          getHostBytes() {
        java.lang.Object ref = host_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          host_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string host = 1;</code>
       */
      public Builder setHost(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        host_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string host = 1;</code>
       */
      public Builder clearHost() {
        bitField0_ = (bitField0_ & ~0x00000001);
        host_ = getDefaultInstance().getHost();
        onChanged();
        return this;
      }
      /**
       * <code>optional string host = 1;</code>
       */
      public Builder setHostBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        host_ = value;
        onChanged();
        return this;
      }

      // optional int32 rpc_port = 2;
      private int rpcPort_ ;
      /**
       * <code>optional int32 rpc_port = 2;</code>
       */
      public boolean hasRpcPort() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional int32 rpc_port = 2;</code>
       */
      public int getRpcPort() {
        return rpcPort_;
      }
      /**
       * <code>optional int32 rpc_port = 2;</code>
       */
      public Builder setRpcPort(int value) {
        bitField0_ |= 0x00000002;
        rpcPort_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 rpc_port = 2;</code>
       */
      public Builder clearRpcPort() {
        bitField0_ = (bitField0_ & ~0x00000002);
        rpcPort_ = 0;
        onChanged();
        return this;
      }

      // optional string tracking_url = 3;
      private java.lang.Object trackingUrl_ = "";
      /**
       * <code>optional string tracking_url = 3;</code>
       */
      public boolean hasTrackingUrl() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional string tracking_url = 3;</code>
       */
      public java.lang.String getTrackingUrl() {
        java.lang.Object ref = trackingUrl_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          trackingUrl_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string tracking_url = 3;</code>
       */
      public com.google.protobuf.ByteString
          getTrackingUrlBytes() {
        java.lang.Object ref = trackingUrl_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          trackingUrl_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string tracking_url = 3;</code>
       */
      public Builder setTrackingUrl(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        trackingUrl_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string tracking_url = 3;</code>
       */
      public Builder clearTrackingUrl() {
        bitField0_ = (bitField0_ & ~0x00000004);
        trackingUrl_ = getDefaultInstance().getTrackingUrl();
        onChanged();
        return this;
      }
      /**
       * <code>optional string tracking_url = 3;</code>
       */
      public Builder setTrackingUrlBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        trackingUrl_ = value;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.RegisterApplicationMasterRequestProto)
    }

    static {
      defaultInstance = new RegisterApplicationMasterRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.RegisterApplicationMasterRequestProto)
  }

  public interface RegisterApplicationMasterResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ResourceProto maximumCapability = 1;
    /**
     * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
     */
    boolean hasMaximumCapability();
    /**
     * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getMaximumCapability();
    /**
     * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getMaximumCapabilityOrBuilder();

    // optional bytes client_to_am_token_master_key = 2;
    /**
     * <code>optional bytes client_to_am_token_master_key = 2;</code>
     */
    boolean hasClientToAmTokenMasterKey();
    /**
     * <code>optional bytes client_to_am_token_master_key = 2;</code>
     */
    com.google.protobuf.ByteString getClientToAmTokenMasterKey();

    // repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;
    /**
     * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto> 
        getApplicationACLsList();
    /**
     * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto getApplicationACLs(int index);
    /**
     * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
     */
    int getApplicationACLsCount();
    /**
     * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProtoOrBuilder> 
        getApplicationACLsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProtoOrBuilder getApplicationACLsOrBuilder(
        int index);

    // repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;
    /**
     * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> 
        getContainersFromPreviousAttemptsList();
    /**
     * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto getContainersFromPreviousAttempts(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
     */
    int getContainersFromPreviousAttemptsCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
        getContainersFromPreviousAttemptsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder getContainersFromPreviousAttemptsOrBuilder(
        int index);

    // optional string queue = 5;
    /**
     * <code>optional string queue = 5;</code>
     */
    boolean hasQueue();
    /**
     * <code>optional string queue = 5;</code>
     */
    java.lang.String getQueue();
    /**
     * <code>optional string queue = 5;</code>
     */
    com.google.protobuf.ByteString
        getQueueBytes();

    // repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto> 
        getNmTokensFromPreviousAttemptsList();
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto getNmTokensFromPreviousAttempts(int index);
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
     */
    int getNmTokensFromPreviousAttemptsCount();
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder> 
        getNmTokensFromPreviousAttemptsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder getNmTokensFromPreviousAttemptsOrBuilder(
        int index);

    // repeated .hadoop.yarn.SchedulerResourceTypes scheduler_resource_types = 7;
    /**
     * <code>repeated .hadoop.yarn.SchedulerResourceTypes scheduler_resource_types = 7;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.SchedulerResourceTypes> getSchedulerResourceTypesList();
    /**
     * <code>repeated .hadoop.yarn.SchedulerResourceTypes scheduler_resource_types = 7;</code>
     */
    int getSchedulerResourceTypesCount();
    /**
     * <code>repeated .hadoop.yarn.SchedulerResourceTypes scheduler_resource_types = 7;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.SchedulerResourceTypes getSchedulerResourceTypes(int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.RegisterApplicationMasterResponseProto}
   */
  public static final class RegisterApplicationMasterResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements RegisterApplicationMasterResponseProtoOrBuilder {
    // Use RegisterApplicationMasterResponseProto.newBuilder() to construct.
    private RegisterApplicationMasterResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private RegisterApplicationMasterResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final RegisterApplicationMasterResponseProto defaultInstance;
    public static RegisterApplicationMasterResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public RegisterApplicationMasterResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private RegisterApplicationMasterResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = maximumCapability_.toBuilder();
              }
              maximumCapability_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(maximumCapability_);
                maximumCapability_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              clientToAmTokenMasterKey_ = input.readBytes();
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
                applicationACLs_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto>();
                mutable_bitField0_ |= 0x00000004;
              }
              applicationACLs_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.PARSER, extensionRegistry));
              break;
            }
            case 34: {
              if (!((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
                containersFromPreviousAttempts_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto>();
                mutable_bitField0_ |= 0x00000008;
              }
              containersFromPreviousAttempts_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.PARSER, extensionRegistry));
              break;
            }
            case 42: {
              bitField0_ |= 0x00000004;
              queue_ = input.readBytes();
              break;
            }
            case 50: {
              if (!((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
                nmTokensFromPreviousAttempts_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto>();
                mutable_bitField0_ |= 0x00000020;
              }
              nmTokensFromPreviousAttempts_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.PARSER, extensionRegistry));
              break;
            }
            case 56: {
              int rawValue = input.readEnum();
              org.apache.hadoop.yarn.proto.YarnServiceProtos.SchedulerResourceTypes value = org.apache.hadoop.yarn.proto.YarnServiceProtos.SchedulerResourceTypes.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(7, rawValue);
              } else {
                if (!((mutable_bitField0_ & 0x00000040) == 0x00000040)) {
                  schedulerResourceTypes_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.SchedulerResourceTypes>();
                  mutable_bitField0_ |= 0x00000040;
                }
                schedulerResourceTypes_.add(value);
              }
              break;
            }
            case 58: {
              int length = input.readRawVarint32();
              int oldLimit = input.pushLimit(length);
              while(input.getBytesUntilLimit() > 0) {
                int rawValue = input.readEnum();
                org.apache.hadoop.yarn.proto.YarnServiceProtos.SchedulerResourceTypes value = org.apache.hadoop.yarn.proto.YarnServiceProtos.SchedulerResourceTypes.valueOf(rawValue);
                if (value == null) {
                  unknownFields.mergeVarintField(7, rawValue);
                } else {
                  if (!((mutable_bitField0_ & 0x00000040) == 0x00000040)) {
                    schedulerResourceTypes_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.SchedulerResourceTypes>();
                    mutable_bitField0_ |= 0x00000040;
                  }
                  schedulerResourceTypes_.add(value);
                }
              }
              input.popLimit(oldLimit);
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
          applicationACLs_ = java.util.Collections.unmodifiableList(applicationACLs_);
        }
        if (((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
          containersFromPreviousAttempts_ = java.util.Collections.unmodifiableList(containersFromPreviousAttempts_);
        }
        if (((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
          nmTokensFromPreviousAttempts_ = java.util.Collections.unmodifiableList(nmTokensFromPreviousAttempts_);
        }
        if (((mutable_bitField0_ & 0x00000040) == 0x00000040)) {
          schedulerResourceTypes_ = java.util.Collections.unmodifiableList(schedulerResourceTypes_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RegisterApplicationMasterResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RegisterApplicationMasterResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<RegisterApplicationMasterResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<RegisterApplicationMasterResponseProto>() {
      public RegisterApplicationMasterResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RegisterApplicationMasterResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<RegisterApplicationMasterResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ResourceProto maximumCapability = 1;
    public static final int MAXIMUMCAPABILITY_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto maximumCapability_;
    /**
     * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
     */
    public boolean hasMaximumCapability() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getMaximumCapability() {
      return maximumCapability_;
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getMaximumCapabilityOrBuilder() {
      return maximumCapability_;
    }

    // optional bytes client_to_am_token_master_key = 2;
    public static final int CLIENT_TO_AM_TOKEN_MASTER_KEY_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString clientToAmTokenMasterKey_;
    /**
     * <code>optional bytes client_to_am_token_master_key = 2;</code>
     */
    public boolean hasClientToAmTokenMasterKey() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional bytes client_to_am_token_master_key = 2;</code>
     */
    public com.google.protobuf.ByteString getClientToAmTokenMasterKey() {
      return clientToAmTokenMasterKey_;
    }

    // repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;
    public static final int APPLICATION_ACLS_FIELD_NUMBER = 3;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto> applicationACLs_;
    /**
     * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto> getApplicationACLsList() {
      return applicationACLs_;
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProtoOrBuilder> 
        getApplicationACLsOrBuilderList() {
      return applicationACLs_;
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
     */
    public int getApplicationACLsCount() {
      return applicationACLs_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto getApplicationACLs(int index) {
      return applicationACLs_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProtoOrBuilder getApplicationACLsOrBuilder(
        int index) {
      return applicationACLs_.get(index);
    }

    // repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;
    public static final int CONTAINERS_FROM_PREVIOUS_ATTEMPTS_FIELD_NUMBER = 4;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> containersFromPreviousAttempts_;
    /**
     * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> getContainersFromPreviousAttemptsList() {
      return containersFromPreviousAttempts_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
        getContainersFromPreviousAttemptsOrBuilderList() {
      return containersFromPreviousAttempts_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
     */
    public int getContainersFromPreviousAttemptsCount() {
      return containersFromPreviousAttempts_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto getContainersFromPreviousAttempts(int index) {
      return containersFromPreviousAttempts_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder getContainersFromPreviousAttemptsOrBuilder(
        int index) {
      return containersFromPreviousAttempts_.get(index);
    }

    // optional string queue = 5;
    public static final int QUEUE_FIELD_NUMBER = 5;
    private java.lang.Object queue_;
    /**
     * <code>optional string queue = 5;</code>
     */
    public boolean hasQueue() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional string queue = 5;</code>
     */
    public java.lang.String getQueue() {
      java.lang.Object ref = queue_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          queue_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string queue = 5;</code>
     */
    public com.google.protobuf.ByteString
        getQueueBytes() {
      java.lang.Object ref = queue_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        queue_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;
    public static final int NM_TOKENS_FROM_PREVIOUS_ATTEMPTS_FIELD_NUMBER = 6;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto> nmTokensFromPreviousAttempts_;
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto> getNmTokensFromPreviousAttemptsList() {
      return nmTokensFromPreviousAttempts_;
    }
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder> 
        getNmTokensFromPreviousAttemptsOrBuilderList() {
      return nmTokensFromPreviousAttempts_;
    }
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
     */
    public int getNmTokensFromPreviousAttemptsCount() {
      return nmTokensFromPreviousAttempts_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto getNmTokensFromPreviousAttempts(int index) {
      return nmTokensFromPreviousAttempts_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder getNmTokensFromPreviousAttemptsOrBuilder(
        int index) {
      return nmTokensFromPreviousAttempts_.get(index);
    }

    // repeated .hadoop.yarn.SchedulerResourceTypes scheduler_resource_types = 7;
    public static final int SCHEDULER_RESOURCE_TYPES_FIELD_NUMBER = 7;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.SchedulerResourceTypes> schedulerResourceTypes_;
    /**
     * <code>repeated .hadoop.yarn.SchedulerResourceTypes scheduler_resource_types = 7;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.SchedulerResourceTypes> getSchedulerResourceTypesList() {
      return schedulerResourceTypes_;
    }
    /**
     * <code>repeated .hadoop.yarn.SchedulerResourceTypes scheduler_resource_types = 7;</code>
     */
    public int getSchedulerResourceTypesCount() {
      return schedulerResourceTypes_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.SchedulerResourceTypes scheduler_resource_types = 7;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.SchedulerResourceTypes getSchedulerResourceTypes(int index) {
      return schedulerResourceTypes_.get(index);
    }

    private void initFields() {
      maximumCapability_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
      clientToAmTokenMasterKey_ = com.google.protobuf.ByteString.EMPTY;
      applicationACLs_ = java.util.Collections.emptyList();
      containersFromPreviousAttempts_ = java.util.Collections.emptyList();
      queue_ = "";
      nmTokensFromPreviousAttempts_ = java.util.Collections.emptyList();
      schedulerResourceTypes_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      for (int i = 0; i < getContainersFromPreviousAttemptsCount(); i++) {
        if (!getContainersFromPreviousAttempts(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getNmTokensFromPreviousAttemptsCount(); i++) {
        if (!getNmTokensFromPreviousAttempts(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, maximumCapability_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, clientToAmTokenMasterKey_);
      }
      for (int i = 0; i < applicationACLs_.size(); i++) {
        output.writeMessage(3, applicationACLs_.get(i));
      }
      for (int i = 0; i < containersFromPreviousAttempts_.size(); i++) {
        output.writeMessage(4, containersFromPreviousAttempts_.get(i));
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeBytes(5, getQueueBytes());
      }
      for (int i = 0; i < nmTokensFromPreviousAttempts_.size(); i++) {
        output.writeMessage(6, nmTokensFromPreviousAttempts_.get(i));
      }
      for (int i = 0; i < schedulerResourceTypes_.size(); i++) {
        output.writeEnum(7, schedulerResourceTypes_.get(i).getNumber());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, maximumCapability_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, clientToAmTokenMasterKey_);
      }
      for (int i = 0; i < applicationACLs_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, applicationACLs_.get(i));
      }
      for (int i = 0; i < containersFromPreviousAttempts_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, containersFromPreviousAttempts_.get(i));
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(5, getQueueBytes());
      }
      for (int i = 0; i < nmTokensFromPreviousAttempts_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, nmTokensFromPreviousAttempts_.get(i));
      }
      {
        int dataSize = 0;
        for (int i = 0; i < schedulerResourceTypes_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeEnumSizeNoTag(schedulerResourceTypes_.get(i).getNumber());
        }
        size += dataSize;
        size += 1 * schedulerResourceTypes_.size();
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto) obj;

      boolean result = true;
      result = result && (hasMaximumCapability() == other.hasMaximumCapability());
      if (hasMaximumCapability()) {
        result = result && getMaximumCapability()
            .equals(other.getMaximumCapability());
      }
      result = result && (hasClientToAmTokenMasterKey() == other.hasClientToAmTokenMasterKey());
      if (hasClientToAmTokenMasterKey()) {
        result = result && getClientToAmTokenMasterKey()
            .equals(other.getClientToAmTokenMasterKey());
      }
      result = result && getApplicationACLsList()
          .equals(other.getApplicationACLsList());
      result = result && getContainersFromPreviousAttemptsList()
          .equals(other.getContainersFromPreviousAttemptsList());
      result = result && (hasQueue() == other.hasQueue());
      if (hasQueue()) {
        result = result && getQueue()
            .equals(other.getQueue());
      }
      result = result && getNmTokensFromPreviousAttemptsList()
          .equals(other.getNmTokensFromPreviousAttemptsList());
      result = result && getSchedulerResourceTypesList()
          .equals(other.getSchedulerResourceTypesList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasMaximumCapability()) {
        hash = (37 * hash) + MAXIMUMCAPABILITY_FIELD_NUMBER;
        hash = (53 * hash) + getMaximumCapability().hashCode();
      }
      if (hasClientToAmTokenMasterKey()) {
        hash = (37 * hash) + CLIENT_TO_AM_TOKEN_MASTER_KEY_FIELD_NUMBER;
        hash = (53 * hash) + getClientToAmTokenMasterKey().hashCode();
      }
      if (getApplicationACLsCount() > 0) {
        hash = (37 * hash) + APPLICATION_ACLS_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationACLsList().hashCode();
      }
      if (getContainersFromPreviousAttemptsCount() > 0) {
        hash = (37 * hash) + CONTAINERS_FROM_PREVIOUS_ATTEMPTS_FIELD_NUMBER;
        hash = (53 * hash) + getContainersFromPreviousAttemptsList().hashCode();
      }
      if (hasQueue()) {
        hash = (37 * hash) + QUEUE_FIELD_NUMBER;
        hash = (53 * hash) + getQueue().hashCode();
      }
      if (getNmTokensFromPreviousAttemptsCount() > 0) {
        hash = (37 * hash) + NM_TOKENS_FROM_PREVIOUS_ATTEMPTS_FIELD_NUMBER;
        hash = (53 * hash) + getNmTokensFromPreviousAttemptsList().hashCode();
      }
      if (getSchedulerResourceTypesCount() > 0) {
        hash = (37 * hash) + SCHEDULER_RESOURCE_TYPES_FIELD_NUMBER;
        hash = (53 * hash) + hashEnumList(getSchedulerResourceTypesList());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.RegisterApplicationMasterResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RegisterApplicationMasterResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RegisterApplicationMasterResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getMaximumCapabilityFieldBuilder();
          getApplicationACLsFieldBuilder();
          getContainersFromPreviousAttemptsFieldBuilder();
          getNmTokensFromPreviousAttemptsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (maximumCapabilityBuilder_ == null) {
          maximumCapability_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
        } else {
          maximumCapabilityBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        clientToAmTokenMasterKey_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        if (applicationACLsBuilder_ == null) {
          applicationACLs_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
        } else {
          applicationACLsBuilder_.clear();
        }
        if (containersFromPreviousAttemptsBuilder_ == null) {
          containersFromPreviousAttempts_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
        } else {
          containersFromPreviousAttemptsBuilder_.clear();
        }
        queue_ = "";
        bitField0_ = (bitField0_ & ~0x00000010);
        if (nmTokensFromPreviousAttemptsBuilder_ == null) {
          nmTokensFromPreviousAttempts_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
        } else {
          nmTokensFromPreviousAttemptsBuilder_.clear();
        }
        schedulerResourceTypes_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000040);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RegisterApplicationMasterResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (maximumCapabilityBuilder_ == null) {
          result.maximumCapability_ = maximumCapability_;
        } else {
          result.maximumCapability_ = maximumCapabilityBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.clientToAmTokenMasterKey_ = clientToAmTokenMasterKey_;
        if (applicationACLsBuilder_ == null) {
          if (((bitField0_ & 0x00000004) == 0x00000004)) {
            applicationACLs_ = java.util.Collections.unmodifiableList(applicationACLs_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.applicationACLs_ = applicationACLs_;
        } else {
          result.applicationACLs_ = applicationACLsBuilder_.build();
        }
        if (containersFromPreviousAttemptsBuilder_ == null) {
          if (((bitField0_ & 0x00000008) == 0x00000008)) {
            containersFromPreviousAttempts_ = java.util.Collections.unmodifiableList(containersFromPreviousAttempts_);
            bitField0_ = (bitField0_ & ~0x00000008);
          }
          result.containersFromPreviousAttempts_ = containersFromPreviousAttempts_;
        } else {
          result.containersFromPreviousAttempts_ = containersFromPreviousAttemptsBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000004;
        }
        result.queue_ = queue_;
        if (nmTokensFromPreviousAttemptsBuilder_ == null) {
          if (((bitField0_ & 0x00000020) == 0x00000020)) {
            nmTokensFromPreviousAttempts_ = java.util.Collections.unmodifiableList(nmTokensFromPreviousAttempts_);
            bitField0_ = (bitField0_ & ~0x00000020);
          }
          result.nmTokensFromPreviousAttempts_ = nmTokensFromPreviousAttempts_;
        } else {
          result.nmTokensFromPreviousAttempts_ = nmTokensFromPreviousAttemptsBuilder_.build();
        }
        if (((bitField0_ & 0x00000040) == 0x00000040)) {
          schedulerResourceTypes_ = java.util.Collections.unmodifiableList(schedulerResourceTypes_);
          bitField0_ = (bitField0_ & ~0x00000040);
        }
        result.schedulerResourceTypes_ = schedulerResourceTypes_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto.getDefaultInstance()) return this;
        if (other.hasMaximumCapability()) {
          mergeMaximumCapability(other.getMaximumCapability());
        }
        if (other.hasClientToAmTokenMasterKey()) {
          setClientToAmTokenMasterKey(other.getClientToAmTokenMasterKey());
        }
        if (applicationACLsBuilder_ == null) {
          if (!other.applicationACLs_.isEmpty()) {
            if (applicationACLs_.isEmpty()) {
              applicationACLs_ = other.applicationACLs_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureApplicationACLsIsMutable();
              applicationACLs_.addAll(other.applicationACLs_);
            }
            onChanged();
          }
        } else {
          if (!other.applicationACLs_.isEmpty()) {
            if (applicationACLsBuilder_.isEmpty()) {
              applicationACLsBuilder_.dispose();
              applicationACLsBuilder_ = null;
              applicationACLs_ = other.applicationACLs_;
              bitField0_ = (bitField0_ & ~0x00000004);
              applicationACLsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getApplicationACLsFieldBuilder() : null;
            } else {
              applicationACLsBuilder_.addAllMessages(other.applicationACLs_);
            }
          }
        }
        if (containersFromPreviousAttemptsBuilder_ == null) {
          if (!other.containersFromPreviousAttempts_.isEmpty()) {
            if (containersFromPreviousAttempts_.isEmpty()) {
              containersFromPreviousAttempts_ = other.containersFromPreviousAttempts_;
              bitField0_ = (bitField0_ & ~0x00000008);
            } else {
              ensureContainersFromPreviousAttemptsIsMutable();
              containersFromPreviousAttempts_.addAll(other.containersFromPreviousAttempts_);
            }
            onChanged();
          }
        } else {
          if (!other.containersFromPreviousAttempts_.isEmpty()) {
            if (containersFromPreviousAttemptsBuilder_.isEmpty()) {
              containersFromPreviousAttemptsBuilder_.dispose();
              containersFromPreviousAttemptsBuilder_ = null;
              containersFromPreviousAttempts_ = other.containersFromPreviousAttempts_;
              bitField0_ = (bitField0_ & ~0x00000008);
              containersFromPreviousAttemptsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getContainersFromPreviousAttemptsFieldBuilder() : null;
            } else {
              containersFromPreviousAttemptsBuilder_.addAllMessages(other.containersFromPreviousAttempts_);
            }
          }
        }
        if (other.hasQueue()) {
          bitField0_ |= 0x00000010;
          queue_ = other.queue_;
          onChanged();
        }
        if (nmTokensFromPreviousAttemptsBuilder_ == null) {
          if (!other.nmTokensFromPreviousAttempts_.isEmpty()) {
            if (nmTokensFromPreviousAttempts_.isEmpty()) {
              nmTokensFromPreviousAttempts_ = other.nmTokensFromPreviousAttempts_;
              bitField0_ = (bitField0_ & ~0x00000020);
            } else {
              ensureNmTokensFromPreviousAttemptsIsMutable();
              nmTokensFromPreviousAttempts_.addAll(other.nmTokensFromPreviousAttempts_);
            }
            onChanged();
          }
        } else {
          if (!other.nmTokensFromPreviousAttempts_.isEmpty()) {
            if (nmTokensFromPreviousAttemptsBuilder_.isEmpty()) {
              nmTokensFromPreviousAttemptsBuilder_.dispose();
              nmTokensFromPreviousAttemptsBuilder_ = null;
              nmTokensFromPreviousAttempts_ = other.nmTokensFromPreviousAttempts_;
              bitField0_ = (bitField0_ & ~0x00000020);
              nmTokensFromPreviousAttemptsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getNmTokensFromPreviousAttemptsFieldBuilder() : null;
            } else {
              nmTokensFromPreviousAttemptsBuilder_.addAllMessages(other.nmTokensFromPreviousAttempts_);
            }
          }
        }
        if (!other.schedulerResourceTypes_.isEmpty()) {
          if (schedulerResourceTypes_.isEmpty()) {
            schedulerResourceTypes_ = other.schedulerResourceTypes_;
            bitField0_ = (bitField0_ & ~0x00000040);
          } else {
            ensureSchedulerResourceTypesIsMutable();
            schedulerResourceTypes_.addAll(other.schedulerResourceTypes_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        for (int i = 0; i < getContainersFromPreviousAttemptsCount(); i++) {
          if (!getContainersFromPreviousAttempts(i).isInitialized()) {
            
            return false;
          }
        }
        for (int i = 0; i < getNmTokensFromPreviousAttemptsCount(); i++) {
          if (!getNmTokensFromPreviousAttempts(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.RegisterApplicationMasterResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ResourceProto maximumCapability = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto maximumCapability_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> maximumCapabilityBuilder_;
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
       */
      public boolean hasMaximumCapability() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getMaximumCapability() {
        if (maximumCapabilityBuilder_ == null) {
          return maximumCapability_;
        } else {
          return maximumCapabilityBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
       */
      public Builder setMaximumCapability(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (maximumCapabilityBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          maximumCapability_ = value;
          onChanged();
        } else {
          maximumCapabilityBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
       */
      public Builder setMaximumCapability(
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder builderForValue) {
        if (maximumCapabilityBuilder_ == null) {
          maximumCapability_ = builderForValue.build();
          onChanged();
        } else {
          maximumCapabilityBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
       */
      public Builder mergeMaximumCapability(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (maximumCapabilityBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              maximumCapability_ != org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance()) {
            maximumCapability_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.newBuilder(maximumCapability_).mergeFrom(value).buildPartial();
          } else {
            maximumCapability_ = value;
          }
          onChanged();
        } else {
          maximumCapabilityBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
       */
      public Builder clearMaximumCapability() {
        if (maximumCapabilityBuilder_ == null) {
          maximumCapability_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
          onChanged();
        } else {
          maximumCapabilityBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder getMaximumCapabilityBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getMaximumCapabilityFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getMaximumCapabilityOrBuilder() {
        if (maximumCapabilityBuilder_ != null) {
          return maximumCapabilityBuilder_.getMessageOrBuilder();
        } else {
          return maximumCapability_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> 
          getMaximumCapabilityFieldBuilder() {
        if (maximumCapabilityBuilder_ == null) {
          maximumCapabilityBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder>(
                  maximumCapability_,
                  getParentForChildren(),
                  isClean());
          maximumCapability_ = null;
        }
        return maximumCapabilityBuilder_;
      }

      // optional bytes client_to_am_token_master_key = 2;
      private com.google.protobuf.ByteString clientToAmTokenMasterKey_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <code>optional bytes client_to_am_token_master_key = 2;</code>
       */
      public boolean hasClientToAmTokenMasterKey() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional bytes client_to_am_token_master_key = 2;</code>
       */
      public com.google.protobuf.ByteString getClientToAmTokenMasterKey() {
        return clientToAmTokenMasterKey_;
      }
      /**
       * <code>optional bytes client_to_am_token_master_key = 2;</code>
       */
      public Builder setClientToAmTokenMasterKey(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        clientToAmTokenMasterKey_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bytes client_to_am_token_master_key = 2;</code>
       */
      public Builder clearClientToAmTokenMasterKey() {
        bitField0_ = (bitField0_ & ~0x00000002);
        clientToAmTokenMasterKey_ = getDefaultInstance().getClientToAmTokenMasterKey();
        onChanged();
        return this;
      }

      // repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto> applicationACLs_ =
        java.util.Collections.emptyList();
      private void ensureApplicationACLsIsMutable() {
        if (!((bitField0_ & 0x00000004) == 0x00000004)) {
          applicationACLs_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto>(applicationACLs_);
          bitField0_ |= 0x00000004;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProtoOrBuilder> applicationACLsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto> getApplicationACLsList() {
        if (applicationACLsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(applicationACLs_);
        } else {
          return applicationACLsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public int getApplicationACLsCount() {
        if (applicationACLsBuilder_ == null) {
          return applicationACLs_.size();
        } else {
          return applicationACLsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto getApplicationACLs(int index) {
        if (applicationACLsBuilder_ == null) {
          return applicationACLs_.get(index);
        } else {
          return applicationACLsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public Builder setApplicationACLs(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto value) {
        if (applicationACLsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureApplicationACLsIsMutable();
          applicationACLs_.set(index, value);
          onChanged();
        } else {
          applicationACLsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public Builder setApplicationACLs(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.Builder builderForValue) {
        if (applicationACLsBuilder_ == null) {
          ensureApplicationACLsIsMutable();
          applicationACLs_.set(index, builderForValue.build());
          onChanged();
        } else {
          applicationACLsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public Builder addApplicationACLs(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto value) {
        if (applicationACLsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureApplicationACLsIsMutable();
          applicationACLs_.add(value);
          onChanged();
        } else {
          applicationACLsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public Builder addApplicationACLs(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto value) {
        if (applicationACLsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureApplicationACLsIsMutable();
          applicationACLs_.add(index, value);
          onChanged();
        } else {
          applicationACLsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public Builder addApplicationACLs(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.Builder builderForValue) {
        if (applicationACLsBuilder_ == null) {
          ensureApplicationACLsIsMutable();
          applicationACLs_.add(builderForValue.build());
          onChanged();
        } else {
          applicationACLsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public Builder addApplicationACLs(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.Builder builderForValue) {
        if (applicationACLsBuilder_ == null) {
          ensureApplicationACLsIsMutable();
          applicationACLs_.add(index, builderForValue.build());
          onChanged();
        } else {
          applicationACLsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public Builder addAllApplicationACLs(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto> values) {
        if (applicationACLsBuilder_ == null) {
          ensureApplicationACLsIsMutable();
          super.addAll(values, applicationACLs_);
          onChanged();
        } else {
          applicationACLsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public Builder clearApplicationACLs() {
        if (applicationACLsBuilder_ == null) {
          applicationACLs_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
        } else {
          applicationACLsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public Builder removeApplicationACLs(int index) {
        if (applicationACLsBuilder_ == null) {
          ensureApplicationACLsIsMutable();
          applicationACLs_.remove(index);
          onChanged();
        } else {
          applicationACLsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.Builder getApplicationACLsBuilder(
          int index) {
        return getApplicationACLsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProtoOrBuilder getApplicationACLsOrBuilder(
          int index) {
        if (applicationACLsBuilder_ == null) {
          return applicationACLs_.get(index);  } else {
          return applicationACLsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProtoOrBuilder> 
           getApplicationACLsOrBuilderList() {
        if (applicationACLsBuilder_ != null) {
          return applicationACLsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(applicationACLs_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.Builder addApplicationACLsBuilder() {
        return getApplicationACLsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.Builder addApplicationACLsBuilder(
          int index) {
        return getApplicationACLsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationACLMapProto application_ACLs = 3;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.Builder> 
           getApplicationACLsBuilderList() {
        return getApplicationACLsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProtoOrBuilder> 
          getApplicationACLsFieldBuilder() {
        if (applicationACLsBuilder_ == null) {
          applicationACLsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationACLMapProtoOrBuilder>(
                  applicationACLs_,
                  ((bitField0_ & 0x00000004) == 0x00000004),
                  getParentForChildren(),
                  isClean());
          applicationACLs_ = null;
        }
        return applicationACLsBuilder_;
      }

      // repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> containersFromPreviousAttempts_ =
        java.util.Collections.emptyList();
      private void ensureContainersFromPreviousAttemptsIsMutable() {
        if (!((bitField0_ & 0x00000008) == 0x00000008)) {
          containersFromPreviousAttempts_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto>(containersFromPreviousAttempts_);
          bitField0_ |= 0x00000008;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> containersFromPreviousAttemptsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> getContainersFromPreviousAttemptsList() {
        if (containersFromPreviousAttemptsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(containersFromPreviousAttempts_);
        } else {
          return containersFromPreviousAttemptsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
       */
      public int getContainersFromPreviousAttemptsCount() {
        if (containersFromPreviousAttemptsBuilder_ == null) {
          return containersFromPreviousAttempts_.size();
        } else {
          return containersFromPreviousAttemptsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto getContainersFromPreviousAttempts(int index) {
        if (containersFromPreviousAttemptsBuilder_ == null) {
          return containersFromPreviousAttempts_.get(index);
        } else {
          return containersFromPreviousAttemptsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
       */
      public Builder setContainersFromPreviousAttempts(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto value) {
        if (containersFromPreviousAttemptsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainersFromPreviousAttemptsIsMutable();
          containersFromPreviousAttempts_.set(index, value);
          onChanged();
        } else {
          containersFromPreviousAttemptsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
       */
      public Builder setContainersFromPreviousAttempts(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder builderForValue) {
        if (containersFromPreviousAttemptsBuilder_ == null) {
          ensureContainersFromPreviousAttemptsIsMutable();
          containersFromPreviousAttempts_.set(index, builderForValue.build());
          onChanged();
        } else {
          containersFromPreviousAttemptsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
       */
      public Builder addContainersFromPreviousAttempts(org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto value) {
        if (containersFromPreviousAttemptsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainersFromPreviousAttemptsIsMutable();
          containersFromPreviousAttempts_.add(value);
          onChanged();
        } else {
          containersFromPreviousAttemptsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
       */
      public Builder addContainersFromPreviousAttempts(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto value) {
        if (containersFromPreviousAttemptsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainersFromPreviousAttemptsIsMutable();
          containersFromPreviousAttempts_.add(index, value);
          onChanged();
        } else {
          containersFromPreviousAttemptsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
       */
      public Builder addContainersFromPreviousAttempts(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder builderForValue) {
        if (containersFromPreviousAttemptsBuilder_ == null) {
          ensureContainersFromPreviousAttemptsIsMutable();
          containersFromPreviousAttempts_.add(builderForValue.build());
          onChanged();
        } else {
          containersFromPreviousAttemptsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
       */
      public Builder addContainersFromPreviousAttempts(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder builderForValue) {
        if (containersFromPreviousAttemptsBuilder_ == null) {
          ensureContainersFromPreviousAttemptsIsMutable();
          containersFromPreviousAttempts_.add(index, builderForValue.build());
          onChanged();
        } else {
          containersFromPreviousAttemptsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
       */
      public Builder addAllContainersFromPreviousAttempts(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> values) {
        if (containersFromPreviousAttemptsBuilder_ == null) {
          ensureContainersFromPreviousAttemptsIsMutable();
          super.addAll(values, containersFromPreviousAttempts_);
          onChanged();
        } else {
          containersFromPreviousAttemptsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
       */
      public Builder clearContainersFromPreviousAttempts() {
        if (containersFromPreviousAttemptsBuilder_ == null) {
          containersFromPreviousAttempts_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
          onChanged();
        } else {
          containersFromPreviousAttemptsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
       */
      public Builder removeContainersFromPreviousAttempts(int index) {
        if (containersFromPreviousAttemptsBuilder_ == null) {
          ensureContainersFromPreviousAttemptsIsMutable();
          containersFromPreviousAttempts_.remove(index);
          onChanged();
        } else {
          containersFromPreviousAttemptsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder getContainersFromPreviousAttemptsBuilder(
          int index) {
        return getContainersFromPreviousAttemptsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder getContainersFromPreviousAttemptsOrBuilder(
          int index) {
        if (containersFromPreviousAttemptsBuilder_ == null) {
          return containersFromPreviousAttempts_.get(index);  } else {
          return containersFromPreviousAttemptsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
           getContainersFromPreviousAttemptsOrBuilderList() {
        if (containersFromPreviousAttemptsBuilder_ != null) {
          return containersFromPreviousAttemptsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(containersFromPreviousAttempts_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder addContainersFromPreviousAttemptsBuilder() {
        return getContainersFromPreviousAttemptsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder addContainersFromPreviousAttemptsBuilder(
          int index) {
        return getContainersFromPreviousAttemptsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto containers_from_previous_attempts = 4;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder> 
           getContainersFromPreviousAttemptsBuilderList() {
        return getContainersFromPreviousAttemptsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
          getContainersFromPreviousAttemptsFieldBuilder() {
        if (containersFromPreviousAttemptsBuilder_ == null) {
          containersFromPreviousAttemptsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder>(
                  containersFromPreviousAttempts_,
                  ((bitField0_ & 0x00000008) == 0x00000008),
                  getParentForChildren(),
                  isClean());
          containersFromPreviousAttempts_ = null;
        }
        return containersFromPreviousAttemptsBuilder_;
      }

      // optional string queue = 5;
      private java.lang.Object queue_ = "";
      /**
       * <code>optional string queue = 5;</code>
       */
      public boolean hasQueue() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional string queue = 5;</code>
       */
      public java.lang.String getQueue() {
        java.lang.Object ref = queue_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          queue_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string queue = 5;</code>
       */
      public com.google.protobuf.ByteString
          getQueueBytes() {
        java.lang.Object ref = queue_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          queue_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string queue = 5;</code>
       */
      public Builder setQueue(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        queue_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string queue = 5;</code>
       */
      public Builder clearQueue() {
        bitField0_ = (bitField0_ & ~0x00000010);
        queue_ = getDefaultInstance().getQueue();
        onChanged();
        return this;
      }
      /**
       * <code>optional string queue = 5;</code>
       */
      public Builder setQueueBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        queue_ = value;
        onChanged();
        return this;
      }

      // repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto> nmTokensFromPreviousAttempts_ =
        java.util.Collections.emptyList();
      private void ensureNmTokensFromPreviousAttemptsIsMutable() {
        if (!((bitField0_ & 0x00000020) == 0x00000020)) {
          nmTokensFromPreviousAttempts_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto>(nmTokensFromPreviousAttempts_);
          bitField0_ |= 0x00000020;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder> nmTokensFromPreviousAttemptsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto> getNmTokensFromPreviousAttemptsList() {
        if (nmTokensFromPreviousAttemptsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(nmTokensFromPreviousAttempts_);
        } else {
          return nmTokensFromPreviousAttemptsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
       */
      public int getNmTokensFromPreviousAttemptsCount() {
        if (nmTokensFromPreviousAttemptsBuilder_ == null) {
          return nmTokensFromPreviousAttempts_.size();
        } else {
          return nmTokensFromPreviousAttemptsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto getNmTokensFromPreviousAttempts(int index) {
        if (nmTokensFromPreviousAttemptsBuilder_ == null) {
          return nmTokensFromPreviousAttempts_.get(index);
        } else {
          return nmTokensFromPreviousAttemptsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
       */
      public Builder setNmTokensFromPreviousAttempts(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto value) {
        if (nmTokensFromPreviousAttemptsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNmTokensFromPreviousAttemptsIsMutable();
          nmTokensFromPreviousAttempts_.set(index, value);
          onChanged();
        } else {
          nmTokensFromPreviousAttemptsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
       */
      public Builder setNmTokensFromPreviousAttempts(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder builderForValue) {
        if (nmTokensFromPreviousAttemptsBuilder_ == null) {
          ensureNmTokensFromPreviousAttemptsIsMutable();
          nmTokensFromPreviousAttempts_.set(index, builderForValue.build());
          onChanged();
        } else {
          nmTokensFromPreviousAttemptsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
       */
      public Builder addNmTokensFromPreviousAttempts(org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto value) {
        if (nmTokensFromPreviousAttemptsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNmTokensFromPreviousAttemptsIsMutable();
          nmTokensFromPreviousAttempts_.add(value);
          onChanged();
        } else {
          nmTokensFromPreviousAttemptsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
       */
      public Builder addNmTokensFromPreviousAttempts(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto value) {
        if (nmTokensFromPreviousAttemptsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNmTokensFromPreviousAttemptsIsMutable();
          nmTokensFromPreviousAttempts_.add(index, value);
          onChanged();
        } else {
          nmTokensFromPreviousAttemptsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
       */
      public Builder addNmTokensFromPreviousAttempts(
          org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder builderForValue) {
        if (nmTokensFromPreviousAttemptsBuilder_ == null) {
          ensureNmTokensFromPreviousAttemptsIsMutable();
          nmTokensFromPreviousAttempts_.add(builderForValue.build());
          onChanged();
        } else {
          nmTokensFromPreviousAttemptsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
       */
      public Builder addNmTokensFromPreviousAttempts(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder builderForValue) {
        if (nmTokensFromPreviousAttemptsBuilder_ == null) {
          ensureNmTokensFromPreviousAttemptsIsMutable();
          nmTokensFromPreviousAttempts_.add(index, builderForValue.build());
          onChanged();
        } else {
          nmTokensFromPreviousAttemptsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
       */
      public Builder addAllNmTokensFromPreviousAttempts(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto> values) {
        if (nmTokensFromPreviousAttemptsBuilder_ == null) {
          ensureNmTokensFromPreviousAttemptsIsMutable();
          super.addAll(values, nmTokensFromPreviousAttempts_);
          onChanged();
        } else {
          nmTokensFromPreviousAttemptsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
       */
      public Builder clearNmTokensFromPreviousAttempts() {
        if (nmTokensFromPreviousAttemptsBuilder_ == null) {
          nmTokensFromPreviousAttempts_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
          onChanged();
        } else {
          nmTokensFromPreviousAttemptsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
       */
      public Builder removeNmTokensFromPreviousAttempts(int index) {
        if (nmTokensFromPreviousAttemptsBuilder_ == null) {
          ensureNmTokensFromPreviousAttemptsIsMutable();
          nmTokensFromPreviousAttempts_.remove(index);
          onChanged();
        } else {
          nmTokensFromPreviousAttemptsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder getNmTokensFromPreviousAttemptsBuilder(
          int index) {
        return getNmTokensFromPreviousAttemptsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder getNmTokensFromPreviousAttemptsOrBuilder(
          int index) {
        if (nmTokensFromPreviousAttemptsBuilder_ == null) {
          return nmTokensFromPreviousAttempts_.get(index);  } else {
          return nmTokensFromPreviousAttemptsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder> 
           getNmTokensFromPreviousAttemptsOrBuilderList() {
        if (nmTokensFromPreviousAttemptsBuilder_ != null) {
          return nmTokensFromPreviousAttemptsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(nmTokensFromPreviousAttempts_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder addNmTokensFromPreviousAttemptsBuilder() {
        return getNmTokensFromPreviousAttemptsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder addNmTokensFromPreviousAttemptsBuilder(
          int index) {
        return getNmTokensFromPreviousAttemptsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens_from_previous_attempts = 6;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder> 
           getNmTokensFromPreviousAttemptsBuilderList() {
        return getNmTokensFromPreviousAttemptsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder> 
          getNmTokensFromPreviousAttemptsFieldBuilder() {
        if (nmTokensFromPreviousAttemptsBuilder_ == null) {
          nmTokensFromPreviousAttemptsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder>(
                  nmTokensFromPreviousAttempts_,
                  ((bitField0_ & 0x00000020) == 0x00000020),
                  getParentForChildren(),
                  isClean());
          nmTokensFromPreviousAttempts_ = null;
        }
        return nmTokensFromPreviousAttemptsBuilder_;
      }

      // repeated .hadoop.yarn.SchedulerResourceTypes scheduler_resource_types = 7;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.SchedulerResourceTypes> schedulerResourceTypes_ =
        java.util.Collections.emptyList();
      private void ensureSchedulerResourceTypesIsMutable() {
        if (!((bitField0_ & 0x00000040) == 0x00000040)) {
          schedulerResourceTypes_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.SchedulerResourceTypes>(schedulerResourceTypes_);
          bitField0_ |= 0x00000040;
        }
      }
      /**
       * <code>repeated .hadoop.yarn.SchedulerResourceTypes scheduler_resource_types = 7;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.SchedulerResourceTypes> getSchedulerResourceTypesList() {
        return java.util.Collections.unmodifiableList(schedulerResourceTypes_);
      }
      /**
       * <code>repeated .hadoop.yarn.SchedulerResourceTypes scheduler_resource_types = 7;</code>
       */
      public int getSchedulerResourceTypesCount() {
        return schedulerResourceTypes_.size();
      }
      /**
       * <code>repeated .hadoop.yarn.SchedulerResourceTypes scheduler_resource_types = 7;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.SchedulerResourceTypes getSchedulerResourceTypes(int index) {
        return schedulerResourceTypes_.get(index);
      }
      /**
       * <code>repeated .hadoop.yarn.SchedulerResourceTypes scheduler_resource_types = 7;</code>
       */
      public Builder setSchedulerResourceTypes(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.SchedulerResourceTypes value) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensureSchedulerResourceTypesIsMutable();
        schedulerResourceTypes_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.SchedulerResourceTypes scheduler_resource_types = 7;</code>
       */
      public Builder addSchedulerResourceTypes(org.apache.hadoop.yarn.proto.YarnServiceProtos.SchedulerResourceTypes value) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensureSchedulerResourceTypesIsMutable();
        schedulerResourceTypes_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.SchedulerResourceTypes scheduler_resource_types = 7;</code>
       */
      public Builder addAllSchedulerResourceTypes(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.SchedulerResourceTypes> values) {
        ensureSchedulerResourceTypesIsMutable();
        super.addAll(values, schedulerResourceTypes_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.SchedulerResourceTypes scheduler_resource_types = 7;</code>
       */
      public Builder clearSchedulerResourceTypes() {
        schedulerResourceTypes_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000040);
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.RegisterApplicationMasterResponseProto)
    }

    static {
      defaultInstance = new RegisterApplicationMasterResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.RegisterApplicationMasterResponseProto)
  }

  public interface FinishApplicationMasterRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional string diagnostics = 1;
    /**
     * <code>optional string diagnostics = 1;</code>
     */
    boolean hasDiagnostics();
    /**
     * <code>optional string diagnostics = 1;</code>
     */
    java.lang.String getDiagnostics();
    /**
     * <code>optional string diagnostics = 1;</code>
     */
    com.google.protobuf.ByteString
        getDiagnosticsBytes();

    // optional string tracking_url = 2;
    /**
     * <code>optional string tracking_url = 2;</code>
     */
    boolean hasTrackingUrl();
    /**
     * <code>optional string tracking_url = 2;</code>
     */
    java.lang.String getTrackingUrl();
    /**
     * <code>optional string tracking_url = 2;</code>
     */
    com.google.protobuf.ByteString
        getTrackingUrlBytes();

    // optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 3;
    /**
     * <code>optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 3;</code>
     */
    boolean hasFinalApplicationStatus();
    /**
     * <code>optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto getFinalApplicationStatus();
  }
  /**
   * Protobuf type {@code hadoop.yarn.FinishApplicationMasterRequestProto}
   */
  public static final class FinishApplicationMasterRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements FinishApplicationMasterRequestProtoOrBuilder {
    // Use FinishApplicationMasterRequestProto.newBuilder() to construct.
    private FinishApplicationMasterRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private FinishApplicationMasterRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final FinishApplicationMasterRequestProto defaultInstance;
    public static FinishApplicationMasterRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public FinishApplicationMasterRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private FinishApplicationMasterRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              diagnostics_ = input.readBytes();
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              trackingUrl_ = input.readBytes();
              break;
            }
            case 24: {
              int rawValue = input.readEnum();
              org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto value = org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(3, rawValue);
              } else {
                bitField0_ |= 0x00000004;
                finalApplicationStatus_ = value;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FinishApplicationMasterRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FinishApplicationMasterRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<FinishApplicationMasterRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<FinishApplicationMasterRequestProto>() {
      public FinishApplicationMasterRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new FinishApplicationMasterRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<FinishApplicationMasterRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional string diagnostics = 1;
    public static final int DIAGNOSTICS_FIELD_NUMBER = 1;
    private java.lang.Object diagnostics_;
    /**
     * <code>optional string diagnostics = 1;</code>
     */
    public boolean hasDiagnostics() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional string diagnostics = 1;</code>
     */
    public java.lang.String getDiagnostics() {
      java.lang.Object ref = diagnostics_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          diagnostics_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string diagnostics = 1;</code>
     */
    public com.google.protobuf.ByteString
        getDiagnosticsBytes() {
      java.lang.Object ref = diagnostics_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        diagnostics_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional string tracking_url = 2;
    public static final int TRACKING_URL_FIELD_NUMBER = 2;
    private java.lang.Object trackingUrl_;
    /**
     * <code>optional string tracking_url = 2;</code>
     */
    public boolean hasTrackingUrl() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional string tracking_url = 2;</code>
     */
    public java.lang.String getTrackingUrl() {
      java.lang.Object ref = trackingUrl_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          trackingUrl_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string tracking_url = 2;</code>
     */
    public com.google.protobuf.ByteString
        getTrackingUrlBytes() {
      java.lang.Object ref = trackingUrl_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        trackingUrl_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 3;
    public static final int FINAL_APPLICATION_STATUS_FIELD_NUMBER = 3;
    private org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto finalApplicationStatus_;
    /**
     * <code>optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 3;</code>
     */
    public boolean hasFinalApplicationStatus() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto getFinalApplicationStatus() {
      return finalApplicationStatus_;
    }

    private void initFields() {
      diagnostics_ = "";
      trackingUrl_ = "";
      finalApplicationStatus_ = org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto.APP_UNDEFINED;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getDiagnosticsBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, getTrackingUrlBytes());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeEnum(3, finalApplicationStatus_.getNumber());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getDiagnosticsBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, getTrackingUrlBytes());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(3, finalApplicationStatus_.getNumber());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto) obj;

      boolean result = true;
      result = result && (hasDiagnostics() == other.hasDiagnostics());
      if (hasDiagnostics()) {
        result = result && getDiagnostics()
            .equals(other.getDiagnostics());
      }
      result = result && (hasTrackingUrl() == other.hasTrackingUrl());
      if (hasTrackingUrl()) {
        result = result && getTrackingUrl()
            .equals(other.getTrackingUrl());
      }
      result = result && (hasFinalApplicationStatus() == other.hasFinalApplicationStatus());
      if (hasFinalApplicationStatus()) {
        result = result &&
            (getFinalApplicationStatus() == other.getFinalApplicationStatus());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasDiagnostics()) {
        hash = (37 * hash) + DIAGNOSTICS_FIELD_NUMBER;
        hash = (53 * hash) + getDiagnostics().hashCode();
      }
      if (hasTrackingUrl()) {
        hash = (37 * hash) + TRACKING_URL_FIELD_NUMBER;
        hash = (53 * hash) + getTrackingUrl().hashCode();
      }
      if (hasFinalApplicationStatus()) {
        hash = (37 * hash) + FINAL_APPLICATION_STATUS_FIELD_NUMBER;
        hash = (53 * hash) + hashEnum(getFinalApplicationStatus());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.FinishApplicationMasterRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FinishApplicationMasterRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FinishApplicationMasterRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        diagnostics_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        trackingUrl_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        finalApplicationStatus_ = org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto.APP_UNDEFINED;
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FinishApplicationMasterRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.diagnostics_ = diagnostics_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.trackingUrl_ = trackingUrl_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.finalApplicationStatus_ = finalApplicationStatus_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto.getDefaultInstance()) return this;
        if (other.hasDiagnostics()) {
          bitField0_ |= 0x00000001;
          diagnostics_ = other.diagnostics_;
          onChanged();
        }
        if (other.hasTrackingUrl()) {
          bitField0_ |= 0x00000002;
          trackingUrl_ = other.trackingUrl_;
          onChanged();
        }
        if (other.hasFinalApplicationStatus()) {
          setFinalApplicationStatus(other.getFinalApplicationStatus());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional string diagnostics = 1;
      private java.lang.Object diagnostics_ = "";
      /**
       * <code>optional string diagnostics = 1;</code>
       */
      public boolean hasDiagnostics() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional string diagnostics = 1;</code>
       */
      public java.lang.String getDiagnostics() {
        java.lang.Object ref = diagnostics_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          diagnostics_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string diagnostics = 1;</code>
       */
      public com.google.protobuf.ByteString
          getDiagnosticsBytes() {
        java.lang.Object ref = diagnostics_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          diagnostics_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string diagnostics = 1;</code>
       */
      public Builder setDiagnostics(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        diagnostics_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string diagnostics = 1;</code>
       */
      public Builder clearDiagnostics() {
        bitField0_ = (bitField0_ & ~0x00000001);
        diagnostics_ = getDefaultInstance().getDiagnostics();
        onChanged();
        return this;
      }
      /**
       * <code>optional string diagnostics = 1;</code>
       */
      public Builder setDiagnosticsBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        diagnostics_ = value;
        onChanged();
        return this;
      }

      // optional string tracking_url = 2;
      private java.lang.Object trackingUrl_ = "";
      /**
       * <code>optional string tracking_url = 2;</code>
       */
      public boolean hasTrackingUrl() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional string tracking_url = 2;</code>
       */
      public java.lang.String getTrackingUrl() {
        java.lang.Object ref = trackingUrl_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          trackingUrl_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string tracking_url = 2;</code>
       */
      public com.google.protobuf.ByteString
          getTrackingUrlBytes() {
        java.lang.Object ref = trackingUrl_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          trackingUrl_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string tracking_url = 2;</code>
       */
      public Builder setTrackingUrl(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        trackingUrl_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string tracking_url = 2;</code>
       */
      public Builder clearTrackingUrl() {
        bitField0_ = (bitField0_ & ~0x00000002);
        trackingUrl_ = getDefaultInstance().getTrackingUrl();
        onChanged();
        return this;
      }
      /**
       * <code>optional string tracking_url = 2;</code>
       */
      public Builder setTrackingUrlBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        trackingUrl_ = value;
        onChanged();
        return this;
      }

      // optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 3;
      private org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto finalApplicationStatus_ = org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto.APP_UNDEFINED;
      /**
       * <code>optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 3;</code>
       */
      public boolean hasFinalApplicationStatus() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto getFinalApplicationStatus() {
        return finalApplicationStatus_;
      }
      /**
       * <code>optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 3;</code>
       */
      public Builder setFinalApplicationStatus(org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000004;
        finalApplicationStatus_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.FinalApplicationStatusProto final_application_status = 3;</code>
       */
      public Builder clearFinalApplicationStatus() {
        bitField0_ = (bitField0_ & ~0x00000004);
        finalApplicationStatus_ = org.apache.hadoop.yarn.proto.YarnProtos.FinalApplicationStatusProto.APP_UNDEFINED;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.FinishApplicationMasterRequestProto)
    }

    static {
      defaultInstance = new FinishApplicationMasterRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.FinishApplicationMasterRequestProto)
  }

  public interface FinishApplicationMasterResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional bool isUnregistered = 1 [default = false];
    /**
     * <code>optional bool isUnregistered = 1 [default = false];</code>
     */
    boolean hasIsUnregistered();
    /**
     * <code>optional bool isUnregistered = 1 [default = false];</code>
     */
    boolean getIsUnregistered();
  }
  /**
   * Protobuf type {@code hadoop.yarn.FinishApplicationMasterResponseProto}
   */
  public static final class FinishApplicationMasterResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements FinishApplicationMasterResponseProtoOrBuilder {
    // Use FinishApplicationMasterResponseProto.newBuilder() to construct.
    private FinishApplicationMasterResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private FinishApplicationMasterResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final FinishApplicationMasterResponseProto defaultInstance;
    public static FinishApplicationMasterResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public FinishApplicationMasterResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private FinishApplicationMasterResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              isUnregistered_ = input.readBool();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FinishApplicationMasterResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FinishApplicationMasterResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<FinishApplicationMasterResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<FinishApplicationMasterResponseProto>() {
      public FinishApplicationMasterResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new FinishApplicationMasterResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<FinishApplicationMasterResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional bool isUnregistered = 1 [default = false];
    public static final int ISUNREGISTERED_FIELD_NUMBER = 1;
    private boolean isUnregistered_;
    /**
     * <code>optional bool isUnregistered = 1 [default = false];</code>
     */
    public boolean hasIsUnregistered() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional bool isUnregistered = 1 [default = false];</code>
     */
    public boolean getIsUnregistered() {
      return isUnregistered_;
    }

    private void initFields() {
      isUnregistered_ = false;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBool(1, isUnregistered_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(1, isUnregistered_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto) obj;

      boolean result = true;
      result = result && (hasIsUnregistered() == other.hasIsUnregistered());
      if (hasIsUnregistered()) {
        result = result && (getIsUnregistered()
            == other.getIsUnregistered());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasIsUnregistered()) {
        hash = (37 * hash) + ISUNREGISTERED_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getIsUnregistered());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.FinishApplicationMasterResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FinishApplicationMasterResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FinishApplicationMasterResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        isUnregistered_ = false;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FinishApplicationMasterResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.isUnregistered_ = isUnregistered_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto.getDefaultInstance()) return this;
        if (other.hasIsUnregistered()) {
          setIsUnregistered(other.getIsUnregistered());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.FinishApplicationMasterResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional bool isUnregistered = 1 [default = false];
      private boolean isUnregistered_ ;
      /**
       * <code>optional bool isUnregistered = 1 [default = false];</code>
       */
      public boolean hasIsUnregistered() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional bool isUnregistered = 1 [default = false];</code>
       */
      public boolean getIsUnregistered() {
        return isUnregistered_;
      }
      /**
       * <code>optional bool isUnregistered = 1 [default = false];</code>
       */
      public Builder setIsUnregistered(boolean value) {
        bitField0_ |= 0x00000001;
        isUnregistered_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool isUnregistered = 1 [default = false];</code>
       */
      public Builder clearIsUnregistered() {
        bitField0_ = (bitField0_ & ~0x00000001);
        isUnregistered_ = false;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.FinishApplicationMasterResponseProto)
    }

    static {
      defaultInstance = new FinishApplicationMasterResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.FinishApplicationMasterResponseProto)
  }

  public interface UpdateContainerRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required int32 container_version = 1;
    /**
     * <code>required int32 container_version = 1;</code>
     */
    boolean hasContainerVersion();
    /**
     * <code>required int32 container_version = 1;</code>
     */
    int getContainerVersion();

    // required .hadoop.yarn.ContainerIdProto container_id = 2;
    /**
     * <code>required .hadoop.yarn.ContainerIdProto container_id = 2;</code>
     */
    boolean hasContainerId();
    /**
     * <code>required .hadoop.yarn.ContainerIdProto container_id = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId();
    /**
     * <code>required .hadoop.yarn.ContainerIdProto container_id = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder();

    // required .hadoop.yarn.ContainerUpdateTypeProto update_type = 3;
    /**
     * <code>required .hadoop.yarn.ContainerUpdateTypeProto update_type = 3;</code>
     */
    boolean hasUpdateType();
    /**
     * <code>required .hadoop.yarn.ContainerUpdateTypeProto update_type = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerUpdateTypeProto getUpdateType();

    // optional .hadoop.yarn.ResourceProto capability = 4;
    /**
     * <code>optional .hadoop.yarn.ResourceProto capability = 4;</code>
     */
    boolean hasCapability();
    /**
     * <code>optional .hadoop.yarn.ResourceProto capability = 4;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getCapability();
    /**
     * <code>optional .hadoop.yarn.ResourceProto capability = 4;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getCapabilityOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.UpdateContainerRequestProto}
   */
  public static final class UpdateContainerRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements UpdateContainerRequestProtoOrBuilder {
    // Use UpdateContainerRequestProto.newBuilder() to construct.
    private UpdateContainerRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private UpdateContainerRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final UpdateContainerRequestProto defaultInstance;
    public static UpdateContainerRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public UpdateContainerRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private UpdateContainerRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              containerVersion_ = input.readInt32();
              break;
            }
            case 18: {
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) == 0x00000002)) {
                subBuilder = containerId_.toBuilder();
              }
              containerId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(containerId_);
                containerId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
            case 24: {
              int rawValue = input.readEnum();
              org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerUpdateTypeProto value = org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerUpdateTypeProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(3, rawValue);
              } else {
                bitField0_ |= 0x00000004;
                updateType_ = value;
              }
              break;
            }
            case 34: {
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000008) == 0x00000008)) {
                subBuilder = capability_.toBuilder();
              }
              capability_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(capability_);
                capability_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000008;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdateContainerRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdateContainerRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<UpdateContainerRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<UpdateContainerRequestProto>() {
      public UpdateContainerRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new UpdateContainerRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<UpdateContainerRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required int32 container_version = 1;
    public static final int CONTAINER_VERSION_FIELD_NUMBER = 1;
    private int containerVersion_;
    /**
     * <code>required int32 container_version = 1;</code>
     */
    public boolean hasContainerVersion() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required int32 container_version = 1;</code>
     */
    public int getContainerVersion() {
      return containerVersion_;
    }

    // required .hadoop.yarn.ContainerIdProto container_id = 2;
    public static final int CONTAINER_ID_FIELD_NUMBER = 2;
    private org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto containerId_;
    /**
     * <code>required .hadoop.yarn.ContainerIdProto container_id = 2;</code>
     */
    public boolean hasContainerId() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required .hadoop.yarn.ContainerIdProto container_id = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId() {
      return containerId_;
    }
    /**
     * <code>required .hadoop.yarn.ContainerIdProto container_id = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder() {
      return containerId_;
    }

    // required .hadoop.yarn.ContainerUpdateTypeProto update_type = 3;
    public static final int UPDATE_TYPE_FIELD_NUMBER = 3;
    private org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerUpdateTypeProto updateType_;
    /**
     * <code>required .hadoop.yarn.ContainerUpdateTypeProto update_type = 3;</code>
     */
    public boolean hasUpdateType() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>required .hadoop.yarn.ContainerUpdateTypeProto update_type = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerUpdateTypeProto getUpdateType() {
      return updateType_;
    }

    // optional .hadoop.yarn.ResourceProto capability = 4;
    public static final int CAPABILITY_FIELD_NUMBER = 4;
    private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto capability_;
    /**
     * <code>optional .hadoop.yarn.ResourceProto capability = 4;</code>
     */
    public boolean hasCapability() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto capability = 4;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getCapability() {
      return capability_;
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto capability = 4;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getCapabilityOrBuilder() {
      return capability_;
    }

    private void initFields() {
      containerVersion_ = 0;
      containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
      updateType_ = org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerUpdateTypeProto.INCREASE_RESOURCE;
      capability_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasContainerVersion()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasContainerId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasUpdateType()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeInt32(1, containerVersion_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeMessage(2, containerId_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeEnum(3, updateType_.getNumber());
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeMessage(4, capability_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(1, containerVersion_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, containerId_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(3, updateType_.getNumber());
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, capability_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto) obj;

      boolean result = true;
      result = result && (hasContainerVersion() == other.hasContainerVersion());
      if (hasContainerVersion()) {
        result = result && (getContainerVersion()
            == other.getContainerVersion());
      }
      result = result && (hasContainerId() == other.hasContainerId());
      if (hasContainerId()) {
        result = result && getContainerId()
            .equals(other.getContainerId());
      }
      result = result && (hasUpdateType() == other.hasUpdateType());
      if (hasUpdateType()) {
        result = result &&
            (getUpdateType() == other.getUpdateType());
      }
      result = result && (hasCapability() == other.hasCapability());
      if (hasCapability()) {
        result = result && getCapability()
            .equals(other.getCapability());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasContainerVersion()) {
        hash = (37 * hash) + CONTAINER_VERSION_FIELD_NUMBER;
        hash = (53 * hash) + getContainerVersion();
      }
      if (hasContainerId()) {
        hash = (37 * hash) + CONTAINER_ID_FIELD_NUMBER;
        hash = (53 * hash) + getContainerId().hashCode();
      }
      if (hasUpdateType()) {
        hash = (37 * hash) + UPDATE_TYPE_FIELD_NUMBER;
        hash = (53 * hash) + hashEnum(getUpdateType());
      }
      if (hasCapability()) {
        hash = (37 * hash) + CAPABILITY_FIELD_NUMBER;
        hash = (53 * hash) + getCapability().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.UpdateContainerRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdateContainerRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdateContainerRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getContainerIdFieldBuilder();
          getCapabilityFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        containerVersion_ = 0;
        bitField0_ = (bitField0_ & ~0x00000001);
        if (containerIdBuilder_ == null) {
          containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
        } else {
          containerIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        updateType_ = org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerUpdateTypeProto.INCREASE_RESOURCE;
        bitField0_ = (bitField0_ & ~0x00000004);
        if (capabilityBuilder_ == null) {
          capability_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
        } else {
          capabilityBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdateContainerRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.containerVersion_ = containerVersion_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        if (containerIdBuilder_ == null) {
          result.containerId_ = containerId_;
        } else {
          result.containerId_ = containerIdBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.updateType_ = updateType_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        if (capabilityBuilder_ == null) {
          result.capability_ = capability_;
        } else {
          result.capability_ = capabilityBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.getDefaultInstance()) return this;
        if (other.hasContainerVersion()) {
          setContainerVersion(other.getContainerVersion());
        }
        if (other.hasContainerId()) {
          mergeContainerId(other.getContainerId());
        }
        if (other.hasUpdateType()) {
          setUpdateType(other.getUpdateType());
        }
        if (other.hasCapability()) {
          mergeCapability(other.getCapability());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasContainerVersion()) {
          
          return false;
        }
        if (!hasContainerId()) {
          
          return false;
        }
        if (!hasUpdateType()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required int32 container_version = 1;
      private int containerVersion_ ;
      /**
       * <code>required int32 container_version = 1;</code>
       */
      public boolean hasContainerVersion() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required int32 container_version = 1;</code>
       */
      public int getContainerVersion() {
        return containerVersion_;
      }
      /**
       * <code>required int32 container_version = 1;</code>
       */
      public Builder setContainerVersion(int value) {
        bitField0_ |= 0x00000001;
        containerVersion_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required int32 container_version = 1;</code>
       */
      public Builder clearContainerVersion() {
        bitField0_ = (bitField0_ & ~0x00000001);
        containerVersion_ = 0;
        onChanged();
        return this;
      }

      // required .hadoop.yarn.ContainerIdProto container_id = 2;
      private org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> containerIdBuilder_;
      /**
       * <code>required .hadoop.yarn.ContainerIdProto container_id = 2;</code>
       */
      public boolean hasContainerId() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required .hadoop.yarn.ContainerIdProto container_id = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId() {
        if (containerIdBuilder_ == null) {
          return containerId_;
        } else {
          return containerIdBuilder_.getMessage();
        }
      }
      /**
       * <code>required .hadoop.yarn.ContainerIdProto container_id = 2;</code>
       */
      public Builder setContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          containerId_ = value;
          onChanged();
        } else {
          containerIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .hadoop.yarn.ContainerIdProto container_id = 2;</code>
       */
      public Builder setContainerId(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containerIdBuilder_ == null) {
          containerId_ = builderForValue.build();
          onChanged();
        } else {
          containerIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .hadoop.yarn.ContainerIdProto container_id = 2;</code>
       */
      public Builder mergeContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002) &&
              containerId_ != org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance()) {
            containerId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.newBuilder(containerId_).mergeFrom(value).buildPartial();
          } else {
            containerId_ = value;
          }
          onChanged();
        } else {
          containerIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .hadoop.yarn.ContainerIdProto container_id = 2;</code>
       */
      public Builder clearContainerId() {
        if (containerIdBuilder_ == null) {
          containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
          onChanged();
        } else {
          containerIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>required .hadoop.yarn.ContainerIdProto container_id = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder getContainerIdBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getContainerIdFieldBuilder().getBuilder();
      }
      /**
       * <code>required .hadoop.yarn.ContainerIdProto container_id = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder() {
        if (containerIdBuilder_ != null) {
          return containerIdBuilder_.getMessageOrBuilder();
        } else {
          return containerId_;
        }
      }
      /**
       * <code>required .hadoop.yarn.ContainerIdProto container_id = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
          getContainerIdFieldBuilder() {
        if (containerIdBuilder_ == null) {
          containerIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder>(
                  containerId_,
                  getParentForChildren(),
                  isClean());
          containerId_ = null;
        }
        return containerIdBuilder_;
      }

      // required .hadoop.yarn.ContainerUpdateTypeProto update_type = 3;
      private org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerUpdateTypeProto updateType_ = org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerUpdateTypeProto.INCREASE_RESOURCE;
      /**
       * <code>required .hadoop.yarn.ContainerUpdateTypeProto update_type = 3;</code>
       */
      public boolean hasUpdateType() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>required .hadoop.yarn.ContainerUpdateTypeProto update_type = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerUpdateTypeProto getUpdateType() {
        return updateType_;
      }
      /**
       * <code>required .hadoop.yarn.ContainerUpdateTypeProto update_type = 3;</code>
       */
      public Builder setUpdateType(org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerUpdateTypeProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000004;
        updateType_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required .hadoop.yarn.ContainerUpdateTypeProto update_type = 3;</code>
       */
      public Builder clearUpdateType() {
        bitField0_ = (bitField0_ & ~0x00000004);
        updateType_ = org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerUpdateTypeProto.INCREASE_RESOURCE;
        onChanged();
        return this;
      }

      // optional .hadoop.yarn.ResourceProto capability = 4;
      private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto capability_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> capabilityBuilder_;
      /**
       * <code>optional .hadoop.yarn.ResourceProto capability = 4;</code>
       */
      public boolean hasCapability() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto capability = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getCapability() {
        if (capabilityBuilder_ == null) {
          return capability_;
        } else {
          return capabilityBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto capability = 4;</code>
       */
      public Builder setCapability(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (capabilityBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          capability_ = value;
          onChanged();
        } else {
          capabilityBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000008;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto capability = 4;</code>
       */
      public Builder setCapability(
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder builderForValue) {
        if (capabilityBuilder_ == null) {
          capability_ = builderForValue.build();
          onChanged();
        } else {
          capabilityBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000008;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto capability = 4;</code>
       */
      public Builder mergeCapability(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (capabilityBuilder_ == null) {
          if (((bitField0_ & 0x00000008) == 0x00000008) &&
              capability_ != org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance()) {
            capability_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.newBuilder(capability_).mergeFrom(value).buildPartial();
          } else {
            capability_ = value;
          }
          onChanged();
        } else {
          capabilityBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000008;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto capability = 4;</code>
       */
      public Builder clearCapability() {
        if (capabilityBuilder_ == null) {
          capability_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
          onChanged();
        } else {
          capabilityBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto capability = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder getCapabilityBuilder() {
        bitField0_ |= 0x00000008;
        onChanged();
        return getCapabilityFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto capability = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getCapabilityOrBuilder() {
        if (capabilityBuilder_ != null) {
          return capabilityBuilder_.getMessageOrBuilder();
        } else {
          return capability_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto capability = 4;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> 
          getCapabilityFieldBuilder() {
        if (capabilityBuilder_ == null) {
          capabilityBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder>(
                  capability_,
                  getParentForChildren(),
                  isClean());
          capability_ = null;
        }
        return capabilityBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.UpdateContainerRequestProto)
    }

    static {
      defaultInstance = new UpdateContainerRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.UpdateContainerRequestProto)
  }

  public interface UpdateContainerErrorProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional string reason = 1;
    /**
     * <code>optional string reason = 1;</code>
     */
    boolean hasReason();
    /**
     * <code>optional string reason = 1;</code>
     */
    java.lang.String getReason();
    /**
     * <code>optional string reason = 1;</code>
     */
    com.google.protobuf.ByteString
        getReasonBytes();

    // optional .hadoop.yarn.UpdateContainerRequestProto update_request = 2;
    /**
     * <code>optional .hadoop.yarn.UpdateContainerRequestProto update_request = 2;</code>
     */
    boolean hasUpdateRequest();
    /**
     * <code>optional .hadoop.yarn.UpdateContainerRequestProto update_request = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto getUpdateRequest();
    /**
     * <code>optional .hadoop.yarn.UpdateContainerRequestProto update_request = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProtoOrBuilder getUpdateRequestOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.UpdateContainerErrorProto}
   */
  public static final class UpdateContainerErrorProto extends
      com.google.protobuf.GeneratedMessage
      implements UpdateContainerErrorProtoOrBuilder {
    // Use UpdateContainerErrorProto.newBuilder() to construct.
    private UpdateContainerErrorProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private UpdateContainerErrorProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final UpdateContainerErrorProto defaultInstance;
    public static UpdateContainerErrorProto getDefaultInstance() {
      return defaultInstance;
    }

    public UpdateContainerErrorProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private UpdateContainerErrorProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              reason_ = input.readBytes();
              break;
            }
            case 18: {
              org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) == 0x00000002)) {
                subBuilder = updateRequest_.toBuilder();
              }
              updateRequest_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(updateRequest_);
                updateRequest_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdateContainerErrorProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdateContainerErrorProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto.Builder.class);
    }

    public static com.google.protobuf.Parser<UpdateContainerErrorProto> PARSER =
        new com.google.protobuf.AbstractParser<UpdateContainerErrorProto>() {
      public UpdateContainerErrorProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new UpdateContainerErrorProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<UpdateContainerErrorProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional string reason = 1;
    public static final int REASON_FIELD_NUMBER = 1;
    private java.lang.Object reason_;
    /**
     * <code>optional string reason = 1;</code>
     */
    public boolean hasReason() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional string reason = 1;</code>
     */
    public java.lang.String getReason() {
      java.lang.Object ref = reason_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          reason_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string reason = 1;</code>
     */
    public com.google.protobuf.ByteString
        getReasonBytes() {
      java.lang.Object ref = reason_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        reason_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional .hadoop.yarn.UpdateContainerRequestProto update_request = 2;
    public static final int UPDATE_REQUEST_FIELD_NUMBER = 2;
    private org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto updateRequest_;
    /**
     * <code>optional .hadoop.yarn.UpdateContainerRequestProto update_request = 2;</code>
     */
    public boolean hasUpdateRequest() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional .hadoop.yarn.UpdateContainerRequestProto update_request = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto getUpdateRequest() {
      return updateRequest_;
    }
    /**
     * <code>optional .hadoop.yarn.UpdateContainerRequestProto update_request = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProtoOrBuilder getUpdateRequestOrBuilder() {
      return updateRequest_;
    }

    private void initFields() {
      reason_ = "";
      updateRequest_ = org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (hasUpdateRequest()) {
        if (!getUpdateRequest().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getReasonBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeMessage(2, updateRequest_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getReasonBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, updateRequest_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto) obj;

      boolean result = true;
      result = result && (hasReason() == other.hasReason());
      if (hasReason()) {
        result = result && getReason()
            .equals(other.getReason());
      }
      result = result && (hasUpdateRequest() == other.hasUpdateRequest());
      if (hasUpdateRequest()) {
        result = result && getUpdateRequest()
            .equals(other.getUpdateRequest());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasReason()) {
        hash = (37 * hash) + REASON_FIELD_NUMBER;
        hash = (53 * hash) + getReason().hashCode();
      }
      if (hasUpdateRequest()) {
        hash = (37 * hash) + UPDATE_REQUEST_FIELD_NUMBER;
        hash = (53 * hash) + getUpdateRequest().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.UpdateContainerErrorProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdateContainerErrorProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdateContainerErrorProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getUpdateRequestFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        reason_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        if (updateRequestBuilder_ == null) {
          updateRequest_ = org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.getDefaultInstance();
        } else {
          updateRequestBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdateContainerErrorProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.reason_ = reason_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        if (updateRequestBuilder_ == null) {
          result.updateRequest_ = updateRequest_;
        } else {
          result.updateRequest_ = updateRequestBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto.getDefaultInstance()) return this;
        if (other.hasReason()) {
          bitField0_ |= 0x00000001;
          reason_ = other.reason_;
          onChanged();
        }
        if (other.hasUpdateRequest()) {
          mergeUpdateRequest(other.getUpdateRequest());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (hasUpdateRequest()) {
          if (!getUpdateRequest().isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional string reason = 1;
      private java.lang.Object reason_ = "";
      /**
       * <code>optional string reason = 1;</code>
       */
      public boolean hasReason() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional string reason = 1;</code>
       */
      public java.lang.String getReason() {
        java.lang.Object ref = reason_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          reason_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string reason = 1;</code>
       */
      public com.google.protobuf.ByteString
          getReasonBytes() {
        java.lang.Object ref = reason_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          reason_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string reason = 1;</code>
       */
      public Builder setReason(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        reason_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string reason = 1;</code>
       */
      public Builder clearReason() {
        bitField0_ = (bitField0_ & ~0x00000001);
        reason_ = getDefaultInstance().getReason();
        onChanged();
        return this;
      }
      /**
       * <code>optional string reason = 1;</code>
       */
      public Builder setReasonBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        reason_ = value;
        onChanged();
        return this;
      }

      // optional .hadoop.yarn.UpdateContainerRequestProto update_request = 2;
      private org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto updateRequest_ = org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProtoOrBuilder> updateRequestBuilder_;
      /**
       * <code>optional .hadoop.yarn.UpdateContainerRequestProto update_request = 2;</code>
       */
      public boolean hasUpdateRequest() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional .hadoop.yarn.UpdateContainerRequestProto update_request = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto getUpdateRequest() {
        if (updateRequestBuilder_ == null) {
          return updateRequest_;
        } else {
          return updateRequestBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.UpdateContainerRequestProto update_request = 2;</code>
       */
      public Builder setUpdateRequest(org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto value) {
        if (updateRequestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          updateRequest_ = value;
          onChanged();
        } else {
          updateRequestBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.UpdateContainerRequestProto update_request = 2;</code>
       */
      public Builder setUpdateRequest(
          org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.Builder builderForValue) {
        if (updateRequestBuilder_ == null) {
          updateRequest_ = builderForValue.build();
          onChanged();
        } else {
          updateRequestBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.UpdateContainerRequestProto update_request = 2;</code>
       */
      public Builder mergeUpdateRequest(org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto value) {
        if (updateRequestBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002) &&
              updateRequest_ != org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.getDefaultInstance()) {
            updateRequest_ =
              org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.newBuilder(updateRequest_).mergeFrom(value).buildPartial();
          } else {
            updateRequest_ = value;
          }
          onChanged();
        } else {
          updateRequestBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.UpdateContainerRequestProto update_request = 2;</code>
       */
      public Builder clearUpdateRequest() {
        if (updateRequestBuilder_ == null) {
          updateRequest_ = org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.getDefaultInstance();
          onChanged();
        } else {
          updateRequestBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.UpdateContainerRequestProto update_request = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.Builder getUpdateRequestBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getUpdateRequestFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.UpdateContainerRequestProto update_request = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProtoOrBuilder getUpdateRequestOrBuilder() {
        if (updateRequestBuilder_ != null) {
          return updateRequestBuilder_.getMessageOrBuilder();
        } else {
          return updateRequest_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.UpdateContainerRequestProto update_request = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProtoOrBuilder> 
          getUpdateRequestFieldBuilder() {
        if (updateRequestBuilder_ == null) {
          updateRequestBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProtoOrBuilder>(
                  updateRequest_,
                  getParentForChildren(),
                  isClean());
          updateRequest_ = null;
        }
        return updateRequestBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.UpdateContainerErrorProto)
    }

    static {
      defaultInstance = new UpdateContainerErrorProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.UpdateContainerErrorProto)
  }

  public interface AllocateRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.ResourceRequestProto ask = 1;
    /**
     * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto> 
        getAskList();
    /**
     * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto getAsk(int index);
    /**
     * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
     */
    int getAskCount();
    /**
     * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProtoOrBuilder> 
        getAskOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProtoOrBuilder getAskOrBuilder(
        int index);

    // repeated .hadoop.yarn.ContainerIdProto release = 2;
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> 
        getReleaseList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getRelease(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
     */
    int getReleaseCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getReleaseOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getReleaseOrBuilder(
        int index);

    // optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;
    /**
     * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
     */
    boolean hasBlacklistRequest();
    /**
     * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto getBlacklistRequest();
    /**
     * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProtoOrBuilder getBlacklistRequestOrBuilder();

    // optional int32 response_id = 4;
    /**
     * <code>optional int32 response_id = 4;</code>
     */
    boolean hasResponseId();
    /**
     * <code>optional int32 response_id = 4;</code>
     */
    int getResponseId();

    // optional float progress = 5;
    /**
     * <code>optional float progress = 5;</code>
     */
    boolean hasProgress();
    /**
     * <code>optional float progress = 5;</code>
     */
    float getProgress();

    // repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto> 
        getIncreaseRequestList();
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto getIncreaseRequest(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
     */
    int getIncreaseRequestCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProtoOrBuilder> 
        getIncreaseRequestOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProtoOrBuilder getIncreaseRequestOrBuilder(
        int index);

    // repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;
    /**
     * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto> 
        getUpdateRequestsList();
    /**
     * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto getUpdateRequests(int index);
    /**
     * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
     */
    int getUpdateRequestsCount();
    /**
     * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProtoOrBuilder> 
        getUpdateRequestsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProtoOrBuilder getUpdateRequestsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.AllocateRequestProto}
   */
  public static final class AllocateRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements AllocateRequestProtoOrBuilder {
    // Use AllocateRequestProto.newBuilder() to construct.
    private AllocateRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private AllocateRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final AllocateRequestProto defaultInstance;
    public static AllocateRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public AllocateRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private AllocateRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                ask_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              ask_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.PARSER, extensionRegistry));
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                release_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>();
                mutable_bitField0_ |= 0x00000002;
              }
              release_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.PARSER, extensionRegistry));
              break;
            }
            case 26: {
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = blacklistRequest_.toBuilder();
              }
              blacklistRequest_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(blacklistRequest_);
                blacklistRequest_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 32: {
              bitField0_ |= 0x00000002;
              responseId_ = input.readInt32();
              break;
            }
            case 45: {
              bitField0_ |= 0x00000004;
              progress_ = input.readFloat();
              break;
            }
            case 50: {
              if (!((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
                increaseRequest_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto>();
                mutable_bitField0_ |= 0x00000020;
              }
              increaseRequest_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.PARSER, extensionRegistry));
              break;
            }
            case 58: {
              if (!((mutable_bitField0_ & 0x00000040) == 0x00000040)) {
                updateRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto>();
                mutable_bitField0_ |= 0x00000040;
              }
              updateRequests_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          ask_ = java.util.Collections.unmodifiableList(ask_);
        }
        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
          release_ = java.util.Collections.unmodifiableList(release_);
        }
        if (((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
          increaseRequest_ = java.util.Collections.unmodifiableList(increaseRequest_);
        }
        if (((mutable_bitField0_ & 0x00000040) == 0x00000040)) {
          updateRequests_ = java.util.Collections.unmodifiableList(updateRequests_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_AllocateRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_AllocateRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<AllocateRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<AllocateRequestProto>() {
      public AllocateRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new AllocateRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<AllocateRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // repeated .hadoop.yarn.ResourceRequestProto ask = 1;
    public static final int ASK_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto> ask_;
    /**
     * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto> getAskList() {
      return ask_;
    }
    /**
     * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProtoOrBuilder> 
        getAskOrBuilderList() {
      return ask_;
    }
    /**
     * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
     */
    public int getAskCount() {
      return ask_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto getAsk(int index) {
      return ask_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProtoOrBuilder getAskOrBuilder(
        int index) {
      return ask_.get(index);
    }

    // repeated .hadoop.yarn.ContainerIdProto release = 2;
    public static final int RELEASE_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> release_;
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getReleaseList() {
      return release_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getReleaseOrBuilderList() {
      return release_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
     */
    public int getReleaseCount() {
      return release_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getRelease(int index) {
      return release_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getReleaseOrBuilder(
        int index) {
      return release_.get(index);
    }

    // optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;
    public static final int BLACKLIST_REQUEST_FIELD_NUMBER = 3;
    private org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto blacklistRequest_;
    /**
     * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
     */
    public boolean hasBlacklistRequest() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto getBlacklistRequest() {
      return blacklistRequest_;
    }
    /**
     * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProtoOrBuilder getBlacklistRequestOrBuilder() {
      return blacklistRequest_;
    }

    // optional int32 response_id = 4;
    public static final int RESPONSE_ID_FIELD_NUMBER = 4;
    private int responseId_;
    /**
     * <code>optional int32 response_id = 4;</code>
     */
    public boolean hasResponseId() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional int32 response_id = 4;</code>
     */
    public int getResponseId() {
      return responseId_;
    }

    // optional float progress = 5;
    public static final int PROGRESS_FIELD_NUMBER = 5;
    private float progress_;
    /**
     * <code>optional float progress = 5;</code>
     */
    public boolean hasProgress() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional float progress = 5;</code>
     */
    public float getProgress() {
      return progress_;
    }

    // repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;
    public static final int INCREASE_REQUEST_FIELD_NUMBER = 6;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto> increaseRequest_;
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto> getIncreaseRequestList() {
      return increaseRequest_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProtoOrBuilder> 
        getIncreaseRequestOrBuilderList() {
      return increaseRequest_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
     */
    public int getIncreaseRequestCount() {
      return increaseRequest_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto getIncreaseRequest(int index) {
      return increaseRequest_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProtoOrBuilder getIncreaseRequestOrBuilder(
        int index) {
      return increaseRequest_.get(index);
    }

    // repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;
    public static final int UPDATE_REQUESTS_FIELD_NUMBER = 7;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto> updateRequests_;
    /**
     * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto> getUpdateRequestsList() {
      return updateRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProtoOrBuilder> 
        getUpdateRequestsOrBuilderList() {
      return updateRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
     */
    public int getUpdateRequestsCount() {
      return updateRequests_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto getUpdateRequests(int index) {
      return updateRequests_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProtoOrBuilder getUpdateRequestsOrBuilder(
        int index) {
      return updateRequests_.get(index);
    }

    private void initFields() {
      ask_ = java.util.Collections.emptyList();
      release_ = java.util.Collections.emptyList();
      blacklistRequest_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.getDefaultInstance();
      responseId_ = 0;
      progress_ = 0F;
      increaseRequest_ = java.util.Collections.emptyList();
      updateRequests_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      for (int i = 0; i < getUpdateRequestsCount(); i++) {
        if (!getUpdateRequests(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < ask_.size(); i++) {
        output.writeMessage(1, ask_.get(i));
      }
      for (int i = 0; i < release_.size(); i++) {
        output.writeMessage(2, release_.get(i));
      }
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(3, blacklistRequest_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeInt32(4, responseId_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeFloat(5, progress_);
      }
      for (int i = 0; i < increaseRequest_.size(); i++) {
        output.writeMessage(6, increaseRequest_.get(i));
      }
      for (int i = 0; i < updateRequests_.size(); i++) {
        output.writeMessage(7, updateRequests_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < ask_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, ask_.get(i));
      }
      for (int i = 0; i < release_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, release_.get(i));
      }
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, blacklistRequest_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(4, responseId_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeFloatSize(5, progress_);
      }
      for (int i = 0; i < increaseRequest_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, increaseRequest_.get(i));
      }
      for (int i = 0; i < updateRequests_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(7, updateRequests_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto) obj;

      boolean result = true;
      result = result && getAskList()
          .equals(other.getAskList());
      result = result && getReleaseList()
          .equals(other.getReleaseList());
      result = result && (hasBlacklistRequest() == other.hasBlacklistRequest());
      if (hasBlacklistRequest()) {
        result = result && getBlacklistRequest()
            .equals(other.getBlacklistRequest());
      }
      result = result && (hasResponseId() == other.hasResponseId());
      if (hasResponseId()) {
        result = result && (getResponseId()
            == other.getResponseId());
      }
      result = result && (hasProgress() == other.hasProgress());
      if (hasProgress()) {
        result = result && (Float.floatToIntBits(getProgress())    == Float.floatToIntBits(other.getProgress()));
      }
      result = result && getIncreaseRequestList()
          .equals(other.getIncreaseRequestList());
      result = result && getUpdateRequestsList()
          .equals(other.getUpdateRequestsList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getAskCount() > 0) {
        hash = (37 * hash) + ASK_FIELD_NUMBER;
        hash = (53 * hash) + getAskList().hashCode();
      }
      if (getReleaseCount() > 0) {
        hash = (37 * hash) + RELEASE_FIELD_NUMBER;
        hash = (53 * hash) + getReleaseList().hashCode();
      }
      if (hasBlacklistRequest()) {
        hash = (37 * hash) + BLACKLIST_REQUEST_FIELD_NUMBER;
        hash = (53 * hash) + getBlacklistRequest().hashCode();
      }
      if (hasResponseId()) {
        hash = (37 * hash) + RESPONSE_ID_FIELD_NUMBER;
        hash = (53 * hash) + getResponseId();
      }
      if (hasProgress()) {
        hash = (37 * hash) + PROGRESS_FIELD_NUMBER;
        hash = (53 * hash) + Float.floatToIntBits(
            getProgress());
      }
      if (getIncreaseRequestCount() > 0) {
        hash = (37 * hash) + INCREASE_REQUEST_FIELD_NUMBER;
        hash = (53 * hash) + getIncreaseRequestList().hashCode();
      }
      if (getUpdateRequestsCount() > 0) {
        hash = (37 * hash) + UPDATE_REQUESTS_FIELD_NUMBER;
        hash = (53 * hash) + getUpdateRequestsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.AllocateRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_AllocateRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_AllocateRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getAskFieldBuilder();
          getReleaseFieldBuilder();
          getBlacklistRequestFieldBuilder();
          getIncreaseRequestFieldBuilder();
          getUpdateRequestsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (askBuilder_ == null) {
          ask_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          askBuilder_.clear();
        }
        if (releaseBuilder_ == null) {
          release_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          releaseBuilder_.clear();
        }
        if (blacklistRequestBuilder_ == null) {
          blacklistRequest_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.getDefaultInstance();
        } else {
          blacklistRequestBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        responseId_ = 0;
        bitField0_ = (bitField0_ & ~0x00000008);
        progress_ = 0F;
        bitField0_ = (bitField0_ & ~0x00000010);
        if (increaseRequestBuilder_ == null) {
          increaseRequest_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
        } else {
          increaseRequestBuilder_.clear();
        }
        if (updateRequestsBuilder_ == null) {
          updateRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000040);
        } else {
          updateRequestsBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_AllocateRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (askBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            ask_ = java.util.Collections.unmodifiableList(ask_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.ask_ = ask_;
        } else {
          result.ask_ = askBuilder_.build();
        }
        if (releaseBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002)) {
            release_ = java.util.Collections.unmodifiableList(release_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.release_ = release_;
        } else {
          result.release_ = releaseBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000001;
        }
        if (blacklistRequestBuilder_ == null) {
          result.blacklistRequest_ = blacklistRequest_;
        } else {
          result.blacklistRequest_ = blacklistRequestBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000002;
        }
        result.responseId_ = responseId_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000004;
        }
        result.progress_ = progress_;
        if (increaseRequestBuilder_ == null) {
          if (((bitField0_ & 0x00000020) == 0x00000020)) {
            increaseRequest_ = java.util.Collections.unmodifiableList(increaseRequest_);
            bitField0_ = (bitField0_ & ~0x00000020);
          }
          result.increaseRequest_ = increaseRequest_;
        } else {
          result.increaseRequest_ = increaseRequestBuilder_.build();
        }
        if (updateRequestsBuilder_ == null) {
          if (((bitField0_ & 0x00000040) == 0x00000040)) {
            updateRequests_ = java.util.Collections.unmodifiableList(updateRequests_);
            bitField0_ = (bitField0_ & ~0x00000040);
          }
          result.updateRequests_ = updateRequests_;
        } else {
          result.updateRequests_ = updateRequestsBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto.getDefaultInstance()) return this;
        if (askBuilder_ == null) {
          if (!other.ask_.isEmpty()) {
            if (ask_.isEmpty()) {
              ask_ = other.ask_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureAskIsMutable();
              ask_.addAll(other.ask_);
            }
            onChanged();
          }
        } else {
          if (!other.ask_.isEmpty()) {
            if (askBuilder_.isEmpty()) {
              askBuilder_.dispose();
              askBuilder_ = null;
              ask_ = other.ask_;
              bitField0_ = (bitField0_ & ~0x00000001);
              askBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getAskFieldBuilder() : null;
            } else {
              askBuilder_.addAllMessages(other.ask_);
            }
          }
        }
        if (releaseBuilder_ == null) {
          if (!other.release_.isEmpty()) {
            if (release_.isEmpty()) {
              release_ = other.release_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureReleaseIsMutable();
              release_.addAll(other.release_);
            }
            onChanged();
          }
        } else {
          if (!other.release_.isEmpty()) {
            if (releaseBuilder_.isEmpty()) {
              releaseBuilder_.dispose();
              releaseBuilder_ = null;
              release_ = other.release_;
              bitField0_ = (bitField0_ & ~0x00000002);
              releaseBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getReleaseFieldBuilder() : null;
            } else {
              releaseBuilder_.addAllMessages(other.release_);
            }
          }
        }
        if (other.hasBlacklistRequest()) {
          mergeBlacklistRequest(other.getBlacklistRequest());
        }
        if (other.hasResponseId()) {
          setResponseId(other.getResponseId());
        }
        if (other.hasProgress()) {
          setProgress(other.getProgress());
        }
        if (increaseRequestBuilder_ == null) {
          if (!other.increaseRequest_.isEmpty()) {
            if (increaseRequest_.isEmpty()) {
              increaseRequest_ = other.increaseRequest_;
              bitField0_ = (bitField0_ & ~0x00000020);
            } else {
              ensureIncreaseRequestIsMutable();
              increaseRequest_.addAll(other.increaseRequest_);
            }
            onChanged();
          }
        } else {
          if (!other.increaseRequest_.isEmpty()) {
            if (increaseRequestBuilder_.isEmpty()) {
              increaseRequestBuilder_.dispose();
              increaseRequestBuilder_ = null;
              increaseRequest_ = other.increaseRequest_;
              bitField0_ = (bitField0_ & ~0x00000020);
              increaseRequestBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getIncreaseRequestFieldBuilder() : null;
            } else {
              increaseRequestBuilder_.addAllMessages(other.increaseRequest_);
            }
          }
        }
        if (updateRequestsBuilder_ == null) {
          if (!other.updateRequests_.isEmpty()) {
            if (updateRequests_.isEmpty()) {
              updateRequests_ = other.updateRequests_;
              bitField0_ = (bitField0_ & ~0x00000040);
            } else {
              ensureUpdateRequestsIsMutable();
              updateRequests_.addAll(other.updateRequests_);
            }
            onChanged();
          }
        } else {
          if (!other.updateRequests_.isEmpty()) {
            if (updateRequestsBuilder_.isEmpty()) {
              updateRequestsBuilder_.dispose();
              updateRequestsBuilder_ = null;
              updateRequests_ = other.updateRequests_;
              bitField0_ = (bitField0_ & ~0x00000040);
              updateRequestsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getUpdateRequestsFieldBuilder() : null;
            } else {
              updateRequestsBuilder_.addAllMessages(other.updateRequests_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        for (int i = 0; i < getUpdateRequestsCount(); i++) {
          if (!getUpdateRequests(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.ResourceRequestProto ask = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto> ask_ =
        java.util.Collections.emptyList();
      private void ensureAskIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          ask_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto>(ask_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProtoOrBuilder> askBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto> getAskList() {
        if (askBuilder_ == null) {
          return java.util.Collections.unmodifiableList(ask_);
        } else {
          return askBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public int getAskCount() {
        if (askBuilder_ == null) {
          return ask_.size();
        } else {
          return askBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto getAsk(int index) {
        if (askBuilder_ == null) {
          return ask_.get(index);
        } else {
          return askBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public Builder setAsk(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto value) {
        if (askBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAskIsMutable();
          ask_.set(index, value);
          onChanged();
        } else {
          askBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public Builder setAsk(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.Builder builderForValue) {
        if (askBuilder_ == null) {
          ensureAskIsMutable();
          ask_.set(index, builderForValue.build());
          onChanged();
        } else {
          askBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public Builder addAsk(org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto value) {
        if (askBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAskIsMutable();
          ask_.add(value);
          onChanged();
        } else {
          askBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public Builder addAsk(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto value) {
        if (askBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAskIsMutable();
          ask_.add(index, value);
          onChanged();
        } else {
          askBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public Builder addAsk(
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.Builder builderForValue) {
        if (askBuilder_ == null) {
          ensureAskIsMutable();
          ask_.add(builderForValue.build());
          onChanged();
        } else {
          askBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public Builder addAsk(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.Builder builderForValue) {
        if (askBuilder_ == null) {
          ensureAskIsMutable();
          ask_.add(index, builderForValue.build());
          onChanged();
        } else {
          askBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public Builder addAllAsk(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto> values) {
        if (askBuilder_ == null) {
          ensureAskIsMutable();
          super.addAll(values, ask_);
          onChanged();
        } else {
          askBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public Builder clearAsk() {
        if (askBuilder_ == null) {
          ask_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          askBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public Builder removeAsk(int index) {
        if (askBuilder_ == null) {
          ensureAskIsMutable();
          ask_.remove(index);
          onChanged();
        } else {
          askBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.Builder getAskBuilder(
          int index) {
        return getAskFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProtoOrBuilder getAskOrBuilder(
          int index) {
        if (askBuilder_ == null) {
          return ask_.get(index);  } else {
          return askBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProtoOrBuilder> 
           getAskOrBuilderList() {
        if (askBuilder_ != null) {
          return askBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(ask_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.Builder addAskBuilder() {
        return getAskFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.Builder addAskBuilder(
          int index) {
        return getAskFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ResourceRequestProto ask = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.Builder> 
           getAskBuilderList() {
        return getAskFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProtoOrBuilder> 
          getAskFieldBuilder() {
        if (askBuilder_ == null) {
          askBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceRequestProtoOrBuilder>(
                  ask_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          ask_ = null;
        }
        return askBuilder_;
      }

      // repeated .hadoop.yarn.ContainerIdProto release = 2;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> release_ =
        java.util.Collections.emptyList();
      private void ensureReleaseIsMutable() {
        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
          release_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>(release_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> releaseBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getReleaseList() {
        if (releaseBuilder_ == null) {
          return java.util.Collections.unmodifiableList(release_);
        } else {
          return releaseBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public int getReleaseCount() {
        if (releaseBuilder_ == null) {
          return release_.size();
        } else {
          return releaseBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getRelease(int index) {
        if (releaseBuilder_ == null) {
          return release_.get(index);
        } else {
          return releaseBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public Builder setRelease(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (releaseBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureReleaseIsMutable();
          release_.set(index, value);
          onChanged();
        } else {
          releaseBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public Builder setRelease(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (releaseBuilder_ == null) {
          ensureReleaseIsMutable();
          release_.set(index, builderForValue.build());
          onChanged();
        } else {
          releaseBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public Builder addRelease(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (releaseBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureReleaseIsMutable();
          release_.add(value);
          onChanged();
        } else {
          releaseBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public Builder addRelease(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (releaseBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureReleaseIsMutable();
          release_.add(index, value);
          onChanged();
        } else {
          releaseBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public Builder addRelease(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (releaseBuilder_ == null) {
          ensureReleaseIsMutable();
          release_.add(builderForValue.build());
          onChanged();
        } else {
          releaseBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public Builder addRelease(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (releaseBuilder_ == null) {
          ensureReleaseIsMutable();
          release_.add(index, builderForValue.build());
          onChanged();
        } else {
          releaseBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public Builder addAllRelease(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> values) {
        if (releaseBuilder_ == null) {
          ensureReleaseIsMutable();
          super.addAll(values, release_);
          onChanged();
        } else {
          releaseBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public Builder clearRelease() {
        if (releaseBuilder_ == null) {
          release_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          releaseBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public Builder removeRelease(int index) {
        if (releaseBuilder_ == null) {
          ensureReleaseIsMutable();
          release_.remove(index);
          onChanged();
        } else {
          releaseBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder getReleaseBuilder(
          int index) {
        return getReleaseFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getReleaseOrBuilder(
          int index) {
        if (releaseBuilder_ == null) {
          return release_.get(index);  } else {
          return releaseBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
           getReleaseOrBuilderList() {
        if (releaseBuilder_ != null) {
          return releaseBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(release_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addReleaseBuilder() {
        return getReleaseFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addReleaseBuilder(
          int index) {
        return getReleaseFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto release = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder> 
           getReleaseBuilderList() {
        return getReleaseFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
          getReleaseFieldBuilder() {
        if (releaseBuilder_ == null) {
          releaseBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder>(
                  release_,
                  ((bitField0_ & 0x00000002) == 0x00000002),
                  getParentForChildren(),
                  isClean());
          release_ = null;
        }
        return releaseBuilder_;
      }

      // optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;
      private org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto blacklistRequest_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProtoOrBuilder> blacklistRequestBuilder_;
      /**
       * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
       */
      public boolean hasBlacklistRequest() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto getBlacklistRequest() {
        if (blacklistRequestBuilder_ == null) {
          return blacklistRequest_;
        } else {
          return blacklistRequestBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
       */
      public Builder setBlacklistRequest(org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto value) {
        if (blacklistRequestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          blacklistRequest_ = value;
          onChanged();
        } else {
          blacklistRequestBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
       */
      public Builder setBlacklistRequest(
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.Builder builderForValue) {
        if (blacklistRequestBuilder_ == null) {
          blacklistRequest_ = builderForValue.build();
          onChanged();
        } else {
          blacklistRequestBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
       */
      public Builder mergeBlacklistRequest(org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto value) {
        if (blacklistRequestBuilder_ == null) {
          if (((bitField0_ & 0x00000004) == 0x00000004) &&
              blacklistRequest_ != org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.getDefaultInstance()) {
            blacklistRequest_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.newBuilder(blacklistRequest_).mergeFrom(value).buildPartial();
          } else {
            blacklistRequest_ = value;
          }
          onChanged();
        } else {
          blacklistRequestBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
       */
      public Builder clearBlacklistRequest() {
        if (blacklistRequestBuilder_ == null) {
          blacklistRequest_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.getDefaultInstance();
          onChanged();
        } else {
          blacklistRequestBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.Builder getBlacklistRequestBuilder() {
        bitField0_ |= 0x00000004;
        onChanged();
        return getBlacklistRequestFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProtoOrBuilder getBlacklistRequestOrBuilder() {
        if (blacklistRequestBuilder_ != null) {
          return blacklistRequestBuilder_.getMessageOrBuilder();
        } else {
          return blacklistRequest_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceBlacklistRequestProto blacklist_request = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProtoOrBuilder> 
          getBlacklistRequestFieldBuilder() {
        if (blacklistRequestBuilder_ == null) {
          blacklistRequestBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceBlacklistRequestProtoOrBuilder>(
                  blacklistRequest_,
                  getParentForChildren(),
                  isClean());
          blacklistRequest_ = null;
        }
        return blacklistRequestBuilder_;
      }

      // optional int32 response_id = 4;
      private int responseId_ ;
      /**
       * <code>optional int32 response_id = 4;</code>
       */
      public boolean hasResponseId() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional int32 response_id = 4;</code>
       */
      public int getResponseId() {
        return responseId_;
      }
      /**
       * <code>optional int32 response_id = 4;</code>
       */
      public Builder setResponseId(int value) {
        bitField0_ |= 0x00000008;
        responseId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 response_id = 4;</code>
       */
      public Builder clearResponseId() {
        bitField0_ = (bitField0_ & ~0x00000008);
        responseId_ = 0;
        onChanged();
        return this;
      }

      // optional float progress = 5;
      private float progress_ ;
      /**
       * <code>optional float progress = 5;</code>
       */
      public boolean hasProgress() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional float progress = 5;</code>
       */
      public float getProgress() {
        return progress_;
      }
      /**
       * <code>optional float progress = 5;</code>
       */
      public Builder setProgress(float value) {
        bitField0_ |= 0x00000010;
        progress_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional float progress = 5;</code>
       */
      public Builder clearProgress() {
        bitField0_ = (bitField0_ & ~0x00000010);
        progress_ = 0F;
        onChanged();
        return this;
      }

      // repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto> increaseRequest_ =
        java.util.Collections.emptyList();
      private void ensureIncreaseRequestIsMutable() {
        if (!((bitField0_ & 0x00000020) == 0x00000020)) {
          increaseRequest_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto>(increaseRequest_);
          bitField0_ |= 0x00000020;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProtoOrBuilder> increaseRequestBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto> getIncreaseRequestList() {
        if (increaseRequestBuilder_ == null) {
          return java.util.Collections.unmodifiableList(increaseRequest_);
        } else {
          return increaseRequestBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public int getIncreaseRequestCount() {
        if (increaseRequestBuilder_ == null) {
          return increaseRequest_.size();
        } else {
          return increaseRequestBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto getIncreaseRequest(int index) {
        if (increaseRequestBuilder_ == null) {
          return increaseRequest_.get(index);
        } else {
          return increaseRequestBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public Builder setIncreaseRequest(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto value) {
        if (increaseRequestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureIncreaseRequestIsMutable();
          increaseRequest_.set(index, value);
          onChanged();
        } else {
          increaseRequestBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public Builder setIncreaseRequest(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.Builder builderForValue) {
        if (increaseRequestBuilder_ == null) {
          ensureIncreaseRequestIsMutable();
          increaseRequest_.set(index, builderForValue.build());
          onChanged();
        } else {
          increaseRequestBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public Builder addIncreaseRequest(org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto value) {
        if (increaseRequestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureIncreaseRequestIsMutable();
          increaseRequest_.add(value);
          onChanged();
        } else {
          increaseRequestBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public Builder addIncreaseRequest(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto value) {
        if (increaseRequestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureIncreaseRequestIsMutable();
          increaseRequest_.add(index, value);
          onChanged();
        } else {
          increaseRequestBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public Builder addIncreaseRequest(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.Builder builderForValue) {
        if (increaseRequestBuilder_ == null) {
          ensureIncreaseRequestIsMutable();
          increaseRequest_.add(builderForValue.build());
          onChanged();
        } else {
          increaseRequestBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public Builder addIncreaseRequest(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.Builder builderForValue) {
        if (increaseRequestBuilder_ == null) {
          ensureIncreaseRequestIsMutable();
          increaseRequest_.add(index, builderForValue.build());
          onChanged();
        } else {
          increaseRequestBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public Builder addAllIncreaseRequest(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto> values) {
        if (increaseRequestBuilder_ == null) {
          ensureIncreaseRequestIsMutable();
          super.addAll(values, increaseRequest_);
          onChanged();
        } else {
          increaseRequestBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public Builder clearIncreaseRequest() {
        if (increaseRequestBuilder_ == null) {
          increaseRequest_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
          onChanged();
        } else {
          increaseRequestBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public Builder removeIncreaseRequest(int index) {
        if (increaseRequestBuilder_ == null) {
          ensureIncreaseRequestIsMutable();
          increaseRequest_.remove(index);
          onChanged();
        } else {
          increaseRequestBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.Builder getIncreaseRequestBuilder(
          int index) {
        return getIncreaseRequestFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProtoOrBuilder getIncreaseRequestOrBuilder(
          int index) {
        if (increaseRequestBuilder_ == null) {
          return increaseRequest_.get(index);  } else {
          return increaseRequestBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProtoOrBuilder> 
           getIncreaseRequestOrBuilderList() {
        if (increaseRequestBuilder_ != null) {
          return increaseRequestBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(increaseRequest_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.Builder addIncreaseRequestBuilder() {
        return getIncreaseRequestFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.Builder addIncreaseRequestBuilder(
          int index) {
        return getIncreaseRequestFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseRequestProto increase_request = 6;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.Builder> 
           getIncreaseRequestBuilderList() {
        return getIncreaseRequestFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProtoOrBuilder> 
          getIncreaseRequestFieldBuilder() {
        if (increaseRequestBuilder_ == null) {
          increaseRequestBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseRequestProtoOrBuilder>(
                  increaseRequest_,
                  ((bitField0_ & 0x00000020) == 0x00000020),
                  getParentForChildren(),
                  isClean());
          increaseRequest_ = null;
        }
        return increaseRequestBuilder_;
      }

      // repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto> updateRequests_ =
        java.util.Collections.emptyList();
      private void ensureUpdateRequestsIsMutable() {
        if (!((bitField0_ & 0x00000040) == 0x00000040)) {
          updateRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto>(updateRequests_);
          bitField0_ |= 0x00000040;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProtoOrBuilder> updateRequestsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto> getUpdateRequestsList() {
        if (updateRequestsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(updateRequests_);
        } else {
          return updateRequestsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
       */
      public int getUpdateRequestsCount() {
        if (updateRequestsBuilder_ == null) {
          return updateRequests_.size();
        } else {
          return updateRequestsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto getUpdateRequests(int index) {
        if (updateRequestsBuilder_ == null) {
          return updateRequests_.get(index);
        } else {
          return updateRequestsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
       */
      public Builder setUpdateRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto value) {
        if (updateRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureUpdateRequestsIsMutable();
          updateRequests_.set(index, value);
          onChanged();
        } else {
          updateRequestsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
       */
      public Builder setUpdateRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.Builder builderForValue) {
        if (updateRequestsBuilder_ == null) {
          ensureUpdateRequestsIsMutable();
          updateRequests_.set(index, builderForValue.build());
          onChanged();
        } else {
          updateRequestsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
       */
      public Builder addUpdateRequests(org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto value) {
        if (updateRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureUpdateRequestsIsMutable();
          updateRequests_.add(value);
          onChanged();
        } else {
          updateRequestsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
       */
      public Builder addUpdateRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto value) {
        if (updateRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureUpdateRequestsIsMutable();
          updateRequests_.add(index, value);
          onChanged();
        } else {
          updateRequestsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
       */
      public Builder addUpdateRequests(
          org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.Builder builderForValue) {
        if (updateRequestsBuilder_ == null) {
          ensureUpdateRequestsIsMutable();
          updateRequests_.add(builderForValue.build());
          onChanged();
        } else {
          updateRequestsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
       */
      public Builder addUpdateRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.Builder builderForValue) {
        if (updateRequestsBuilder_ == null) {
          ensureUpdateRequestsIsMutable();
          updateRequests_.add(index, builderForValue.build());
          onChanged();
        } else {
          updateRequestsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
       */
      public Builder addAllUpdateRequests(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto> values) {
        if (updateRequestsBuilder_ == null) {
          ensureUpdateRequestsIsMutable();
          super.addAll(values, updateRequests_);
          onChanged();
        } else {
          updateRequestsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
       */
      public Builder clearUpdateRequests() {
        if (updateRequestsBuilder_ == null) {
          updateRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000040);
          onChanged();
        } else {
          updateRequestsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
       */
      public Builder removeUpdateRequests(int index) {
        if (updateRequestsBuilder_ == null) {
          ensureUpdateRequestsIsMutable();
          updateRequests_.remove(index);
          onChanged();
        } else {
          updateRequestsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.Builder getUpdateRequestsBuilder(
          int index) {
        return getUpdateRequestsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProtoOrBuilder getUpdateRequestsOrBuilder(
          int index) {
        if (updateRequestsBuilder_ == null) {
          return updateRequests_.get(index);  } else {
          return updateRequestsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProtoOrBuilder> 
           getUpdateRequestsOrBuilderList() {
        if (updateRequestsBuilder_ != null) {
          return updateRequestsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(updateRequests_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.Builder addUpdateRequestsBuilder() {
        return getUpdateRequestsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.Builder addUpdateRequestsBuilder(
          int index) {
        return getUpdateRequestsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerRequestProto update_requests = 7;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.Builder> 
           getUpdateRequestsBuilderList() {
        return getUpdateRequestsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProtoOrBuilder> 
          getUpdateRequestsFieldBuilder() {
        if (updateRequestsBuilder_ == null) {
          updateRequestsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerRequestProtoOrBuilder>(
                  updateRequests_,
                  ((bitField0_ & 0x00000040) == 0x00000040),
                  getParentForChildren(),
                  isClean());
          updateRequests_ = null;
        }
        return updateRequestsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.AllocateRequestProto)
    }

    static {
      defaultInstance = new AllocateRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.AllocateRequestProto)
  }

  public interface NMTokenProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.NodeIdProto nodeId = 1;
    /**
     * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
     */
    boolean hasNodeId();
    /**
     * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto getNodeId();
    /**
     * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder getNodeIdOrBuilder();

    // optional .hadoop.common.TokenProto token = 2;
    /**
     * <code>optional .hadoop.common.TokenProto token = 2;</code>
     */
    boolean hasToken();
    /**
     * <code>optional .hadoop.common.TokenProto token = 2;</code>
     */
    org.apache.hadoop.security.proto.SecurityProtos.TokenProto getToken();
    /**
     * <code>optional .hadoop.common.TokenProto token = 2;</code>
     */
    org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder getTokenOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.NMTokenProto}
   */
  public static final class NMTokenProto extends
      com.google.protobuf.GeneratedMessage
      implements NMTokenProtoOrBuilder {
    // Use NMTokenProto.newBuilder() to construct.
    private NMTokenProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private NMTokenProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final NMTokenProto defaultInstance;
    public static NMTokenProto getDefaultInstance() {
      return defaultInstance;
    }

    public NMTokenProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private NMTokenProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = nodeId_.toBuilder();
              }
              nodeId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(nodeId_);
                nodeId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) == 0x00000002)) {
                subBuilder = token_.toBuilder();
              }
              token_ = input.readMessage(org.apache.hadoop.security.proto.SecurityProtos.TokenProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(token_);
                token_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_NMTokenProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_NMTokenProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder.class);
    }

    public static com.google.protobuf.Parser<NMTokenProto> PARSER =
        new com.google.protobuf.AbstractParser<NMTokenProto>() {
      public NMTokenProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new NMTokenProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<NMTokenProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.NodeIdProto nodeId = 1;
    public static final int NODEID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto nodeId_;
    /**
     * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
     */
    public boolean hasNodeId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto getNodeId() {
      return nodeId_;
    }
    /**
     * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder getNodeIdOrBuilder() {
      return nodeId_;
    }

    // optional .hadoop.common.TokenProto token = 2;
    public static final int TOKEN_FIELD_NUMBER = 2;
    private org.apache.hadoop.security.proto.SecurityProtos.TokenProto token_;
    /**
     * <code>optional .hadoop.common.TokenProto token = 2;</code>
     */
    public boolean hasToken() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional .hadoop.common.TokenProto token = 2;</code>
     */
    public org.apache.hadoop.security.proto.SecurityProtos.TokenProto getToken() {
      return token_;
    }
    /**
     * <code>optional .hadoop.common.TokenProto token = 2;</code>
     */
    public org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder getTokenOrBuilder() {
      return token_;
    }

    private void initFields() {
      nodeId_ = org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance();
      token_ = org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (hasToken()) {
        if (!getToken().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, nodeId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeMessage(2, token_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, nodeId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, token_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto) obj;

      boolean result = true;
      result = result && (hasNodeId() == other.hasNodeId());
      if (hasNodeId()) {
        result = result && getNodeId()
            .equals(other.getNodeId());
      }
      result = result && (hasToken() == other.hasToken());
      if (hasToken()) {
        result = result && getToken()
            .equals(other.getToken());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasNodeId()) {
        hash = (37 * hash) + NODEID_FIELD_NUMBER;
        hash = (53 * hash) + getNodeId().hashCode();
      }
      if (hasToken()) {
        hash = (37 * hash) + TOKEN_FIELD_NUMBER;
        hash = (53 * hash) + getToken().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.NMTokenProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_NMTokenProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_NMTokenProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getNodeIdFieldBuilder();
          getTokenFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (nodeIdBuilder_ == null) {
          nodeId_ = org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance();
        } else {
          nodeIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (tokenBuilder_ == null) {
          token_ = org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance();
        } else {
          tokenBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_NMTokenProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (nodeIdBuilder_ == null) {
          result.nodeId_ = nodeId_;
        } else {
          result.nodeId_ = nodeIdBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        if (tokenBuilder_ == null) {
          result.token_ = token_;
        } else {
          result.token_ = tokenBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.getDefaultInstance()) return this;
        if (other.hasNodeId()) {
          mergeNodeId(other.getNodeId());
        }
        if (other.hasToken()) {
          mergeToken(other.getToken());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (hasToken()) {
          if (!getToken().isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.NodeIdProto nodeId = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto nodeId_ = org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder> nodeIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
       */
      public boolean hasNodeId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto getNodeId() {
        if (nodeIdBuilder_ == null) {
          return nodeId_;
        } else {
          return nodeIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
       */
      public Builder setNodeId(org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto value) {
        if (nodeIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          nodeId_ = value;
          onChanged();
        } else {
          nodeIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
       */
      public Builder setNodeId(
          org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder builderForValue) {
        if (nodeIdBuilder_ == null) {
          nodeId_ = builderForValue.build();
          onChanged();
        } else {
          nodeIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
       */
      public Builder mergeNodeId(org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto value) {
        if (nodeIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              nodeId_ != org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance()) {
            nodeId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.newBuilder(nodeId_).mergeFrom(value).buildPartial();
          } else {
            nodeId_ = value;
          }
          onChanged();
        } else {
          nodeIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
       */
      public Builder clearNodeId() {
        if (nodeIdBuilder_ == null) {
          nodeId_ = org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.getDefaultInstance();
          onChanged();
        } else {
          nodeIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder getNodeIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getNodeIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder getNodeIdOrBuilder() {
        if (nodeIdBuilder_ != null) {
          return nodeIdBuilder_.getMessageOrBuilder();
        } else {
          return nodeId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.NodeIdProto nodeId = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder> 
          getNodeIdFieldBuilder() {
        if (nodeIdBuilder_ == null) {
          nodeIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdProtoOrBuilder>(
                  nodeId_,
                  getParentForChildren(),
                  isClean());
          nodeId_ = null;
        }
        return nodeIdBuilder_;
      }

      // optional .hadoop.common.TokenProto token = 2;
      private org.apache.hadoop.security.proto.SecurityProtos.TokenProto token_ = org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.security.proto.SecurityProtos.TokenProto, org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder, org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder> tokenBuilder_;
      /**
       * <code>optional .hadoop.common.TokenProto token = 2;</code>
       */
      public boolean hasToken() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional .hadoop.common.TokenProto token = 2;</code>
       */
      public org.apache.hadoop.security.proto.SecurityProtos.TokenProto getToken() {
        if (tokenBuilder_ == null) {
          return token_;
        } else {
          return tokenBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.common.TokenProto token = 2;</code>
       */
      public Builder setToken(org.apache.hadoop.security.proto.SecurityProtos.TokenProto value) {
        if (tokenBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          token_ = value;
          onChanged();
        } else {
          tokenBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.common.TokenProto token = 2;</code>
       */
      public Builder setToken(
          org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder builderForValue) {
        if (tokenBuilder_ == null) {
          token_ = builderForValue.build();
          onChanged();
        } else {
          tokenBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.common.TokenProto token = 2;</code>
       */
      public Builder mergeToken(org.apache.hadoop.security.proto.SecurityProtos.TokenProto value) {
        if (tokenBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002) &&
              token_ != org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance()) {
            token_ =
              org.apache.hadoop.security.proto.SecurityProtos.TokenProto.newBuilder(token_).mergeFrom(value).buildPartial();
          } else {
            token_ = value;
          }
          onChanged();
        } else {
          tokenBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.common.TokenProto token = 2;</code>
       */
      public Builder clearToken() {
        if (tokenBuilder_ == null) {
          token_ = org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance();
          onChanged();
        } else {
          tokenBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>optional .hadoop.common.TokenProto token = 2;</code>
       */
      public org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder getTokenBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getTokenFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.common.TokenProto token = 2;</code>
       */
      public org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder getTokenOrBuilder() {
        if (tokenBuilder_ != null) {
          return tokenBuilder_.getMessageOrBuilder();
        } else {
          return token_;
        }
      }
      /**
       * <code>optional .hadoop.common.TokenProto token = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.security.proto.SecurityProtos.TokenProto, org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder, org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder> 
          getTokenFieldBuilder() {
        if (tokenBuilder_ == null) {
          tokenBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.security.proto.SecurityProtos.TokenProto, org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder, org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder>(
                  token_,
                  getParentForChildren(),
                  isClean());
          token_ = null;
        }
        return tokenBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.NMTokenProto)
    }

    static {
      defaultInstance = new NMTokenProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.NMTokenProto)
  }

  public interface UpdatedContainerProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required .hadoop.yarn.ContainerUpdateTypeProto update_type = 1;
    /**
     * <code>required .hadoop.yarn.ContainerUpdateTypeProto update_type = 1;</code>
     */
    boolean hasUpdateType();
    /**
     * <code>required .hadoop.yarn.ContainerUpdateTypeProto update_type = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerUpdateTypeProto getUpdateType();

    // required .hadoop.yarn.ContainerProto container = 2;
    /**
     * <code>required .hadoop.yarn.ContainerProto container = 2;</code>
     */
    boolean hasContainer();
    /**
     * <code>required .hadoop.yarn.ContainerProto container = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto getContainer();
    /**
     * <code>required .hadoop.yarn.ContainerProto container = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder getContainerOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.UpdatedContainerProto}
   */
  public static final class UpdatedContainerProto extends
      com.google.protobuf.GeneratedMessage
      implements UpdatedContainerProtoOrBuilder {
    // Use UpdatedContainerProto.newBuilder() to construct.
    private UpdatedContainerProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private UpdatedContainerProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final UpdatedContainerProto defaultInstance;
    public static UpdatedContainerProto getDefaultInstance() {
      return defaultInstance;
    }

    public UpdatedContainerProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private UpdatedContainerProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              int rawValue = input.readEnum();
              org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerUpdateTypeProto value = org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerUpdateTypeProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(1, rawValue);
              } else {
                bitField0_ |= 0x00000001;
                updateType_ = value;
              }
              break;
            }
            case 18: {
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) == 0x00000002)) {
                subBuilder = container_.toBuilder();
              }
              container_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(container_);
                container_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdatedContainerProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdatedContainerProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto.Builder.class);
    }

    public static com.google.protobuf.Parser<UpdatedContainerProto> PARSER =
        new com.google.protobuf.AbstractParser<UpdatedContainerProto>() {
      public UpdatedContainerProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new UpdatedContainerProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<UpdatedContainerProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required .hadoop.yarn.ContainerUpdateTypeProto update_type = 1;
    public static final int UPDATE_TYPE_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerUpdateTypeProto updateType_;
    /**
     * <code>required .hadoop.yarn.ContainerUpdateTypeProto update_type = 1;</code>
     */
    public boolean hasUpdateType() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required .hadoop.yarn.ContainerUpdateTypeProto update_type = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerUpdateTypeProto getUpdateType() {
      return updateType_;
    }

    // required .hadoop.yarn.ContainerProto container = 2;
    public static final int CONTAINER_FIELD_NUMBER = 2;
    private org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto container_;
    /**
     * <code>required .hadoop.yarn.ContainerProto container = 2;</code>
     */
    public boolean hasContainer() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required .hadoop.yarn.ContainerProto container = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto getContainer() {
      return container_;
    }
    /**
     * <code>required .hadoop.yarn.ContainerProto container = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder getContainerOrBuilder() {
      return container_;
    }

    private void initFields() {
      updateType_ = org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerUpdateTypeProto.INCREASE_RESOURCE;
      container_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasUpdateType()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasContainer()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getContainer().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeEnum(1, updateType_.getNumber());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeMessage(2, container_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(1, updateType_.getNumber());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, container_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto) obj;

      boolean result = true;
      result = result && (hasUpdateType() == other.hasUpdateType());
      if (hasUpdateType()) {
        result = result &&
            (getUpdateType() == other.getUpdateType());
      }
      result = result && (hasContainer() == other.hasContainer());
      if (hasContainer()) {
        result = result && getContainer()
            .equals(other.getContainer());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasUpdateType()) {
        hash = (37 * hash) + UPDATE_TYPE_FIELD_NUMBER;
        hash = (53 * hash) + hashEnum(getUpdateType());
      }
      if (hasContainer()) {
        hash = (37 * hash) + CONTAINER_FIELD_NUMBER;
        hash = (53 * hash) + getContainer().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.UpdatedContainerProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdatedContainerProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdatedContainerProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getContainerFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        updateType_ = org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerUpdateTypeProto.INCREASE_RESOURCE;
        bitField0_ = (bitField0_ & ~0x00000001);
        if (containerBuilder_ == null) {
          container_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.getDefaultInstance();
        } else {
          containerBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdatedContainerProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.updateType_ = updateType_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        if (containerBuilder_ == null) {
          result.container_ = container_;
        } else {
          result.container_ = containerBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto.getDefaultInstance()) return this;
        if (other.hasUpdateType()) {
          setUpdateType(other.getUpdateType());
        }
        if (other.hasContainer()) {
          mergeContainer(other.getContainer());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasUpdateType()) {
          
          return false;
        }
        if (!hasContainer()) {
          
          return false;
        }
        if (!getContainer().isInitialized()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required .hadoop.yarn.ContainerUpdateTypeProto update_type = 1;
      private org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerUpdateTypeProto updateType_ = org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerUpdateTypeProto.INCREASE_RESOURCE;
      /**
       * <code>required .hadoop.yarn.ContainerUpdateTypeProto update_type = 1;</code>
       */
      public boolean hasUpdateType() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required .hadoop.yarn.ContainerUpdateTypeProto update_type = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerUpdateTypeProto getUpdateType() {
        return updateType_;
      }
      /**
       * <code>required .hadoop.yarn.ContainerUpdateTypeProto update_type = 1;</code>
       */
      public Builder setUpdateType(org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerUpdateTypeProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000001;
        updateType_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required .hadoop.yarn.ContainerUpdateTypeProto update_type = 1;</code>
       */
      public Builder clearUpdateType() {
        bitField0_ = (bitField0_ & ~0x00000001);
        updateType_ = org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerUpdateTypeProto.INCREASE_RESOURCE;
        onChanged();
        return this;
      }

      // required .hadoop.yarn.ContainerProto container = 2;
      private org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto container_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> containerBuilder_;
      /**
       * <code>required .hadoop.yarn.ContainerProto container = 2;</code>
       */
      public boolean hasContainer() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required .hadoop.yarn.ContainerProto container = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto getContainer() {
        if (containerBuilder_ == null) {
          return container_;
        } else {
          return containerBuilder_.getMessage();
        }
      }
      /**
       * <code>required .hadoop.yarn.ContainerProto container = 2;</code>
       */
      public Builder setContainer(org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto value) {
        if (containerBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          container_ = value;
          onChanged();
        } else {
          containerBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .hadoop.yarn.ContainerProto container = 2;</code>
       */
      public Builder setContainer(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder builderForValue) {
        if (containerBuilder_ == null) {
          container_ = builderForValue.build();
          onChanged();
        } else {
          containerBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .hadoop.yarn.ContainerProto container = 2;</code>
       */
      public Builder mergeContainer(org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto value) {
        if (containerBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002) &&
              container_ != org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.getDefaultInstance()) {
            container_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.newBuilder(container_).mergeFrom(value).buildPartial();
          } else {
            container_ = value;
          }
          onChanged();
        } else {
          containerBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .hadoop.yarn.ContainerProto container = 2;</code>
       */
      public Builder clearContainer() {
        if (containerBuilder_ == null) {
          container_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.getDefaultInstance();
          onChanged();
        } else {
          containerBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>required .hadoop.yarn.ContainerProto container = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder getContainerBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getContainerFieldBuilder().getBuilder();
      }
      /**
       * <code>required .hadoop.yarn.ContainerProto container = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder getContainerOrBuilder() {
        if (containerBuilder_ != null) {
          return containerBuilder_.getMessageOrBuilder();
        } else {
          return container_;
        }
      }
      /**
       * <code>required .hadoop.yarn.ContainerProto container = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
          getContainerFieldBuilder() {
        if (containerBuilder_ == null) {
          containerBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder>(
                  container_,
                  getParentForChildren(),
                  isClean());
          container_ = null;
        }
        return containerBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.UpdatedContainerProto)
    }

    static {
      defaultInstance = new UpdatedContainerProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.UpdatedContainerProto)
  }

  public interface AllocateResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.AMCommandProto a_m_command = 1;
    /**
     * <code>optional .hadoop.yarn.AMCommandProto a_m_command = 1;</code>
     */
    boolean hasAMCommand();
    /**
     * <code>optional .hadoop.yarn.AMCommandProto a_m_command = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.AMCommandProto getAMCommand();

    // optional int32 response_id = 2;
    /**
     * <code>optional int32 response_id = 2;</code>
     */
    boolean hasResponseId();
    /**
     * <code>optional int32 response_id = 2;</code>
     */
    int getResponseId();

    // repeated .hadoop.yarn.ContainerProto allocated_containers = 3;
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> 
        getAllocatedContainersList();
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto getAllocatedContainers(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
     */
    int getAllocatedContainersCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
        getAllocatedContainersOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder getAllocatedContainersOrBuilder(
        int index);

    // repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto> 
        getCompletedContainerStatusesList();
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto getCompletedContainerStatuses(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
     */
    int getCompletedContainerStatusesCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder> 
        getCompletedContainerStatusesOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder getCompletedContainerStatusesOrBuilder(
        int index);

    // optional .hadoop.yarn.ResourceProto limit = 5;
    /**
     * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
     */
    boolean hasLimit();
    /**
     * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getLimit();
    /**
     * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getLimitOrBuilder();

    // repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto> 
        getUpdatedNodesList();
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto getUpdatedNodes(int index);
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
     */
    int getUpdatedNodesCount();
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder> 
        getUpdatedNodesOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder getUpdatedNodesOrBuilder(
        int index);

    // optional int32 num_cluster_nodes = 7;
    /**
     * <code>optional int32 num_cluster_nodes = 7;</code>
     */
    boolean hasNumClusterNodes();
    /**
     * <code>optional int32 num_cluster_nodes = 7;</code>
     */
    int getNumClusterNodes();

    // optional .hadoop.yarn.PreemptionMessageProto preempt = 8;
    /**
     * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
     */
    boolean hasPreempt();
    /**
     * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto getPreempt();
    /**
     * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProtoOrBuilder getPreemptOrBuilder();

    // repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto> 
        getNmTokensList();
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto getNmTokens(int index);
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
     */
    int getNmTokensCount();
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder> 
        getNmTokensOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder getNmTokensOrBuilder(
        int index);

    // repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto> 
        getIncreasedContainersList();
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto getIncreasedContainers(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
     */
    int getIncreasedContainersCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProtoOrBuilder> 
        getIncreasedContainersOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProtoOrBuilder getIncreasedContainersOrBuilder(
        int index);

    // repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto> 
        getDecreasedContainersList();
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto getDecreasedContainers(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
     */
    int getDecreasedContainersCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProtoOrBuilder> 
        getDecreasedContainersOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProtoOrBuilder getDecreasedContainersOrBuilder(
        int index);

    // optional .hadoop.common.TokenProto am_rm_token = 12;
    /**
     * <code>optional .hadoop.common.TokenProto am_rm_token = 12;</code>
     */
    boolean hasAmRmToken();
    /**
     * <code>optional .hadoop.common.TokenProto am_rm_token = 12;</code>
     */
    org.apache.hadoop.security.proto.SecurityProtos.TokenProto getAmRmToken();
    /**
     * <code>optional .hadoop.common.TokenProto am_rm_token = 12;</code>
     */
    org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder getAmRmTokenOrBuilder();

    // optional .hadoop.yarn.PriorityProto application_priority = 13;
    /**
     * <code>optional .hadoop.yarn.PriorityProto application_priority = 13;</code>
     */
    boolean hasApplicationPriority();
    /**
     * <code>optional .hadoop.yarn.PriorityProto application_priority = 13;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto getApplicationPriority();
    /**
     * <code>optional .hadoop.yarn.PriorityProto application_priority = 13;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder getApplicationPriorityOrBuilder();

    // repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;
    /**
     * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto> 
        getUpdateErrorsList();
    /**
     * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto getUpdateErrors(int index);
    /**
     * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
     */
    int getUpdateErrorsCount();
    /**
     * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProtoOrBuilder> 
        getUpdateErrorsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProtoOrBuilder getUpdateErrorsOrBuilder(
        int index);

    // repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;
    /**
     * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto> 
        getUpdatedContainersList();
    /**
     * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto getUpdatedContainers(int index);
    /**
     * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
     */
    int getUpdatedContainersCount();
    /**
     * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProtoOrBuilder> 
        getUpdatedContainersOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProtoOrBuilder getUpdatedContainersOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.AllocateResponseProto}
   */
  public static final class AllocateResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements AllocateResponseProtoOrBuilder {
    // Use AllocateResponseProto.newBuilder() to construct.
    private AllocateResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private AllocateResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final AllocateResponseProto defaultInstance;
    public static AllocateResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public AllocateResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private AllocateResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              int rawValue = input.readEnum();
              org.apache.hadoop.yarn.proto.YarnProtos.AMCommandProto value = org.apache.hadoop.yarn.proto.YarnProtos.AMCommandProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(1, rawValue);
              } else {
                bitField0_ |= 0x00000001;
                aMCommand_ = value;
              }
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              responseId_ = input.readInt32();
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
                allocatedContainers_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto>();
                mutable_bitField0_ |= 0x00000004;
              }
              allocatedContainers_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.PARSER, extensionRegistry));
              break;
            }
            case 34: {
              if (!((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
                completedContainerStatuses_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto>();
                mutable_bitField0_ |= 0x00000008;
              }
              completedContainerStatuses_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.PARSER, extensionRegistry));
              break;
            }
            case 42: {
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000004) == 0x00000004)) {
                subBuilder = limit_.toBuilder();
              }
              limit_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(limit_);
                limit_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000004;
              break;
            }
            case 50: {
              if (!((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
                updatedNodes_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto>();
                mutable_bitField0_ |= 0x00000020;
              }
              updatedNodes_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.PARSER, extensionRegistry));
              break;
            }
            case 56: {
              bitField0_ |= 0x00000008;
              numClusterNodes_ = input.readInt32();
              break;
            }
            case 66: {
              org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000010) == 0x00000010)) {
                subBuilder = preempt_.toBuilder();
              }
              preempt_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(preempt_);
                preempt_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000010;
              break;
            }
            case 74: {
              if (!((mutable_bitField0_ & 0x00000100) == 0x00000100)) {
                nmTokens_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto>();
                mutable_bitField0_ |= 0x00000100;
              }
              nmTokens_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.PARSER, extensionRegistry));
              break;
            }
            case 82: {
              if (!((mutable_bitField0_ & 0x00000200) == 0x00000200)) {
                increasedContainers_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto>();
                mutable_bitField0_ |= 0x00000200;
              }
              increasedContainers_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.PARSER, extensionRegistry));
              break;
            }
            case 90: {
              if (!((mutable_bitField0_ & 0x00000400) == 0x00000400)) {
                decreasedContainers_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto>();
                mutable_bitField0_ |= 0x00000400;
              }
              decreasedContainers_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.PARSER, extensionRegistry));
              break;
            }
            case 98: {
              org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000020) == 0x00000020)) {
                subBuilder = amRmToken_.toBuilder();
              }
              amRmToken_ = input.readMessage(org.apache.hadoop.security.proto.SecurityProtos.TokenProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(amRmToken_);
                amRmToken_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000020;
              break;
            }
            case 106: {
              org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000040) == 0x00000040)) {
                subBuilder = applicationPriority_.toBuilder();
              }
              applicationPriority_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationPriority_);
                applicationPriority_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000040;
              break;
            }
            case 122: {
              if (!((mutable_bitField0_ & 0x00002000) == 0x00002000)) {
                updateErrors_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto>();
                mutable_bitField0_ |= 0x00002000;
              }
              updateErrors_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto.PARSER, extensionRegistry));
              break;
            }
            case 130: {
              if (!((mutable_bitField0_ & 0x00004000) == 0x00004000)) {
                updatedContainers_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto>();
                mutable_bitField0_ |= 0x00004000;
              }
              updatedContainers_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
          allocatedContainers_ = java.util.Collections.unmodifiableList(allocatedContainers_);
        }
        if (((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
          completedContainerStatuses_ = java.util.Collections.unmodifiableList(completedContainerStatuses_);
        }
        if (((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
          updatedNodes_ = java.util.Collections.unmodifiableList(updatedNodes_);
        }
        if (((mutable_bitField0_ & 0x00000100) == 0x00000100)) {
          nmTokens_ = java.util.Collections.unmodifiableList(nmTokens_);
        }
        if (((mutable_bitField0_ & 0x00000200) == 0x00000200)) {
          increasedContainers_ = java.util.Collections.unmodifiableList(increasedContainers_);
        }
        if (((mutable_bitField0_ & 0x00000400) == 0x00000400)) {
          decreasedContainers_ = java.util.Collections.unmodifiableList(decreasedContainers_);
        }
        if (((mutable_bitField0_ & 0x00002000) == 0x00002000)) {
          updateErrors_ = java.util.Collections.unmodifiableList(updateErrors_);
        }
        if (((mutable_bitField0_ & 0x00004000) == 0x00004000)) {
          updatedContainers_ = java.util.Collections.unmodifiableList(updatedContainers_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_AllocateResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_AllocateResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<AllocateResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<AllocateResponseProto>() {
      public AllocateResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new AllocateResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<AllocateResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.AMCommandProto a_m_command = 1;
    public static final int A_M_COMMAND_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.AMCommandProto aMCommand_;
    /**
     * <code>optional .hadoop.yarn.AMCommandProto a_m_command = 1;</code>
     */
    public boolean hasAMCommand() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.AMCommandProto a_m_command = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.AMCommandProto getAMCommand() {
      return aMCommand_;
    }

    // optional int32 response_id = 2;
    public static final int RESPONSE_ID_FIELD_NUMBER = 2;
    private int responseId_;
    /**
     * <code>optional int32 response_id = 2;</code>
     */
    public boolean hasResponseId() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional int32 response_id = 2;</code>
     */
    public int getResponseId() {
      return responseId_;
    }

    // repeated .hadoop.yarn.ContainerProto allocated_containers = 3;
    public static final int ALLOCATED_CONTAINERS_FIELD_NUMBER = 3;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> allocatedContainers_;
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> getAllocatedContainersList() {
      return allocatedContainers_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
        getAllocatedContainersOrBuilderList() {
      return allocatedContainers_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
     */
    public int getAllocatedContainersCount() {
      return allocatedContainers_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto getAllocatedContainers(int index) {
      return allocatedContainers_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder getAllocatedContainersOrBuilder(
        int index) {
      return allocatedContainers_.get(index);
    }

    // repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;
    public static final int COMPLETED_CONTAINER_STATUSES_FIELD_NUMBER = 4;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto> completedContainerStatuses_;
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto> getCompletedContainerStatusesList() {
      return completedContainerStatuses_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder> 
        getCompletedContainerStatusesOrBuilderList() {
      return completedContainerStatuses_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
     */
    public int getCompletedContainerStatusesCount() {
      return completedContainerStatuses_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto getCompletedContainerStatuses(int index) {
      return completedContainerStatuses_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder getCompletedContainerStatusesOrBuilder(
        int index) {
      return completedContainerStatuses_.get(index);
    }

    // optional .hadoop.yarn.ResourceProto limit = 5;
    public static final int LIMIT_FIELD_NUMBER = 5;
    private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto limit_;
    /**
     * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
     */
    public boolean hasLimit() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getLimit() {
      return limit_;
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getLimitOrBuilder() {
      return limit_;
    }

    // repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;
    public static final int UPDATED_NODES_FIELD_NUMBER = 6;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto> updatedNodes_;
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto> getUpdatedNodesList() {
      return updatedNodes_;
    }
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder> 
        getUpdatedNodesOrBuilderList() {
      return updatedNodes_;
    }
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
     */
    public int getUpdatedNodesCount() {
      return updatedNodes_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto getUpdatedNodes(int index) {
      return updatedNodes_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder getUpdatedNodesOrBuilder(
        int index) {
      return updatedNodes_.get(index);
    }

    // optional int32 num_cluster_nodes = 7;
    public static final int NUM_CLUSTER_NODES_FIELD_NUMBER = 7;
    private int numClusterNodes_;
    /**
     * <code>optional int32 num_cluster_nodes = 7;</code>
     */
    public boolean hasNumClusterNodes() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>optional int32 num_cluster_nodes = 7;</code>
     */
    public int getNumClusterNodes() {
      return numClusterNodes_;
    }

    // optional .hadoop.yarn.PreemptionMessageProto preempt = 8;
    public static final int PREEMPT_FIELD_NUMBER = 8;
    private org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto preempt_;
    /**
     * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
     */
    public boolean hasPreempt() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto getPreempt() {
      return preempt_;
    }
    /**
     * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProtoOrBuilder getPreemptOrBuilder() {
      return preempt_;
    }

    // repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;
    public static final int NM_TOKENS_FIELD_NUMBER = 9;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto> nmTokens_;
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto> getNmTokensList() {
      return nmTokens_;
    }
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder> 
        getNmTokensOrBuilderList() {
      return nmTokens_;
    }
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
     */
    public int getNmTokensCount() {
      return nmTokens_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto getNmTokens(int index) {
      return nmTokens_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder getNmTokensOrBuilder(
        int index) {
      return nmTokens_.get(index);
    }

    // repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;
    public static final int INCREASED_CONTAINERS_FIELD_NUMBER = 10;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto> increasedContainers_;
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto> getIncreasedContainersList() {
      return increasedContainers_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProtoOrBuilder> 
        getIncreasedContainersOrBuilderList() {
      return increasedContainers_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
     */
    public int getIncreasedContainersCount() {
      return increasedContainers_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto getIncreasedContainers(int index) {
      return increasedContainers_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProtoOrBuilder getIncreasedContainersOrBuilder(
        int index) {
      return increasedContainers_.get(index);
    }

    // repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;
    public static final int DECREASED_CONTAINERS_FIELD_NUMBER = 11;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto> decreasedContainers_;
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto> getDecreasedContainersList() {
      return decreasedContainers_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProtoOrBuilder> 
        getDecreasedContainersOrBuilderList() {
      return decreasedContainers_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
     */
    public int getDecreasedContainersCount() {
      return decreasedContainers_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto getDecreasedContainers(int index) {
      return decreasedContainers_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProtoOrBuilder getDecreasedContainersOrBuilder(
        int index) {
      return decreasedContainers_.get(index);
    }

    // optional .hadoop.common.TokenProto am_rm_token = 12;
    public static final int AM_RM_TOKEN_FIELD_NUMBER = 12;
    private org.apache.hadoop.security.proto.SecurityProtos.TokenProto amRmToken_;
    /**
     * <code>optional .hadoop.common.TokenProto am_rm_token = 12;</code>
     */
    public boolean hasAmRmToken() {
      return ((bitField0_ & 0x00000020) == 0x00000020);
    }
    /**
     * <code>optional .hadoop.common.TokenProto am_rm_token = 12;</code>
     */
    public org.apache.hadoop.security.proto.SecurityProtos.TokenProto getAmRmToken() {
      return amRmToken_;
    }
    /**
     * <code>optional .hadoop.common.TokenProto am_rm_token = 12;</code>
     */
    public org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder getAmRmTokenOrBuilder() {
      return amRmToken_;
    }

    // optional .hadoop.yarn.PriorityProto application_priority = 13;
    public static final int APPLICATION_PRIORITY_FIELD_NUMBER = 13;
    private org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto applicationPriority_;
    /**
     * <code>optional .hadoop.yarn.PriorityProto application_priority = 13;</code>
     */
    public boolean hasApplicationPriority() {
      return ((bitField0_ & 0x00000040) == 0x00000040);
    }
    /**
     * <code>optional .hadoop.yarn.PriorityProto application_priority = 13;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto getApplicationPriority() {
      return applicationPriority_;
    }
    /**
     * <code>optional .hadoop.yarn.PriorityProto application_priority = 13;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder getApplicationPriorityOrBuilder() {
      return applicationPriority_;
    }

    // repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;
    public static final int UPDATE_ERRORS_FIELD_NUMBER = 15;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto> updateErrors_;
    /**
     * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto> getUpdateErrorsList() {
      return updateErrors_;
    }
    /**
     * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProtoOrBuilder> 
        getUpdateErrorsOrBuilderList() {
      return updateErrors_;
    }
    /**
     * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
     */
    public int getUpdateErrorsCount() {
      return updateErrors_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto getUpdateErrors(int index) {
      return updateErrors_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProtoOrBuilder getUpdateErrorsOrBuilder(
        int index) {
      return updateErrors_.get(index);
    }

    // repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;
    public static final int UPDATED_CONTAINERS_FIELD_NUMBER = 16;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto> updatedContainers_;
    /**
     * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto> getUpdatedContainersList() {
      return updatedContainers_;
    }
    /**
     * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProtoOrBuilder> 
        getUpdatedContainersOrBuilderList() {
      return updatedContainers_;
    }
    /**
     * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
     */
    public int getUpdatedContainersCount() {
      return updatedContainers_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto getUpdatedContainers(int index) {
      return updatedContainers_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProtoOrBuilder getUpdatedContainersOrBuilder(
        int index) {
      return updatedContainers_.get(index);
    }

    private void initFields() {
      aMCommand_ = org.apache.hadoop.yarn.proto.YarnProtos.AMCommandProto.AM_RESYNC;
      responseId_ = 0;
      allocatedContainers_ = java.util.Collections.emptyList();
      completedContainerStatuses_ = java.util.Collections.emptyList();
      limit_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
      updatedNodes_ = java.util.Collections.emptyList();
      numClusterNodes_ = 0;
      preempt_ = org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.getDefaultInstance();
      nmTokens_ = java.util.Collections.emptyList();
      increasedContainers_ = java.util.Collections.emptyList();
      decreasedContainers_ = java.util.Collections.emptyList();
      amRmToken_ = org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance();
      applicationPriority_ = org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance();
      updateErrors_ = java.util.Collections.emptyList();
      updatedContainers_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      for (int i = 0; i < getAllocatedContainersCount(); i++) {
        if (!getAllocatedContainers(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getNmTokensCount(); i++) {
        if (!getNmTokens(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getIncreasedContainersCount(); i++) {
        if (!getIncreasedContainers(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasAmRmToken()) {
        if (!getAmRmToken().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getUpdateErrorsCount(); i++) {
        if (!getUpdateErrors(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      for (int i = 0; i < getUpdatedContainersCount(); i++) {
        if (!getUpdatedContainers(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeEnum(1, aMCommand_.getNumber());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeInt32(2, responseId_);
      }
      for (int i = 0; i < allocatedContainers_.size(); i++) {
        output.writeMessage(3, allocatedContainers_.get(i));
      }
      for (int i = 0; i < completedContainerStatuses_.size(); i++) {
        output.writeMessage(4, completedContainerStatuses_.get(i));
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeMessage(5, limit_);
      }
      for (int i = 0; i < updatedNodes_.size(); i++) {
        output.writeMessage(6, updatedNodes_.get(i));
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeInt32(7, numClusterNodes_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeMessage(8, preempt_);
      }
      for (int i = 0; i < nmTokens_.size(); i++) {
        output.writeMessage(9, nmTokens_.get(i));
      }
      for (int i = 0; i < increasedContainers_.size(); i++) {
        output.writeMessage(10, increasedContainers_.get(i));
      }
      for (int i = 0; i < decreasedContainers_.size(); i++) {
        output.writeMessage(11, decreasedContainers_.get(i));
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        output.writeMessage(12, amRmToken_);
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        output.writeMessage(13, applicationPriority_);
      }
      for (int i = 0; i < updateErrors_.size(); i++) {
        output.writeMessage(15, updateErrors_.get(i));
      }
      for (int i = 0; i < updatedContainers_.size(); i++) {
        output.writeMessage(16, updatedContainers_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(1, aMCommand_.getNumber());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, responseId_);
      }
      for (int i = 0; i < allocatedContainers_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, allocatedContainers_.get(i));
      }
      for (int i = 0; i < completedContainerStatuses_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, completedContainerStatuses_.get(i));
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, limit_);
      }
      for (int i = 0; i < updatedNodes_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, updatedNodes_.get(i));
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(7, numClusterNodes_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(8, preempt_);
      }
      for (int i = 0; i < nmTokens_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(9, nmTokens_.get(i));
      }
      for (int i = 0; i < increasedContainers_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(10, increasedContainers_.get(i));
      }
      for (int i = 0; i < decreasedContainers_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(11, decreasedContainers_.get(i));
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(12, amRmToken_);
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(13, applicationPriority_);
      }
      for (int i = 0; i < updateErrors_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(15, updateErrors_.get(i));
      }
      for (int i = 0; i < updatedContainers_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(16, updatedContainers_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto) obj;

      boolean result = true;
      result = result && (hasAMCommand() == other.hasAMCommand());
      if (hasAMCommand()) {
        result = result &&
            (getAMCommand() == other.getAMCommand());
      }
      result = result && (hasResponseId() == other.hasResponseId());
      if (hasResponseId()) {
        result = result && (getResponseId()
            == other.getResponseId());
      }
      result = result && getAllocatedContainersList()
          .equals(other.getAllocatedContainersList());
      result = result && getCompletedContainerStatusesList()
          .equals(other.getCompletedContainerStatusesList());
      result = result && (hasLimit() == other.hasLimit());
      if (hasLimit()) {
        result = result && getLimit()
            .equals(other.getLimit());
      }
      result = result && getUpdatedNodesList()
          .equals(other.getUpdatedNodesList());
      result = result && (hasNumClusterNodes() == other.hasNumClusterNodes());
      if (hasNumClusterNodes()) {
        result = result && (getNumClusterNodes()
            == other.getNumClusterNodes());
      }
      result = result && (hasPreempt() == other.hasPreempt());
      if (hasPreempt()) {
        result = result && getPreempt()
            .equals(other.getPreempt());
      }
      result = result && getNmTokensList()
          .equals(other.getNmTokensList());
      result = result && getIncreasedContainersList()
          .equals(other.getIncreasedContainersList());
      result = result && getDecreasedContainersList()
          .equals(other.getDecreasedContainersList());
      result = result && (hasAmRmToken() == other.hasAmRmToken());
      if (hasAmRmToken()) {
        result = result && getAmRmToken()
            .equals(other.getAmRmToken());
      }
      result = result && (hasApplicationPriority() == other.hasApplicationPriority());
      if (hasApplicationPriority()) {
        result = result && getApplicationPriority()
            .equals(other.getApplicationPriority());
      }
      result = result && getUpdateErrorsList()
          .equals(other.getUpdateErrorsList());
      result = result && getUpdatedContainersList()
          .equals(other.getUpdatedContainersList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasAMCommand()) {
        hash = (37 * hash) + A_M_COMMAND_FIELD_NUMBER;
        hash = (53 * hash) + hashEnum(getAMCommand());
      }
      if (hasResponseId()) {
        hash = (37 * hash) + RESPONSE_ID_FIELD_NUMBER;
        hash = (53 * hash) + getResponseId();
      }
      if (getAllocatedContainersCount() > 0) {
        hash = (37 * hash) + ALLOCATED_CONTAINERS_FIELD_NUMBER;
        hash = (53 * hash) + getAllocatedContainersList().hashCode();
      }
      if (getCompletedContainerStatusesCount() > 0) {
        hash = (37 * hash) + COMPLETED_CONTAINER_STATUSES_FIELD_NUMBER;
        hash = (53 * hash) + getCompletedContainerStatusesList().hashCode();
      }
      if (hasLimit()) {
        hash = (37 * hash) + LIMIT_FIELD_NUMBER;
        hash = (53 * hash) + getLimit().hashCode();
      }
      if (getUpdatedNodesCount() > 0) {
        hash = (37 * hash) + UPDATED_NODES_FIELD_NUMBER;
        hash = (53 * hash) + getUpdatedNodesList().hashCode();
      }
      if (hasNumClusterNodes()) {
        hash = (37 * hash) + NUM_CLUSTER_NODES_FIELD_NUMBER;
        hash = (53 * hash) + getNumClusterNodes();
      }
      if (hasPreempt()) {
        hash = (37 * hash) + PREEMPT_FIELD_NUMBER;
        hash = (53 * hash) + getPreempt().hashCode();
      }
      if (getNmTokensCount() > 0) {
        hash = (37 * hash) + NM_TOKENS_FIELD_NUMBER;
        hash = (53 * hash) + getNmTokensList().hashCode();
      }
      if (getIncreasedContainersCount() > 0) {
        hash = (37 * hash) + INCREASED_CONTAINERS_FIELD_NUMBER;
        hash = (53 * hash) + getIncreasedContainersList().hashCode();
      }
      if (getDecreasedContainersCount() > 0) {
        hash = (37 * hash) + DECREASED_CONTAINERS_FIELD_NUMBER;
        hash = (53 * hash) + getDecreasedContainersList().hashCode();
      }
      if (hasAmRmToken()) {
        hash = (37 * hash) + AM_RM_TOKEN_FIELD_NUMBER;
        hash = (53 * hash) + getAmRmToken().hashCode();
      }
      if (hasApplicationPriority()) {
        hash = (37 * hash) + APPLICATION_PRIORITY_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationPriority().hashCode();
      }
      if (getUpdateErrorsCount() > 0) {
        hash = (37 * hash) + UPDATE_ERRORS_FIELD_NUMBER;
        hash = (53 * hash) + getUpdateErrorsList().hashCode();
      }
      if (getUpdatedContainersCount() > 0) {
        hash = (37 * hash) + UPDATED_CONTAINERS_FIELD_NUMBER;
        hash = (53 * hash) + getUpdatedContainersList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.AllocateResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_AllocateResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_AllocateResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getAllocatedContainersFieldBuilder();
          getCompletedContainerStatusesFieldBuilder();
          getLimitFieldBuilder();
          getUpdatedNodesFieldBuilder();
          getPreemptFieldBuilder();
          getNmTokensFieldBuilder();
          getIncreasedContainersFieldBuilder();
          getDecreasedContainersFieldBuilder();
          getAmRmTokenFieldBuilder();
          getApplicationPriorityFieldBuilder();
          getUpdateErrorsFieldBuilder();
          getUpdatedContainersFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        aMCommand_ = org.apache.hadoop.yarn.proto.YarnProtos.AMCommandProto.AM_RESYNC;
        bitField0_ = (bitField0_ & ~0x00000001);
        responseId_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        if (allocatedContainersBuilder_ == null) {
          allocatedContainers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
        } else {
          allocatedContainersBuilder_.clear();
        }
        if (completedContainerStatusesBuilder_ == null) {
          completedContainerStatuses_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
        } else {
          completedContainerStatusesBuilder_.clear();
        }
        if (limitBuilder_ == null) {
          limit_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
        } else {
          limitBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000010);
        if (updatedNodesBuilder_ == null) {
          updatedNodes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
        } else {
          updatedNodesBuilder_.clear();
        }
        numClusterNodes_ = 0;
        bitField0_ = (bitField0_ & ~0x00000040);
        if (preemptBuilder_ == null) {
          preempt_ = org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.getDefaultInstance();
        } else {
          preemptBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000080);
        if (nmTokensBuilder_ == null) {
          nmTokens_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000100);
        } else {
          nmTokensBuilder_.clear();
        }
        if (increasedContainersBuilder_ == null) {
          increasedContainers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000200);
        } else {
          increasedContainersBuilder_.clear();
        }
        if (decreasedContainersBuilder_ == null) {
          decreasedContainers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000400);
        } else {
          decreasedContainersBuilder_.clear();
        }
        if (amRmTokenBuilder_ == null) {
          amRmToken_ = org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance();
        } else {
          amRmTokenBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000800);
        if (applicationPriorityBuilder_ == null) {
          applicationPriority_ = org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance();
        } else {
          applicationPriorityBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00001000);
        if (updateErrorsBuilder_ == null) {
          updateErrors_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00002000);
        } else {
          updateErrorsBuilder_.clear();
        }
        if (updatedContainersBuilder_ == null) {
          updatedContainers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00004000);
        } else {
          updatedContainersBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_AllocateResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.aMCommand_ = aMCommand_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.responseId_ = responseId_;
        if (allocatedContainersBuilder_ == null) {
          if (((bitField0_ & 0x00000004) == 0x00000004)) {
            allocatedContainers_ = java.util.Collections.unmodifiableList(allocatedContainers_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.allocatedContainers_ = allocatedContainers_;
        } else {
          result.allocatedContainers_ = allocatedContainersBuilder_.build();
        }
        if (completedContainerStatusesBuilder_ == null) {
          if (((bitField0_ & 0x00000008) == 0x00000008)) {
            completedContainerStatuses_ = java.util.Collections.unmodifiableList(completedContainerStatuses_);
            bitField0_ = (bitField0_ & ~0x00000008);
          }
          result.completedContainerStatuses_ = completedContainerStatuses_;
        } else {
          result.completedContainerStatuses_ = completedContainerStatusesBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000004;
        }
        if (limitBuilder_ == null) {
          result.limit_ = limit_;
        } else {
          result.limit_ = limitBuilder_.build();
        }
        if (updatedNodesBuilder_ == null) {
          if (((bitField0_ & 0x00000020) == 0x00000020)) {
            updatedNodes_ = java.util.Collections.unmodifiableList(updatedNodes_);
            bitField0_ = (bitField0_ & ~0x00000020);
          }
          result.updatedNodes_ = updatedNodes_;
        } else {
          result.updatedNodes_ = updatedNodesBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000008;
        }
        result.numClusterNodes_ = numClusterNodes_;
        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
          to_bitField0_ |= 0x00000010;
        }
        if (preemptBuilder_ == null) {
          result.preempt_ = preempt_;
        } else {
          result.preempt_ = preemptBuilder_.build();
        }
        if (nmTokensBuilder_ == null) {
          if (((bitField0_ & 0x00000100) == 0x00000100)) {
            nmTokens_ = java.util.Collections.unmodifiableList(nmTokens_);
            bitField0_ = (bitField0_ & ~0x00000100);
          }
          result.nmTokens_ = nmTokens_;
        } else {
          result.nmTokens_ = nmTokensBuilder_.build();
        }
        if (increasedContainersBuilder_ == null) {
          if (((bitField0_ & 0x00000200) == 0x00000200)) {
            increasedContainers_ = java.util.Collections.unmodifiableList(increasedContainers_);
            bitField0_ = (bitField0_ & ~0x00000200);
          }
          result.increasedContainers_ = increasedContainers_;
        } else {
          result.increasedContainers_ = increasedContainersBuilder_.build();
        }
        if (decreasedContainersBuilder_ == null) {
          if (((bitField0_ & 0x00000400) == 0x00000400)) {
            decreasedContainers_ = java.util.Collections.unmodifiableList(decreasedContainers_);
            bitField0_ = (bitField0_ & ~0x00000400);
          }
          result.decreasedContainers_ = decreasedContainers_;
        } else {
          result.decreasedContainers_ = decreasedContainersBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000800) == 0x00000800)) {
          to_bitField0_ |= 0x00000020;
        }
        if (amRmTokenBuilder_ == null) {
          result.amRmToken_ = amRmToken_;
        } else {
          result.amRmToken_ = amRmTokenBuilder_.build();
        }
        if (((from_bitField0_ & 0x00001000) == 0x00001000)) {
          to_bitField0_ |= 0x00000040;
        }
        if (applicationPriorityBuilder_ == null) {
          result.applicationPriority_ = applicationPriority_;
        } else {
          result.applicationPriority_ = applicationPriorityBuilder_.build();
        }
        if (updateErrorsBuilder_ == null) {
          if (((bitField0_ & 0x00002000) == 0x00002000)) {
            updateErrors_ = java.util.Collections.unmodifiableList(updateErrors_);
            bitField0_ = (bitField0_ & ~0x00002000);
          }
          result.updateErrors_ = updateErrors_;
        } else {
          result.updateErrors_ = updateErrorsBuilder_.build();
        }
        if (updatedContainersBuilder_ == null) {
          if (((bitField0_ & 0x00004000) == 0x00004000)) {
            updatedContainers_ = java.util.Collections.unmodifiableList(updatedContainers_);
            bitField0_ = (bitField0_ & ~0x00004000);
          }
          result.updatedContainers_ = updatedContainers_;
        } else {
          result.updatedContainers_ = updatedContainersBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto.getDefaultInstance()) return this;
        if (other.hasAMCommand()) {
          setAMCommand(other.getAMCommand());
        }
        if (other.hasResponseId()) {
          setResponseId(other.getResponseId());
        }
        if (allocatedContainersBuilder_ == null) {
          if (!other.allocatedContainers_.isEmpty()) {
            if (allocatedContainers_.isEmpty()) {
              allocatedContainers_ = other.allocatedContainers_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureAllocatedContainersIsMutable();
              allocatedContainers_.addAll(other.allocatedContainers_);
            }
            onChanged();
          }
        } else {
          if (!other.allocatedContainers_.isEmpty()) {
            if (allocatedContainersBuilder_.isEmpty()) {
              allocatedContainersBuilder_.dispose();
              allocatedContainersBuilder_ = null;
              allocatedContainers_ = other.allocatedContainers_;
              bitField0_ = (bitField0_ & ~0x00000004);
              allocatedContainersBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getAllocatedContainersFieldBuilder() : null;
            } else {
              allocatedContainersBuilder_.addAllMessages(other.allocatedContainers_);
            }
          }
        }
        if (completedContainerStatusesBuilder_ == null) {
          if (!other.completedContainerStatuses_.isEmpty()) {
            if (completedContainerStatuses_.isEmpty()) {
              completedContainerStatuses_ = other.completedContainerStatuses_;
              bitField0_ = (bitField0_ & ~0x00000008);
            } else {
              ensureCompletedContainerStatusesIsMutable();
              completedContainerStatuses_.addAll(other.completedContainerStatuses_);
            }
            onChanged();
          }
        } else {
          if (!other.completedContainerStatuses_.isEmpty()) {
            if (completedContainerStatusesBuilder_.isEmpty()) {
              completedContainerStatusesBuilder_.dispose();
              completedContainerStatusesBuilder_ = null;
              completedContainerStatuses_ = other.completedContainerStatuses_;
              bitField0_ = (bitField0_ & ~0x00000008);
              completedContainerStatusesBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getCompletedContainerStatusesFieldBuilder() : null;
            } else {
              completedContainerStatusesBuilder_.addAllMessages(other.completedContainerStatuses_);
            }
          }
        }
        if (other.hasLimit()) {
          mergeLimit(other.getLimit());
        }
        if (updatedNodesBuilder_ == null) {
          if (!other.updatedNodes_.isEmpty()) {
            if (updatedNodes_.isEmpty()) {
              updatedNodes_ = other.updatedNodes_;
              bitField0_ = (bitField0_ & ~0x00000020);
            } else {
              ensureUpdatedNodesIsMutable();
              updatedNodes_.addAll(other.updatedNodes_);
            }
            onChanged();
          }
        } else {
          if (!other.updatedNodes_.isEmpty()) {
            if (updatedNodesBuilder_.isEmpty()) {
              updatedNodesBuilder_.dispose();
              updatedNodesBuilder_ = null;
              updatedNodes_ = other.updatedNodes_;
              bitField0_ = (bitField0_ & ~0x00000020);
              updatedNodesBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getUpdatedNodesFieldBuilder() : null;
            } else {
              updatedNodesBuilder_.addAllMessages(other.updatedNodes_);
            }
          }
        }
        if (other.hasNumClusterNodes()) {
          setNumClusterNodes(other.getNumClusterNodes());
        }
        if (other.hasPreempt()) {
          mergePreempt(other.getPreempt());
        }
        if (nmTokensBuilder_ == null) {
          if (!other.nmTokens_.isEmpty()) {
            if (nmTokens_.isEmpty()) {
              nmTokens_ = other.nmTokens_;
              bitField0_ = (bitField0_ & ~0x00000100);
            } else {
              ensureNmTokensIsMutable();
              nmTokens_.addAll(other.nmTokens_);
            }
            onChanged();
          }
        } else {
          if (!other.nmTokens_.isEmpty()) {
            if (nmTokensBuilder_.isEmpty()) {
              nmTokensBuilder_.dispose();
              nmTokensBuilder_ = null;
              nmTokens_ = other.nmTokens_;
              bitField0_ = (bitField0_ & ~0x00000100);
              nmTokensBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getNmTokensFieldBuilder() : null;
            } else {
              nmTokensBuilder_.addAllMessages(other.nmTokens_);
            }
          }
        }
        if (increasedContainersBuilder_ == null) {
          if (!other.increasedContainers_.isEmpty()) {
            if (increasedContainers_.isEmpty()) {
              increasedContainers_ = other.increasedContainers_;
              bitField0_ = (bitField0_ & ~0x00000200);
            } else {
              ensureIncreasedContainersIsMutable();
              increasedContainers_.addAll(other.increasedContainers_);
            }
            onChanged();
          }
        } else {
          if (!other.increasedContainers_.isEmpty()) {
            if (increasedContainersBuilder_.isEmpty()) {
              increasedContainersBuilder_.dispose();
              increasedContainersBuilder_ = null;
              increasedContainers_ = other.increasedContainers_;
              bitField0_ = (bitField0_ & ~0x00000200);
              increasedContainersBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getIncreasedContainersFieldBuilder() : null;
            } else {
              increasedContainersBuilder_.addAllMessages(other.increasedContainers_);
            }
          }
        }
        if (decreasedContainersBuilder_ == null) {
          if (!other.decreasedContainers_.isEmpty()) {
            if (decreasedContainers_.isEmpty()) {
              decreasedContainers_ = other.decreasedContainers_;
              bitField0_ = (bitField0_ & ~0x00000400);
            } else {
              ensureDecreasedContainersIsMutable();
              decreasedContainers_.addAll(other.decreasedContainers_);
            }
            onChanged();
          }
        } else {
          if (!other.decreasedContainers_.isEmpty()) {
            if (decreasedContainersBuilder_.isEmpty()) {
              decreasedContainersBuilder_.dispose();
              decreasedContainersBuilder_ = null;
              decreasedContainers_ = other.decreasedContainers_;
              bitField0_ = (bitField0_ & ~0x00000400);
              decreasedContainersBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getDecreasedContainersFieldBuilder() : null;
            } else {
              decreasedContainersBuilder_.addAllMessages(other.decreasedContainers_);
            }
          }
        }
        if (other.hasAmRmToken()) {
          mergeAmRmToken(other.getAmRmToken());
        }
        if (other.hasApplicationPriority()) {
          mergeApplicationPriority(other.getApplicationPriority());
        }
        if (updateErrorsBuilder_ == null) {
          if (!other.updateErrors_.isEmpty()) {
            if (updateErrors_.isEmpty()) {
              updateErrors_ = other.updateErrors_;
              bitField0_ = (bitField0_ & ~0x00002000);
            } else {
              ensureUpdateErrorsIsMutable();
              updateErrors_.addAll(other.updateErrors_);
            }
            onChanged();
          }
        } else {
          if (!other.updateErrors_.isEmpty()) {
            if (updateErrorsBuilder_.isEmpty()) {
              updateErrorsBuilder_.dispose();
              updateErrorsBuilder_ = null;
              updateErrors_ = other.updateErrors_;
              bitField0_ = (bitField0_ & ~0x00002000);
              updateErrorsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getUpdateErrorsFieldBuilder() : null;
            } else {
              updateErrorsBuilder_.addAllMessages(other.updateErrors_);
            }
          }
        }
        if (updatedContainersBuilder_ == null) {
          if (!other.updatedContainers_.isEmpty()) {
            if (updatedContainers_.isEmpty()) {
              updatedContainers_ = other.updatedContainers_;
              bitField0_ = (bitField0_ & ~0x00004000);
            } else {
              ensureUpdatedContainersIsMutable();
              updatedContainers_.addAll(other.updatedContainers_);
            }
            onChanged();
          }
        } else {
          if (!other.updatedContainers_.isEmpty()) {
            if (updatedContainersBuilder_.isEmpty()) {
              updatedContainersBuilder_.dispose();
              updatedContainersBuilder_ = null;
              updatedContainers_ = other.updatedContainers_;
              bitField0_ = (bitField0_ & ~0x00004000);
              updatedContainersBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getUpdatedContainersFieldBuilder() : null;
            } else {
              updatedContainersBuilder_.addAllMessages(other.updatedContainers_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        for (int i = 0; i < getAllocatedContainersCount(); i++) {
          if (!getAllocatedContainers(i).isInitialized()) {
            
            return false;
          }
        }
        for (int i = 0; i < getNmTokensCount(); i++) {
          if (!getNmTokens(i).isInitialized()) {
            
            return false;
          }
        }
        for (int i = 0; i < getIncreasedContainersCount(); i++) {
          if (!getIncreasedContainers(i).isInitialized()) {
            
            return false;
          }
        }
        if (hasAmRmToken()) {
          if (!getAmRmToken().isInitialized()) {
            
            return false;
          }
        }
        for (int i = 0; i < getUpdateErrorsCount(); i++) {
          if (!getUpdateErrors(i).isInitialized()) {
            
            return false;
          }
        }
        for (int i = 0; i < getUpdatedContainersCount(); i++) {
          if (!getUpdatedContainers(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.AllocateResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.AMCommandProto a_m_command = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.AMCommandProto aMCommand_ = org.apache.hadoop.yarn.proto.YarnProtos.AMCommandProto.AM_RESYNC;
      /**
       * <code>optional .hadoop.yarn.AMCommandProto a_m_command = 1;</code>
       */
      public boolean hasAMCommand() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.AMCommandProto a_m_command = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.AMCommandProto getAMCommand() {
        return aMCommand_;
      }
      /**
       * <code>optional .hadoop.yarn.AMCommandProto a_m_command = 1;</code>
       */
      public Builder setAMCommand(org.apache.hadoop.yarn.proto.YarnProtos.AMCommandProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000001;
        aMCommand_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.AMCommandProto a_m_command = 1;</code>
       */
      public Builder clearAMCommand() {
        bitField0_ = (bitField0_ & ~0x00000001);
        aMCommand_ = org.apache.hadoop.yarn.proto.YarnProtos.AMCommandProto.AM_RESYNC;
        onChanged();
        return this;
      }

      // optional int32 response_id = 2;
      private int responseId_ ;
      /**
       * <code>optional int32 response_id = 2;</code>
       */
      public boolean hasResponseId() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional int32 response_id = 2;</code>
       */
      public int getResponseId() {
        return responseId_;
      }
      /**
       * <code>optional int32 response_id = 2;</code>
       */
      public Builder setResponseId(int value) {
        bitField0_ |= 0x00000002;
        responseId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 response_id = 2;</code>
       */
      public Builder clearResponseId() {
        bitField0_ = (bitField0_ & ~0x00000002);
        responseId_ = 0;
        onChanged();
        return this;
      }

      // repeated .hadoop.yarn.ContainerProto allocated_containers = 3;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> allocatedContainers_ =
        java.util.Collections.emptyList();
      private void ensureAllocatedContainersIsMutable() {
        if (!((bitField0_ & 0x00000004) == 0x00000004)) {
          allocatedContainers_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto>(allocatedContainers_);
          bitField0_ |= 0x00000004;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> allocatedContainersBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> getAllocatedContainersList() {
        if (allocatedContainersBuilder_ == null) {
          return java.util.Collections.unmodifiableList(allocatedContainers_);
        } else {
          return allocatedContainersBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public int getAllocatedContainersCount() {
        if (allocatedContainersBuilder_ == null) {
          return allocatedContainers_.size();
        } else {
          return allocatedContainersBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto getAllocatedContainers(int index) {
        if (allocatedContainersBuilder_ == null) {
          return allocatedContainers_.get(index);
        } else {
          return allocatedContainersBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public Builder setAllocatedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto value) {
        if (allocatedContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAllocatedContainersIsMutable();
          allocatedContainers_.set(index, value);
          onChanged();
        } else {
          allocatedContainersBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public Builder setAllocatedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder builderForValue) {
        if (allocatedContainersBuilder_ == null) {
          ensureAllocatedContainersIsMutable();
          allocatedContainers_.set(index, builderForValue.build());
          onChanged();
        } else {
          allocatedContainersBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public Builder addAllocatedContainers(org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto value) {
        if (allocatedContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAllocatedContainersIsMutable();
          allocatedContainers_.add(value);
          onChanged();
        } else {
          allocatedContainersBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public Builder addAllocatedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto value) {
        if (allocatedContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureAllocatedContainersIsMutable();
          allocatedContainers_.add(index, value);
          onChanged();
        } else {
          allocatedContainersBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public Builder addAllocatedContainers(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder builderForValue) {
        if (allocatedContainersBuilder_ == null) {
          ensureAllocatedContainersIsMutable();
          allocatedContainers_.add(builderForValue.build());
          onChanged();
        } else {
          allocatedContainersBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public Builder addAllocatedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder builderForValue) {
        if (allocatedContainersBuilder_ == null) {
          ensureAllocatedContainersIsMutable();
          allocatedContainers_.add(index, builderForValue.build());
          onChanged();
        } else {
          allocatedContainersBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public Builder addAllAllocatedContainers(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto> values) {
        if (allocatedContainersBuilder_ == null) {
          ensureAllocatedContainersIsMutable();
          super.addAll(values, allocatedContainers_);
          onChanged();
        } else {
          allocatedContainersBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public Builder clearAllocatedContainers() {
        if (allocatedContainersBuilder_ == null) {
          allocatedContainers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
        } else {
          allocatedContainersBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public Builder removeAllocatedContainers(int index) {
        if (allocatedContainersBuilder_ == null) {
          ensureAllocatedContainersIsMutable();
          allocatedContainers_.remove(index);
          onChanged();
        } else {
          allocatedContainersBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder getAllocatedContainersBuilder(
          int index) {
        return getAllocatedContainersFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder getAllocatedContainersOrBuilder(
          int index) {
        if (allocatedContainersBuilder_ == null) {
          return allocatedContainers_.get(index);  } else {
          return allocatedContainersBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
           getAllocatedContainersOrBuilderList() {
        if (allocatedContainersBuilder_ != null) {
          return allocatedContainersBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(allocatedContainers_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder addAllocatedContainersBuilder() {
        return getAllocatedContainersFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder addAllocatedContainersBuilder(
          int index) {
        return getAllocatedContainersFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerProto allocated_containers = 3;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder> 
           getAllocatedContainersBuilderList() {
        return getAllocatedContainersFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder> 
          getAllocatedContainersFieldBuilder() {
        if (allocatedContainersBuilder_ == null) {
          allocatedContainersBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerProtoOrBuilder>(
                  allocatedContainers_,
                  ((bitField0_ & 0x00000004) == 0x00000004),
                  getParentForChildren(),
                  isClean());
          allocatedContainers_ = null;
        }
        return allocatedContainersBuilder_;
      }

      // repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto> completedContainerStatuses_ =
        java.util.Collections.emptyList();
      private void ensureCompletedContainerStatusesIsMutable() {
        if (!((bitField0_ & 0x00000008) == 0x00000008)) {
          completedContainerStatuses_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto>(completedContainerStatuses_);
          bitField0_ |= 0x00000008;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder> completedContainerStatusesBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto> getCompletedContainerStatusesList() {
        if (completedContainerStatusesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(completedContainerStatuses_);
        } else {
          return completedContainerStatusesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public int getCompletedContainerStatusesCount() {
        if (completedContainerStatusesBuilder_ == null) {
          return completedContainerStatuses_.size();
        } else {
          return completedContainerStatusesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto getCompletedContainerStatuses(int index) {
        if (completedContainerStatusesBuilder_ == null) {
          return completedContainerStatuses_.get(index);
        } else {
          return completedContainerStatusesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public Builder setCompletedContainerStatuses(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto value) {
        if (completedContainerStatusesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureCompletedContainerStatusesIsMutable();
          completedContainerStatuses_.set(index, value);
          onChanged();
        } else {
          completedContainerStatusesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public Builder setCompletedContainerStatuses(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder builderForValue) {
        if (completedContainerStatusesBuilder_ == null) {
          ensureCompletedContainerStatusesIsMutable();
          completedContainerStatuses_.set(index, builderForValue.build());
          onChanged();
        } else {
          completedContainerStatusesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public Builder addCompletedContainerStatuses(org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto value) {
        if (completedContainerStatusesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureCompletedContainerStatusesIsMutable();
          completedContainerStatuses_.add(value);
          onChanged();
        } else {
          completedContainerStatusesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public Builder addCompletedContainerStatuses(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto value) {
        if (completedContainerStatusesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureCompletedContainerStatusesIsMutable();
          completedContainerStatuses_.add(index, value);
          onChanged();
        } else {
          completedContainerStatusesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public Builder addCompletedContainerStatuses(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder builderForValue) {
        if (completedContainerStatusesBuilder_ == null) {
          ensureCompletedContainerStatusesIsMutable();
          completedContainerStatuses_.add(builderForValue.build());
          onChanged();
        } else {
          completedContainerStatusesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public Builder addCompletedContainerStatuses(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder builderForValue) {
        if (completedContainerStatusesBuilder_ == null) {
          ensureCompletedContainerStatusesIsMutable();
          completedContainerStatuses_.add(index, builderForValue.build());
          onChanged();
        } else {
          completedContainerStatusesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public Builder addAllCompletedContainerStatuses(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto> values) {
        if (completedContainerStatusesBuilder_ == null) {
          ensureCompletedContainerStatusesIsMutable();
          super.addAll(values, completedContainerStatuses_);
          onChanged();
        } else {
          completedContainerStatusesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public Builder clearCompletedContainerStatuses() {
        if (completedContainerStatusesBuilder_ == null) {
          completedContainerStatuses_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
          onChanged();
        } else {
          completedContainerStatusesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public Builder removeCompletedContainerStatuses(int index) {
        if (completedContainerStatusesBuilder_ == null) {
          ensureCompletedContainerStatusesIsMutable();
          completedContainerStatuses_.remove(index);
          onChanged();
        } else {
          completedContainerStatusesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder getCompletedContainerStatusesBuilder(
          int index) {
        return getCompletedContainerStatusesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder getCompletedContainerStatusesOrBuilder(
          int index) {
        if (completedContainerStatusesBuilder_ == null) {
          return completedContainerStatuses_.get(index);  } else {
          return completedContainerStatusesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder> 
           getCompletedContainerStatusesOrBuilderList() {
        if (completedContainerStatusesBuilder_ != null) {
          return completedContainerStatusesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(completedContainerStatuses_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder addCompletedContainerStatusesBuilder() {
        return getCompletedContainerStatusesFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder addCompletedContainerStatusesBuilder(
          int index) {
        return getCompletedContainerStatusesFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto completed_container_statuses = 4;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder> 
           getCompletedContainerStatusesBuilderList() {
        return getCompletedContainerStatusesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder> 
          getCompletedContainerStatusesFieldBuilder() {
        if (completedContainerStatusesBuilder_ == null) {
          completedContainerStatusesBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder>(
                  completedContainerStatuses_,
                  ((bitField0_ & 0x00000008) == 0x00000008),
                  getParentForChildren(),
                  isClean());
          completedContainerStatuses_ = null;
        }
        return completedContainerStatusesBuilder_;
      }

      // optional .hadoop.yarn.ResourceProto limit = 5;
      private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto limit_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> limitBuilder_;
      /**
       * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
       */
      public boolean hasLimit() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getLimit() {
        if (limitBuilder_ == null) {
          return limit_;
        } else {
          return limitBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
       */
      public Builder setLimit(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (limitBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          limit_ = value;
          onChanged();
        } else {
          limitBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
       */
      public Builder setLimit(
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder builderForValue) {
        if (limitBuilder_ == null) {
          limit_ = builderForValue.build();
          onChanged();
        } else {
          limitBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
       */
      public Builder mergeLimit(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (limitBuilder_ == null) {
          if (((bitField0_ & 0x00000010) == 0x00000010) &&
              limit_ != org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance()) {
            limit_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.newBuilder(limit_).mergeFrom(value).buildPartial();
          } else {
            limit_ = value;
          }
          onChanged();
        } else {
          limitBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
       */
      public Builder clearLimit() {
        if (limitBuilder_ == null) {
          limit_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
          onChanged();
        } else {
          limitBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000010);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder getLimitBuilder() {
        bitField0_ |= 0x00000010;
        onChanged();
        return getLimitFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getLimitOrBuilder() {
        if (limitBuilder_ != null) {
          return limitBuilder_.getMessageOrBuilder();
        } else {
          return limit_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto limit = 5;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> 
          getLimitFieldBuilder() {
        if (limitBuilder_ == null) {
          limitBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder>(
                  limit_,
                  getParentForChildren(),
                  isClean());
          limit_ = null;
        }
        return limitBuilder_;
      }

      // repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto> updatedNodes_ =
        java.util.Collections.emptyList();
      private void ensureUpdatedNodesIsMutable() {
        if (!((bitField0_ & 0x00000020) == 0x00000020)) {
          updatedNodes_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto>(updatedNodes_);
          bitField0_ |= 0x00000020;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder> updatedNodesBuilder_;

      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto> getUpdatedNodesList() {
        if (updatedNodesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(updatedNodes_);
        } else {
          return updatedNodesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public int getUpdatedNodesCount() {
        if (updatedNodesBuilder_ == null) {
          return updatedNodes_.size();
        } else {
          return updatedNodesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto getUpdatedNodes(int index) {
        if (updatedNodesBuilder_ == null) {
          return updatedNodes_.get(index);
        } else {
          return updatedNodesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public Builder setUpdatedNodes(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto value) {
        if (updatedNodesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureUpdatedNodesIsMutable();
          updatedNodes_.set(index, value);
          onChanged();
        } else {
          updatedNodesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public Builder setUpdatedNodes(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder builderForValue) {
        if (updatedNodesBuilder_ == null) {
          ensureUpdatedNodesIsMutable();
          updatedNodes_.set(index, builderForValue.build());
          onChanged();
        } else {
          updatedNodesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public Builder addUpdatedNodes(org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto value) {
        if (updatedNodesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureUpdatedNodesIsMutable();
          updatedNodes_.add(value);
          onChanged();
        } else {
          updatedNodesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public Builder addUpdatedNodes(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto value) {
        if (updatedNodesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureUpdatedNodesIsMutable();
          updatedNodes_.add(index, value);
          onChanged();
        } else {
          updatedNodesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public Builder addUpdatedNodes(
          org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder builderForValue) {
        if (updatedNodesBuilder_ == null) {
          ensureUpdatedNodesIsMutable();
          updatedNodes_.add(builderForValue.build());
          onChanged();
        } else {
          updatedNodesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public Builder addUpdatedNodes(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder builderForValue) {
        if (updatedNodesBuilder_ == null) {
          ensureUpdatedNodesIsMutable();
          updatedNodes_.add(index, builderForValue.build());
          onChanged();
        } else {
          updatedNodesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public Builder addAllUpdatedNodes(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto> values) {
        if (updatedNodesBuilder_ == null) {
          ensureUpdatedNodesIsMutable();
          super.addAll(values, updatedNodes_);
          onChanged();
        } else {
          updatedNodesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public Builder clearUpdatedNodes() {
        if (updatedNodesBuilder_ == null) {
          updatedNodes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
          onChanged();
        } else {
          updatedNodesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public Builder removeUpdatedNodes(int index) {
        if (updatedNodesBuilder_ == null) {
          ensureUpdatedNodesIsMutable();
          updatedNodes_.remove(index);
          onChanged();
        } else {
          updatedNodesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder getUpdatedNodesBuilder(
          int index) {
        return getUpdatedNodesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder getUpdatedNodesOrBuilder(
          int index) {
        if (updatedNodesBuilder_ == null) {
          return updatedNodes_.get(index);  } else {
          return updatedNodesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder> 
           getUpdatedNodesOrBuilderList() {
        if (updatedNodesBuilder_ != null) {
          return updatedNodesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(updatedNodes_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder addUpdatedNodesBuilder() {
        return getUpdatedNodesFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder addUpdatedNodesBuilder(
          int index) {
        return getUpdatedNodesFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto updated_nodes = 6;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder> 
           getUpdatedNodesBuilderList() {
        return getUpdatedNodesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder> 
          getUpdatedNodesFieldBuilder() {
        if (updatedNodesBuilder_ == null) {
          updatedNodesBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder>(
                  updatedNodes_,
                  ((bitField0_ & 0x00000020) == 0x00000020),
                  getParentForChildren(),
                  isClean());
          updatedNodes_ = null;
        }
        return updatedNodesBuilder_;
      }

      // optional int32 num_cluster_nodes = 7;
      private int numClusterNodes_ ;
      /**
       * <code>optional int32 num_cluster_nodes = 7;</code>
       */
      public boolean hasNumClusterNodes() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      /**
       * <code>optional int32 num_cluster_nodes = 7;</code>
       */
      public int getNumClusterNodes() {
        return numClusterNodes_;
      }
      /**
       * <code>optional int32 num_cluster_nodes = 7;</code>
       */
      public Builder setNumClusterNodes(int value) {
        bitField0_ |= 0x00000040;
        numClusterNodes_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int32 num_cluster_nodes = 7;</code>
       */
      public Builder clearNumClusterNodes() {
        bitField0_ = (bitField0_ & ~0x00000040);
        numClusterNodes_ = 0;
        onChanged();
        return this;
      }

      // optional .hadoop.yarn.PreemptionMessageProto preempt = 8;
      private org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto preempt_ = org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto, org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProtoOrBuilder> preemptBuilder_;
      /**
       * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
       */
      public boolean hasPreempt() {
        return ((bitField0_ & 0x00000080) == 0x00000080);
      }
      /**
       * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto getPreempt() {
        if (preemptBuilder_ == null) {
          return preempt_;
        } else {
          return preemptBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
       */
      public Builder setPreempt(org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto value) {
        if (preemptBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          preempt_ = value;
          onChanged();
        } else {
          preemptBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
       */
      public Builder setPreempt(
          org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.Builder builderForValue) {
        if (preemptBuilder_ == null) {
          preempt_ = builderForValue.build();
          onChanged();
        } else {
          preemptBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
       */
      public Builder mergePreempt(org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto value) {
        if (preemptBuilder_ == null) {
          if (((bitField0_ & 0x00000080) == 0x00000080) &&
              preempt_ != org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.getDefaultInstance()) {
            preempt_ =
              org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.newBuilder(preempt_).mergeFrom(value).buildPartial();
          } else {
            preempt_ = value;
          }
          onChanged();
        } else {
          preemptBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
       */
      public Builder clearPreempt() {
        if (preemptBuilder_ == null) {
          preempt_ = org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.getDefaultInstance();
          onChanged();
        } else {
          preemptBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000080);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.Builder getPreemptBuilder() {
        bitField0_ |= 0x00000080;
        onChanged();
        return getPreemptFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProtoOrBuilder getPreemptOrBuilder() {
        if (preemptBuilder_ != null) {
          return preemptBuilder_.getMessageOrBuilder();
        } else {
          return preempt_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.PreemptionMessageProto preempt = 8;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto, org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProtoOrBuilder> 
          getPreemptFieldBuilder() {
        if (preemptBuilder_ == null) {
          preemptBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto, org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.PreemptionMessageProtoOrBuilder>(
                  preempt_,
                  getParentForChildren(),
                  isClean());
          preempt_ = null;
        }
        return preemptBuilder_;
      }

      // repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto> nmTokens_ =
        java.util.Collections.emptyList();
      private void ensureNmTokensIsMutable() {
        if (!((bitField0_ & 0x00000100) == 0x00000100)) {
          nmTokens_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto>(nmTokens_);
          bitField0_ |= 0x00000100;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder> nmTokensBuilder_;

      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto> getNmTokensList() {
        if (nmTokensBuilder_ == null) {
          return java.util.Collections.unmodifiableList(nmTokens_);
        } else {
          return nmTokensBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public int getNmTokensCount() {
        if (nmTokensBuilder_ == null) {
          return nmTokens_.size();
        } else {
          return nmTokensBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto getNmTokens(int index) {
        if (nmTokensBuilder_ == null) {
          return nmTokens_.get(index);
        } else {
          return nmTokensBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public Builder setNmTokens(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto value) {
        if (nmTokensBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNmTokensIsMutable();
          nmTokens_.set(index, value);
          onChanged();
        } else {
          nmTokensBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public Builder setNmTokens(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder builderForValue) {
        if (nmTokensBuilder_ == null) {
          ensureNmTokensIsMutable();
          nmTokens_.set(index, builderForValue.build());
          onChanged();
        } else {
          nmTokensBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public Builder addNmTokens(org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto value) {
        if (nmTokensBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNmTokensIsMutable();
          nmTokens_.add(value);
          onChanged();
        } else {
          nmTokensBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public Builder addNmTokens(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto value) {
        if (nmTokensBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNmTokensIsMutable();
          nmTokens_.add(index, value);
          onChanged();
        } else {
          nmTokensBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public Builder addNmTokens(
          org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder builderForValue) {
        if (nmTokensBuilder_ == null) {
          ensureNmTokensIsMutable();
          nmTokens_.add(builderForValue.build());
          onChanged();
        } else {
          nmTokensBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public Builder addNmTokens(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder builderForValue) {
        if (nmTokensBuilder_ == null) {
          ensureNmTokensIsMutable();
          nmTokens_.add(index, builderForValue.build());
          onChanged();
        } else {
          nmTokensBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public Builder addAllNmTokens(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto> values) {
        if (nmTokensBuilder_ == null) {
          ensureNmTokensIsMutable();
          super.addAll(values, nmTokens_);
          onChanged();
        } else {
          nmTokensBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public Builder clearNmTokens() {
        if (nmTokensBuilder_ == null) {
          nmTokens_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000100);
          onChanged();
        } else {
          nmTokensBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public Builder removeNmTokens(int index) {
        if (nmTokensBuilder_ == null) {
          ensureNmTokensIsMutable();
          nmTokens_.remove(index);
          onChanged();
        } else {
          nmTokensBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder getNmTokensBuilder(
          int index) {
        return getNmTokensFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder getNmTokensOrBuilder(
          int index) {
        if (nmTokensBuilder_ == null) {
          return nmTokens_.get(index);  } else {
          return nmTokensBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder> 
           getNmTokensOrBuilderList() {
        if (nmTokensBuilder_ != null) {
          return nmTokensBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(nmTokens_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder addNmTokensBuilder() {
        return getNmTokensFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder addNmTokensBuilder(
          int index) {
        return getNmTokensFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.NMTokenProto nm_tokens = 9;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder> 
           getNmTokensBuilderList() {
        return getNmTokensFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder> 
          getNmTokensFieldBuilder() {
        if (nmTokensBuilder_ == null) {
          nmTokensBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.NMTokenProtoOrBuilder>(
                  nmTokens_,
                  ((bitField0_ & 0x00000100) == 0x00000100),
                  getParentForChildren(),
                  isClean());
          nmTokens_ = null;
        }
        return nmTokensBuilder_;
      }

      // repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto> increasedContainers_ =
        java.util.Collections.emptyList();
      private void ensureIncreasedContainersIsMutable() {
        if (!((bitField0_ & 0x00000200) == 0x00000200)) {
          increasedContainers_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto>(increasedContainers_);
          bitField0_ |= 0x00000200;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProtoOrBuilder> increasedContainersBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto> getIncreasedContainersList() {
        if (increasedContainersBuilder_ == null) {
          return java.util.Collections.unmodifiableList(increasedContainers_);
        } else {
          return increasedContainersBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public int getIncreasedContainersCount() {
        if (increasedContainersBuilder_ == null) {
          return increasedContainers_.size();
        } else {
          return increasedContainersBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto getIncreasedContainers(int index) {
        if (increasedContainersBuilder_ == null) {
          return increasedContainers_.get(index);
        } else {
          return increasedContainersBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public Builder setIncreasedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto value) {
        if (increasedContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureIncreasedContainersIsMutable();
          increasedContainers_.set(index, value);
          onChanged();
        } else {
          increasedContainersBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public Builder setIncreasedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.Builder builderForValue) {
        if (increasedContainersBuilder_ == null) {
          ensureIncreasedContainersIsMutable();
          increasedContainers_.set(index, builderForValue.build());
          onChanged();
        } else {
          increasedContainersBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public Builder addIncreasedContainers(org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto value) {
        if (increasedContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureIncreasedContainersIsMutable();
          increasedContainers_.add(value);
          onChanged();
        } else {
          increasedContainersBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public Builder addIncreasedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto value) {
        if (increasedContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureIncreasedContainersIsMutable();
          increasedContainers_.add(index, value);
          onChanged();
        } else {
          increasedContainersBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public Builder addIncreasedContainers(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.Builder builderForValue) {
        if (increasedContainersBuilder_ == null) {
          ensureIncreasedContainersIsMutable();
          increasedContainers_.add(builderForValue.build());
          onChanged();
        } else {
          increasedContainersBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public Builder addIncreasedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.Builder builderForValue) {
        if (increasedContainersBuilder_ == null) {
          ensureIncreasedContainersIsMutable();
          increasedContainers_.add(index, builderForValue.build());
          onChanged();
        } else {
          increasedContainersBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public Builder addAllIncreasedContainers(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto> values) {
        if (increasedContainersBuilder_ == null) {
          ensureIncreasedContainersIsMutable();
          super.addAll(values, increasedContainers_);
          onChanged();
        } else {
          increasedContainersBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public Builder clearIncreasedContainers() {
        if (increasedContainersBuilder_ == null) {
          increasedContainers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000200);
          onChanged();
        } else {
          increasedContainersBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public Builder removeIncreasedContainers(int index) {
        if (increasedContainersBuilder_ == null) {
          ensureIncreasedContainersIsMutable();
          increasedContainers_.remove(index);
          onChanged();
        } else {
          increasedContainersBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.Builder getIncreasedContainersBuilder(
          int index) {
        return getIncreasedContainersFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProtoOrBuilder getIncreasedContainersOrBuilder(
          int index) {
        if (increasedContainersBuilder_ == null) {
          return increasedContainers_.get(index);  } else {
          return increasedContainersBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProtoOrBuilder> 
           getIncreasedContainersOrBuilderList() {
        if (increasedContainersBuilder_ != null) {
          return increasedContainersBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(increasedContainers_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.Builder addIncreasedContainersBuilder() {
        return getIncreasedContainersFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.Builder addIncreasedContainersBuilder(
          int index) {
        return getIncreasedContainersFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceIncreaseProto increased_containers = 10;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.Builder> 
           getIncreasedContainersBuilderList() {
        return getIncreasedContainersFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProtoOrBuilder> 
          getIncreasedContainersFieldBuilder() {
        if (increasedContainersBuilder_ == null) {
          increasedContainersBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceIncreaseProtoOrBuilder>(
                  increasedContainers_,
                  ((bitField0_ & 0x00000200) == 0x00000200),
                  getParentForChildren(),
                  isClean());
          increasedContainers_ = null;
        }
        return increasedContainersBuilder_;
      }

      // repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto> decreasedContainers_ =
        java.util.Collections.emptyList();
      private void ensureDecreasedContainersIsMutable() {
        if (!((bitField0_ & 0x00000400) == 0x00000400)) {
          decreasedContainers_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto>(decreasedContainers_);
          bitField0_ |= 0x00000400;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProtoOrBuilder> decreasedContainersBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto> getDecreasedContainersList() {
        if (decreasedContainersBuilder_ == null) {
          return java.util.Collections.unmodifiableList(decreasedContainers_);
        } else {
          return decreasedContainersBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public int getDecreasedContainersCount() {
        if (decreasedContainersBuilder_ == null) {
          return decreasedContainers_.size();
        } else {
          return decreasedContainersBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto getDecreasedContainers(int index) {
        if (decreasedContainersBuilder_ == null) {
          return decreasedContainers_.get(index);
        } else {
          return decreasedContainersBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public Builder setDecreasedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto value) {
        if (decreasedContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureDecreasedContainersIsMutable();
          decreasedContainers_.set(index, value);
          onChanged();
        } else {
          decreasedContainersBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public Builder setDecreasedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.Builder builderForValue) {
        if (decreasedContainersBuilder_ == null) {
          ensureDecreasedContainersIsMutable();
          decreasedContainers_.set(index, builderForValue.build());
          onChanged();
        } else {
          decreasedContainersBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public Builder addDecreasedContainers(org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto value) {
        if (decreasedContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureDecreasedContainersIsMutable();
          decreasedContainers_.add(value);
          onChanged();
        } else {
          decreasedContainersBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public Builder addDecreasedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto value) {
        if (decreasedContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureDecreasedContainersIsMutable();
          decreasedContainers_.add(index, value);
          onChanged();
        } else {
          decreasedContainersBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public Builder addDecreasedContainers(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.Builder builderForValue) {
        if (decreasedContainersBuilder_ == null) {
          ensureDecreasedContainersIsMutable();
          decreasedContainers_.add(builderForValue.build());
          onChanged();
        } else {
          decreasedContainersBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public Builder addDecreasedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.Builder builderForValue) {
        if (decreasedContainersBuilder_ == null) {
          ensureDecreasedContainersIsMutable();
          decreasedContainers_.add(index, builderForValue.build());
          onChanged();
        } else {
          decreasedContainersBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public Builder addAllDecreasedContainers(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto> values) {
        if (decreasedContainersBuilder_ == null) {
          ensureDecreasedContainersIsMutable();
          super.addAll(values, decreasedContainers_);
          onChanged();
        } else {
          decreasedContainersBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public Builder clearDecreasedContainers() {
        if (decreasedContainersBuilder_ == null) {
          decreasedContainers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000400);
          onChanged();
        } else {
          decreasedContainersBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public Builder removeDecreasedContainers(int index) {
        if (decreasedContainersBuilder_ == null) {
          ensureDecreasedContainersIsMutable();
          decreasedContainers_.remove(index);
          onChanged();
        } else {
          decreasedContainersBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.Builder getDecreasedContainersBuilder(
          int index) {
        return getDecreasedContainersFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProtoOrBuilder getDecreasedContainersOrBuilder(
          int index) {
        if (decreasedContainersBuilder_ == null) {
          return decreasedContainers_.get(index);  } else {
          return decreasedContainersBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProtoOrBuilder> 
           getDecreasedContainersOrBuilderList() {
        if (decreasedContainersBuilder_ != null) {
          return decreasedContainersBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(decreasedContainers_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.Builder addDecreasedContainersBuilder() {
        return getDecreasedContainersFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.Builder addDecreasedContainersBuilder(
          int index) {
        return getDecreasedContainersFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerResourceDecreaseProto decreased_containers = 11;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.Builder> 
           getDecreasedContainersBuilderList() {
        return getDecreasedContainersFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProtoOrBuilder> 
          getDecreasedContainersFieldBuilder() {
        if (decreasedContainersBuilder_ == null) {
          decreasedContainersBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerResourceDecreaseProtoOrBuilder>(
                  decreasedContainers_,
                  ((bitField0_ & 0x00000400) == 0x00000400),
                  getParentForChildren(),
                  isClean());
          decreasedContainers_ = null;
        }
        return decreasedContainersBuilder_;
      }

      // optional .hadoop.common.TokenProto am_rm_token = 12;
      private org.apache.hadoop.security.proto.SecurityProtos.TokenProto amRmToken_ = org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.security.proto.SecurityProtos.TokenProto, org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder, org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder> amRmTokenBuilder_;
      /**
       * <code>optional .hadoop.common.TokenProto am_rm_token = 12;</code>
       */
      public boolean hasAmRmToken() {
        return ((bitField0_ & 0x00000800) == 0x00000800);
      }
      /**
       * <code>optional .hadoop.common.TokenProto am_rm_token = 12;</code>
       */
      public org.apache.hadoop.security.proto.SecurityProtos.TokenProto getAmRmToken() {
        if (amRmTokenBuilder_ == null) {
          return amRmToken_;
        } else {
          return amRmTokenBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.common.TokenProto am_rm_token = 12;</code>
       */
      public Builder setAmRmToken(org.apache.hadoop.security.proto.SecurityProtos.TokenProto value) {
        if (amRmTokenBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          amRmToken_ = value;
          onChanged();
        } else {
          amRmTokenBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000800;
        return this;
      }
      /**
       * <code>optional .hadoop.common.TokenProto am_rm_token = 12;</code>
       */
      public Builder setAmRmToken(
          org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder builderForValue) {
        if (amRmTokenBuilder_ == null) {
          amRmToken_ = builderForValue.build();
          onChanged();
        } else {
          amRmTokenBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000800;
        return this;
      }
      /**
       * <code>optional .hadoop.common.TokenProto am_rm_token = 12;</code>
       */
      public Builder mergeAmRmToken(org.apache.hadoop.security.proto.SecurityProtos.TokenProto value) {
        if (amRmTokenBuilder_ == null) {
          if (((bitField0_ & 0x00000800) == 0x00000800) &&
              amRmToken_ != org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance()) {
            amRmToken_ =
              org.apache.hadoop.security.proto.SecurityProtos.TokenProto.newBuilder(amRmToken_).mergeFrom(value).buildPartial();
          } else {
            amRmToken_ = value;
          }
          onChanged();
        } else {
          amRmTokenBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000800;
        return this;
      }
      /**
       * <code>optional .hadoop.common.TokenProto am_rm_token = 12;</code>
       */
      public Builder clearAmRmToken() {
        if (amRmTokenBuilder_ == null) {
          amRmToken_ = org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance();
          onChanged();
        } else {
          amRmTokenBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000800);
        return this;
      }
      /**
       * <code>optional .hadoop.common.TokenProto am_rm_token = 12;</code>
       */
      public org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder getAmRmTokenBuilder() {
        bitField0_ |= 0x00000800;
        onChanged();
        return getAmRmTokenFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.common.TokenProto am_rm_token = 12;</code>
       */
      public org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder getAmRmTokenOrBuilder() {
        if (amRmTokenBuilder_ != null) {
          return amRmTokenBuilder_.getMessageOrBuilder();
        } else {
          return amRmToken_;
        }
      }
      /**
       * <code>optional .hadoop.common.TokenProto am_rm_token = 12;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.security.proto.SecurityProtos.TokenProto, org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder, org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder> 
          getAmRmTokenFieldBuilder() {
        if (amRmTokenBuilder_ == null) {
          amRmTokenBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.security.proto.SecurityProtos.TokenProto, org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder, org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder>(
                  amRmToken_,
                  getParentForChildren(),
                  isClean());
          amRmToken_ = null;
        }
        return amRmTokenBuilder_;
      }

      // optional .hadoop.yarn.PriorityProto application_priority = 13;
      private org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto applicationPriority_ = org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder> applicationPriorityBuilder_;
      /**
       * <code>optional .hadoop.yarn.PriorityProto application_priority = 13;</code>
       */
      public boolean hasApplicationPriority() {
        return ((bitField0_ & 0x00001000) == 0x00001000);
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto application_priority = 13;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto getApplicationPriority() {
        if (applicationPriorityBuilder_ == null) {
          return applicationPriority_;
        } else {
          return applicationPriorityBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto application_priority = 13;</code>
       */
      public Builder setApplicationPriority(org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto value) {
        if (applicationPriorityBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationPriority_ = value;
          onChanged();
        } else {
          applicationPriorityBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00001000;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto application_priority = 13;</code>
       */
      public Builder setApplicationPriority(
          org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder builderForValue) {
        if (applicationPriorityBuilder_ == null) {
          applicationPriority_ = builderForValue.build();
          onChanged();
        } else {
          applicationPriorityBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00001000;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto application_priority = 13;</code>
       */
      public Builder mergeApplicationPriority(org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto value) {
        if (applicationPriorityBuilder_ == null) {
          if (((bitField0_ & 0x00001000) == 0x00001000) &&
              applicationPriority_ != org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance()) {
            applicationPriority_ =
              org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.newBuilder(applicationPriority_).mergeFrom(value).buildPartial();
          } else {
            applicationPriority_ = value;
          }
          onChanged();
        } else {
          applicationPriorityBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00001000;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto application_priority = 13;</code>
       */
      public Builder clearApplicationPriority() {
        if (applicationPriorityBuilder_ == null) {
          applicationPriority_ = org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance();
          onChanged();
        } else {
          applicationPriorityBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00001000);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto application_priority = 13;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder getApplicationPriorityBuilder() {
        bitField0_ |= 0x00001000;
        onChanged();
        return getApplicationPriorityFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto application_priority = 13;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder getApplicationPriorityOrBuilder() {
        if (applicationPriorityBuilder_ != null) {
          return applicationPriorityBuilder_.getMessageOrBuilder();
        } else {
          return applicationPriority_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto application_priority = 13;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder> 
          getApplicationPriorityFieldBuilder() {
        if (applicationPriorityBuilder_ == null) {
          applicationPriorityBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder>(
                  applicationPriority_,
                  getParentForChildren(),
                  isClean());
          applicationPriority_ = null;
        }
        return applicationPriorityBuilder_;
      }

      // repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto> updateErrors_ =
        java.util.Collections.emptyList();
      private void ensureUpdateErrorsIsMutable() {
        if (!((bitField0_ & 0x00002000) == 0x00002000)) {
          updateErrors_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto>(updateErrors_);
          bitField0_ |= 0x00002000;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProtoOrBuilder> updateErrorsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto> getUpdateErrorsList() {
        if (updateErrorsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(updateErrors_);
        } else {
          return updateErrorsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
       */
      public int getUpdateErrorsCount() {
        if (updateErrorsBuilder_ == null) {
          return updateErrors_.size();
        } else {
          return updateErrorsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto getUpdateErrors(int index) {
        if (updateErrorsBuilder_ == null) {
          return updateErrors_.get(index);
        } else {
          return updateErrorsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
       */
      public Builder setUpdateErrors(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto value) {
        if (updateErrorsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureUpdateErrorsIsMutable();
          updateErrors_.set(index, value);
          onChanged();
        } else {
          updateErrorsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
       */
      public Builder setUpdateErrors(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto.Builder builderForValue) {
        if (updateErrorsBuilder_ == null) {
          ensureUpdateErrorsIsMutable();
          updateErrors_.set(index, builderForValue.build());
          onChanged();
        } else {
          updateErrorsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
       */
      public Builder addUpdateErrors(org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto value) {
        if (updateErrorsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureUpdateErrorsIsMutable();
          updateErrors_.add(value);
          onChanged();
        } else {
          updateErrorsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
       */
      public Builder addUpdateErrors(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto value) {
        if (updateErrorsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureUpdateErrorsIsMutable();
          updateErrors_.add(index, value);
          onChanged();
        } else {
          updateErrorsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
       */
      public Builder addUpdateErrors(
          org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto.Builder builderForValue) {
        if (updateErrorsBuilder_ == null) {
          ensureUpdateErrorsIsMutable();
          updateErrors_.add(builderForValue.build());
          onChanged();
        } else {
          updateErrorsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
       */
      public Builder addUpdateErrors(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto.Builder builderForValue) {
        if (updateErrorsBuilder_ == null) {
          ensureUpdateErrorsIsMutable();
          updateErrors_.add(index, builderForValue.build());
          onChanged();
        } else {
          updateErrorsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
       */
      public Builder addAllUpdateErrors(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto> values) {
        if (updateErrorsBuilder_ == null) {
          ensureUpdateErrorsIsMutable();
          super.addAll(values, updateErrors_);
          onChanged();
        } else {
          updateErrorsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
       */
      public Builder clearUpdateErrors() {
        if (updateErrorsBuilder_ == null) {
          updateErrors_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00002000);
          onChanged();
        } else {
          updateErrorsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
       */
      public Builder removeUpdateErrors(int index) {
        if (updateErrorsBuilder_ == null) {
          ensureUpdateErrorsIsMutable();
          updateErrors_.remove(index);
          onChanged();
        } else {
          updateErrorsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto.Builder getUpdateErrorsBuilder(
          int index) {
        return getUpdateErrorsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProtoOrBuilder getUpdateErrorsOrBuilder(
          int index) {
        if (updateErrorsBuilder_ == null) {
          return updateErrors_.get(index);  } else {
          return updateErrorsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProtoOrBuilder> 
           getUpdateErrorsOrBuilderList() {
        if (updateErrorsBuilder_ != null) {
          return updateErrorsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(updateErrors_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto.Builder addUpdateErrorsBuilder() {
        return getUpdateErrorsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto.Builder addUpdateErrorsBuilder(
          int index) {
        return getUpdateErrorsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.UpdateContainerErrorProto update_errors = 15;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto.Builder> 
           getUpdateErrorsBuilderList() {
        return getUpdateErrorsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProtoOrBuilder> 
          getUpdateErrorsFieldBuilder() {
        if (updateErrorsBuilder_ == null) {
          updateErrorsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateContainerErrorProtoOrBuilder>(
                  updateErrors_,
                  ((bitField0_ & 0x00002000) == 0x00002000),
                  getParentForChildren(),
                  isClean());
          updateErrors_ = null;
        }
        return updateErrorsBuilder_;
      }

      // repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto> updatedContainers_ =
        java.util.Collections.emptyList();
      private void ensureUpdatedContainersIsMutable() {
        if (!((bitField0_ & 0x00004000) == 0x00004000)) {
          updatedContainers_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto>(updatedContainers_);
          bitField0_ |= 0x00004000;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProtoOrBuilder> updatedContainersBuilder_;

      /**
       * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto> getUpdatedContainersList() {
        if (updatedContainersBuilder_ == null) {
          return java.util.Collections.unmodifiableList(updatedContainers_);
        } else {
          return updatedContainersBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
       */
      public int getUpdatedContainersCount() {
        if (updatedContainersBuilder_ == null) {
          return updatedContainers_.size();
        } else {
          return updatedContainersBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto getUpdatedContainers(int index) {
        if (updatedContainersBuilder_ == null) {
          return updatedContainers_.get(index);
        } else {
          return updatedContainersBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
       */
      public Builder setUpdatedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto value) {
        if (updatedContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureUpdatedContainersIsMutable();
          updatedContainers_.set(index, value);
          onChanged();
        } else {
          updatedContainersBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
       */
      public Builder setUpdatedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto.Builder builderForValue) {
        if (updatedContainersBuilder_ == null) {
          ensureUpdatedContainersIsMutable();
          updatedContainers_.set(index, builderForValue.build());
          onChanged();
        } else {
          updatedContainersBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
       */
      public Builder addUpdatedContainers(org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto value) {
        if (updatedContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureUpdatedContainersIsMutable();
          updatedContainers_.add(value);
          onChanged();
        } else {
          updatedContainersBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
       */
      public Builder addUpdatedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto value) {
        if (updatedContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureUpdatedContainersIsMutable();
          updatedContainers_.add(index, value);
          onChanged();
        } else {
          updatedContainersBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
       */
      public Builder addUpdatedContainers(
          org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto.Builder builderForValue) {
        if (updatedContainersBuilder_ == null) {
          ensureUpdatedContainersIsMutable();
          updatedContainers_.add(builderForValue.build());
          onChanged();
        } else {
          updatedContainersBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
       */
      public Builder addUpdatedContainers(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto.Builder builderForValue) {
        if (updatedContainersBuilder_ == null) {
          ensureUpdatedContainersIsMutable();
          updatedContainers_.add(index, builderForValue.build());
          onChanged();
        } else {
          updatedContainersBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
       */
      public Builder addAllUpdatedContainers(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto> values) {
        if (updatedContainersBuilder_ == null) {
          ensureUpdatedContainersIsMutable();
          super.addAll(values, updatedContainers_);
          onChanged();
        } else {
          updatedContainersBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
       */
      public Builder clearUpdatedContainers() {
        if (updatedContainersBuilder_ == null) {
          updatedContainers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00004000);
          onChanged();
        } else {
          updatedContainersBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
       */
      public Builder removeUpdatedContainers(int index) {
        if (updatedContainersBuilder_ == null) {
          ensureUpdatedContainersIsMutable();
          updatedContainers_.remove(index);
          onChanged();
        } else {
          updatedContainersBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto.Builder getUpdatedContainersBuilder(
          int index) {
        return getUpdatedContainersFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProtoOrBuilder getUpdatedContainersOrBuilder(
          int index) {
        if (updatedContainersBuilder_ == null) {
          return updatedContainers_.get(index);  } else {
          return updatedContainersBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProtoOrBuilder> 
           getUpdatedContainersOrBuilderList() {
        if (updatedContainersBuilder_ != null) {
          return updatedContainersBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(updatedContainers_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto.Builder addUpdatedContainersBuilder() {
        return getUpdatedContainersFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto.Builder addUpdatedContainersBuilder(
          int index) {
        return getUpdatedContainersFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.UpdatedContainerProto updated_containers = 16;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto.Builder> 
           getUpdatedContainersBuilderList() {
        return getUpdatedContainersFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProtoOrBuilder> 
          getUpdatedContainersFieldBuilder() {
        if (updatedContainersBuilder_ == null) {
          updatedContainersBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdatedContainerProtoOrBuilder>(
                  updatedContainers_,
                  ((bitField0_ & 0x00004000) == 0x00004000),
                  getParentForChildren(),
                  isClean());
          updatedContainers_ = null;
        }
        return updatedContainersBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.AllocateResponseProto)
    }

    static {
      defaultInstance = new AllocateResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.AllocateResponseProto)
  }

  public interface GetNewApplicationRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetNewApplicationRequestProto}
   */
  public static final class GetNewApplicationRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetNewApplicationRequestProtoOrBuilder {
    // Use GetNewApplicationRequestProto.newBuilder() to construct.
    private GetNewApplicationRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetNewApplicationRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetNewApplicationRequestProto defaultInstance;
    public static GetNewApplicationRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetNewApplicationRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetNewApplicationRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewApplicationRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewApplicationRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetNewApplicationRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetNewApplicationRequestProto>() {
      public GetNewApplicationRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetNewApplicationRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetNewApplicationRequestProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetNewApplicationRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewApplicationRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewApplicationRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewApplicationRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetNewApplicationRequestProto)
    }

    static {
      defaultInstance = new GetNewApplicationRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetNewApplicationRequestProto)
  }

  public interface GetNewApplicationResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ApplicationIdProto application_id = 1;
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    boolean hasApplicationId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder();

    // optional .hadoop.yarn.ResourceProto maximumCapability = 2;
    /**
     * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
     */
    boolean hasMaximumCapability();
    /**
     * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getMaximumCapability();
    /**
     * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getMaximumCapabilityOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetNewApplicationResponseProto}
   */
  public static final class GetNewApplicationResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetNewApplicationResponseProtoOrBuilder {
    // Use GetNewApplicationResponseProto.newBuilder() to construct.
    private GetNewApplicationResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetNewApplicationResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetNewApplicationResponseProto defaultInstance;
    public static GetNewApplicationResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetNewApplicationResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetNewApplicationResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = applicationId_.toBuilder();
              }
              applicationId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationId_);
                applicationId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) == 0x00000002)) {
                subBuilder = maximumCapability_.toBuilder();
              }
              maximumCapability_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(maximumCapability_);
                maximumCapability_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewApplicationResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewApplicationResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetNewApplicationResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetNewApplicationResponseProto>() {
      public GetNewApplicationResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetNewApplicationResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetNewApplicationResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ApplicationIdProto application_id = 1;
    public static final int APPLICATION_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto applicationId_;
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public boolean hasApplicationId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId() {
      return applicationId_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder() {
      return applicationId_;
    }

    // optional .hadoop.yarn.ResourceProto maximumCapability = 2;
    public static final int MAXIMUMCAPABILITY_FIELD_NUMBER = 2;
    private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto maximumCapability_;
    /**
     * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
     */
    public boolean hasMaximumCapability() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getMaximumCapability() {
      return maximumCapability_;
    }
    /**
     * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getMaximumCapabilityOrBuilder() {
      return maximumCapability_;
    }

    private void initFields() {
      applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
      maximumCapability_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, applicationId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeMessage(2, maximumCapability_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, applicationId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, maximumCapability_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto) obj;

      boolean result = true;
      result = result && (hasApplicationId() == other.hasApplicationId());
      if (hasApplicationId()) {
        result = result && getApplicationId()
            .equals(other.getApplicationId());
      }
      result = result && (hasMaximumCapability() == other.hasMaximumCapability());
      if (hasMaximumCapability()) {
        result = result && getMaximumCapability()
            .equals(other.getMaximumCapability());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasApplicationId()) {
        hash = (37 * hash) + APPLICATION_ID_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationId().hashCode();
      }
      if (hasMaximumCapability()) {
        hash = (37 * hash) + MAXIMUMCAPABILITY_FIELD_NUMBER;
        hash = (53 * hash) + getMaximumCapability().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetNewApplicationResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewApplicationResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewApplicationResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getApplicationIdFieldBuilder();
          getMaximumCapabilityFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (applicationIdBuilder_ == null) {
          applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
        } else {
          applicationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (maximumCapabilityBuilder_ == null) {
          maximumCapability_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
        } else {
          maximumCapabilityBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewApplicationResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (applicationIdBuilder_ == null) {
          result.applicationId_ = applicationId_;
        } else {
          result.applicationId_ = applicationIdBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        if (maximumCapabilityBuilder_ == null) {
          result.maximumCapability_ = maximumCapability_;
        } else {
          result.maximumCapability_ = maximumCapabilityBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto.getDefaultInstance()) return this;
        if (other.hasApplicationId()) {
          mergeApplicationId(other.getApplicationId());
        }
        if (other.hasMaximumCapability()) {
          mergeMaximumCapability(other.getMaximumCapability());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewApplicationResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ApplicationIdProto application_id = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> applicationIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public boolean hasApplicationId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId() {
        if (applicationIdBuilder_ == null) {
          return applicationId_;
        } else {
          return applicationIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder setApplicationId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationId_ = value;
          onChanged();
        } else {
          applicationIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder setApplicationId(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder builderForValue) {
        if (applicationIdBuilder_ == null) {
          applicationId_ = builderForValue.build();
          onChanged();
        } else {
          applicationIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder mergeApplicationId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              applicationId_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance()) {
            applicationId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.newBuilder(applicationId_).mergeFrom(value).buildPartial();
          } else {
            applicationId_ = value;
          }
          onChanged();
        } else {
          applicationIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder clearApplicationId() {
        if (applicationIdBuilder_ == null) {
          applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
          onChanged();
        } else {
          applicationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder getApplicationIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getApplicationIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder() {
        if (applicationIdBuilder_ != null) {
          return applicationIdBuilder_.getMessageOrBuilder();
        } else {
          return applicationId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> 
          getApplicationIdFieldBuilder() {
        if (applicationIdBuilder_ == null) {
          applicationIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder>(
                  applicationId_,
                  getParentForChildren(),
                  isClean());
          applicationId_ = null;
        }
        return applicationIdBuilder_;
      }

      // optional .hadoop.yarn.ResourceProto maximumCapability = 2;
      private org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto maximumCapability_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> maximumCapabilityBuilder_;
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
       */
      public boolean hasMaximumCapability() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto getMaximumCapability() {
        if (maximumCapabilityBuilder_ == null) {
          return maximumCapability_;
        } else {
          return maximumCapabilityBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
       */
      public Builder setMaximumCapability(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (maximumCapabilityBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          maximumCapability_ = value;
          onChanged();
        } else {
          maximumCapabilityBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
       */
      public Builder setMaximumCapability(
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder builderForValue) {
        if (maximumCapabilityBuilder_ == null) {
          maximumCapability_ = builderForValue.build();
          onChanged();
        } else {
          maximumCapabilityBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
       */
      public Builder mergeMaximumCapability(org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto value) {
        if (maximumCapabilityBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002) &&
              maximumCapability_ != org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance()) {
            maximumCapability_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.newBuilder(maximumCapability_).mergeFrom(value).buildPartial();
          } else {
            maximumCapability_ = value;
          }
          onChanged();
        } else {
          maximumCapabilityBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
       */
      public Builder clearMaximumCapability() {
        if (maximumCapabilityBuilder_ == null) {
          maximumCapability_ = org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.getDefaultInstance();
          onChanged();
        } else {
          maximumCapabilityBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder getMaximumCapabilityBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getMaximumCapabilityFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder getMaximumCapabilityOrBuilder() {
        if (maximumCapabilityBuilder_ != null) {
          return maximumCapabilityBuilder_.getMessageOrBuilder();
        } else {
          return maximumCapability_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ResourceProto maximumCapability = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder> 
          getMaximumCapabilityFieldBuilder() {
        if (maximumCapabilityBuilder_ == null) {
          maximumCapabilityBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ResourceProtoOrBuilder>(
                  maximumCapability_,
                  getParentForChildren(),
                  isClean());
          maximumCapability_ = null;
        }
        return maximumCapabilityBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetNewApplicationResponseProto)
    }

    static {
      defaultInstance = new GetNewApplicationResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetNewApplicationResponseProto)
  }

  public interface GetApplicationReportRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ApplicationIdProto application_id = 1;
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    boolean hasApplicationId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetApplicationReportRequestProto}
   */
  public static final class GetApplicationReportRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetApplicationReportRequestProtoOrBuilder {
    // Use GetApplicationReportRequestProto.newBuilder() to construct.
    private GetApplicationReportRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetApplicationReportRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetApplicationReportRequestProto defaultInstance;
    public static GetApplicationReportRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetApplicationReportRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetApplicationReportRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = applicationId_.toBuilder();
              }
              applicationId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationId_);
                applicationId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationReportRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationReportRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetApplicationReportRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetApplicationReportRequestProto>() {
      public GetApplicationReportRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetApplicationReportRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetApplicationReportRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ApplicationIdProto application_id = 1;
    public static final int APPLICATION_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto applicationId_;
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public boolean hasApplicationId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId() {
      return applicationId_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder() {
      return applicationId_;
    }

    private void initFields() {
      applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, applicationId_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, applicationId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto) obj;

      boolean result = true;
      result = result && (hasApplicationId() == other.hasApplicationId());
      if (hasApplicationId()) {
        result = result && getApplicationId()
            .equals(other.getApplicationId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasApplicationId()) {
        hash = (37 * hash) + APPLICATION_ID_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationId().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetApplicationReportRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationReportRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationReportRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getApplicationIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (applicationIdBuilder_ == null) {
          applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
        } else {
          applicationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationReportRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (applicationIdBuilder_ == null) {
          result.applicationId_ = applicationId_;
        } else {
          result.applicationId_ = applicationIdBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto.getDefaultInstance()) return this;
        if (other.hasApplicationId()) {
          mergeApplicationId(other.getApplicationId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ApplicationIdProto application_id = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> applicationIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public boolean hasApplicationId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId() {
        if (applicationIdBuilder_ == null) {
          return applicationId_;
        } else {
          return applicationIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder setApplicationId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationId_ = value;
          onChanged();
        } else {
          applicationIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder setApplicationId(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder builderForValue) {
        if (applicationIdBuilder_ == null) {
          applicationId_ = builderForValue.build();
          onChanged();
        } else {
          applicationIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder mergeApplicationId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              applicationId_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance()) {
            applicationId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.newBuilder(applicationId_).mergeFrom(value).buildPartial();
          } else {
            applicationId_ = value;
          }
          onChanged();
        } else {
          applicationIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder clearApplicationId() {
        if (applicationIdBuilder_ == null) {
          applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
          onChanged();
        } else {
          applicationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder getApplicationIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getApplicationIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder() {
        if (applicationIdBuilder_ != null) {
          return applicationIdBuilder_.getMessageOrBuilder();
        } else {
          return applicationId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> 
          getApplicationIdFieldBuilder() {
        if (applicationIdBuilder_ == null) {
          applicationIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder>(
                  applicationId_,
                  getParentForChildren(),
                  isClean());
          applicationId_ = null;
        }
        return applicationIdBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetApplicationReportRequestProto)
    }

    static {
      defaultInstance = new GetApplicationReportRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetApplicationReportRequestProto)
  }

  public interface GetApplicationReportResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ApplicationReportProto application_report = 1;
    /**
     * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
     */
    boolean hasApplicationReport();
    /**
     * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto getApplicationReport();
    /**
     * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder getApplicationReportOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetApplicationReportResponseProto}
   */
  public static final class GetApplicationReportResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetApplicationReportResponseProtoOrBuilder {
    // Use GetApplicationReportResponseProto.newBuilder() to construct.
    private GetApplicationReportResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetApplicationReportResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetApplicationReportResponseProto defaultInstance;
    public static GetApplicationReportResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetApplicationReportResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetApplicationReportResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = applicationReport_.toBuilder();
              }
              applicationReport_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationReport_);
                applicationReport_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationReportResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationReportResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetApplicationReportResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetApplicationReportResponseProto>() {
      public GetApplicationReportResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetApplicationReportResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetApplicationReportResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ApplicationReportProto application_report = 1;
    public static final int APPLICATION_REPORT_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto applicationReport_;
    /**
     * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
     */
    public boolean hasApplicationReport() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto getApplicationReport() {
      return applicationReport_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder getApplicationReportOrBuilder() {
      return applicationReport_;
    }

    private void initFields() {
      applicationReport_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (hasApplicationReport()) {
        if (!getApplicationReport().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, applicationReport_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, applicationReport_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto) obj;

      boolean result = true;
      result = result && (hasApplicationReport() == other.hasApplicationReport());
      if (hasApplicationReport()) {
        result = result && getApplicationReport()
            .equals(other.getApplicationReport());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasApplicationReport()) {
        hash = (37 * hash) + APPLICATION_REPORT_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationReport().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetApplicationReportResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationReportResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationReportResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getApplicationReportFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (applicationReportBuilder_ == null) {
          applicationReport_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.getDefaultInstance();
        } else {
          applicationReportBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationReportResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (applicationReportBuilder_ == null) {
          result.applicationReport_ = applicationReport_;
        } else {
          result.applicationReport_ = applicationReportBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto.getDefaultInstance()) return this;
        if (other.hasApplicationReport()) {
          mergeApplicationReport(other.getApplicationReport());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (hasApplicationReport()) {
          if (!getApplicationReport().isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationReportResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ApplicationReportProto application_report = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto applicationReport_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder> applicationReportBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
       */
      public boolean hasApplicationReport() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto getApplicationReport() {
        if (applicationReportBuilder_ == null) {
          return applicationReport_;
        } else {
          return applicationReportBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
       */
      public Builder setApplicationReport(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto value) {
        if (applicationReportBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationReport_ = value;
          onChanged();
        } else {
          applicationReportBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
       */
      public Builder setApplicationReport(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder builderForValue) {
        if (applicationReportBuilder_ == null) {
          applicationReport_ = builderForValue.build();
          onChanged();
        } else {
          applicationReportBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
       */
      public Builder mergeApplicationReport(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto value) {
        if (applicationReportBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              applicationReport_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.getDefaultInstance()) {
            applicationReport_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.newBuilder(applicationReport_).mergeFrom(value).buildPartial();
          } else {
            applicationReport_ = value;
          }
          onChanged();
        } else {
          applicationReportBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
       */
      public Builder clearApplicationReport() {
        if (applicationReportBuilder_ == null) {
          applicationReport_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.getDefaultInstance();
          onChanged();
        } else {
          applicationReportBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder getApplicationReportBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getApplicationReportFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder getApplicationReportOrBuilder() {
        if (applicationReportBuilder_ != null) {
          return applicationReportBuilder_.getMessageOrBuilder();
        } else {
          return applicationReport_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationReportProto application_report = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder> 
          getApplicationReportFieldBuilder() {
        if (applicationReportBuilder_ == null) {
          applicationReportBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder>(
                  applicationReport_,
                  getParentForChildren(),
                  isClean());
          applicationReport_ = null;
        }
        return applicationReportBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetApplicationReportResponseProto)
    }

    static {
      defaultInstance = new GetApplicationReportResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetApplicationReportResponseProto)
  }

  public interface SubmitApplicationRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;
    /**
     * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
     */
    boolean hasApplicationSubmissionContext();
    /**
     * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto getApplicationSubmissionContext();
    /**
     * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProtoOrBuilder getApplicationSubmissionContextOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.SubmitApplicationRequestProto}
   */
  public static final class SubmitApplicationRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements SubmitApplicationRequestProtoOrBuilder {
    // Use SubmitApplicationRequestProto.newBuilder() to construct.
    private SubmitApplicationRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private SubmitApplicationRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final SubmitApplicationRequestProto defaultInstance;
    public static SubmitApplicationRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public SubmitApplicationRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private SubmitApplicationRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = applicationSubmissionContext_.toBuilder();
              }
              applicationSubmissionContext_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationSubmissionContext_);
                applicationSubmissionContext_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SubmitApplicationRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SubmitApplicationRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<SubmitApplicationRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<SubmitApplicationRequestProto>() {
      public SubmitApplicationRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SubmitApplicationRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<SubmitApplicationRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;
    public static final int APPLICATION_SUBMISSION_CONTEXT_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto applicationSubmissionContext_;
    /**
     * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
     */
    public boolean hasApplicationSubmissionContext() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto getApplicationSubmissionContext() {
      return applicationSubmissionContext_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProtoOrBuilder getApplicationSubmissionContextOrBuilder() {
      return applicationSubmissionContext_;
    }

    private void initFields() {
      applicationSubmissionContext_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, applicationSubmissionContext_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, applicationSubmissionContext_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto) obj;

      boolean result = true;
      result = result && (hasApplicationSubmissionContext() == other.hasApplicationSubmissionContext());
      if (hasApplicationSubmissionContext()) {
        result = result && getApplicationSubmissionContext()
            .equals(other.getApplicationSubmissionContext());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasApplicationSubmissionContext()) {
        hash = (37 * hash) + APPLICATION_SUBMISSION_CONTEXT_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationSubmissionContext().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.SubmitApplicationRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SubmitApplicationRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SubmitApplicationRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getApplicationSubmissionContextFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (applicationSubmissionContextBuilder_ == null) {
          applicationSubmissionContext_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.getDefaultInstance();
        } else {
          applicationSubmissionContextBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SubmitApplicationRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (applicationSubmissionContextBuilder_ == null) {
          result.applicationSubmissionContext_ = applicationSubmissionContext_;
        } else {
          result.applicationSubmissionContext_ = applicationSubmissionContextBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto.getDefaultInstance()) return this;
        if (other.hasApplicationSubmissionContext()) {
          mergeApplicationSubmissionContext(other.getApplicationSubmissionContext());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto applicationSubmissionContext_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProtoOrBuilder> applicationSubmissionContextBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
       */
      public boolean hasApplicationSubmissionContext() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto getApplicationSubmissionContext() {
        if (applicationSubmissionContextBuilder_ == null) {
          return applicationSubmissionContext_;
        } else {
          return applicationSubmissionContextBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
       */
      public Builder setApplicationSubmissionContext(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto value) {
        if (applicationSubmissionContextBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationSubmissionContext_ = value;
          onChanged();
        } else {
          applicationSubmissionContextBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
       */
      public Builder setApplicationSubmissionContext(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.Builder builderForValue) {
        if (applicationSubmissionContextBuilder_ == null) {
          applicationSubmissionContext_ = builderForValue.build();
          onChanged();
        } else {
          applicationSubmissionContextBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
       */
      public Builder mergeApplicationSubmissionContext(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto value) {
        if (applicationSubmissionContextBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              applicationSubmissionContext_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.getDefaultInstance()) {
            applicationSubmissionContext_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.newBuilder(applicationSubmissionContext_).mergeFrom(value).buildPartial();
          } else {
            applicationSubmissionContext_ = value;
          }
          onChanged();
        } else {
          applicationSubmissionContextBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
       */
      public Builder clearApplicationSubmissionContext() {
        if (applicationSubmissionContextBuilder_ == null) {
          applicationSubmissionContext_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.getDefaultInstance();
          onChanged();
        } else {
          applicationSubmissionContextBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.Builder getApplicationSubmissionContextBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getApplicationSubmissionContextFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProtoOrBuilder getApplicationSubmissionContextOrBuilder() {
        if (applicationSubmissionContextBuilder_ != null) {
          return applicationSubmissionContextBuilder_.getMessageOrBuilder();
        } else {
          return applicationSubmissionContext_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationSubmissionContextProto application_submission_context = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProtoOrBuilder> 
          getApplicationSubmissionContextFieldBuilder() {
        if (applicationSubmissionContextBuilder_ == null) {
          applicationSubmissionContextBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationSubmissionContextProtoOrBuilder>(
                  applicationSubmissionContext_,
                  getParentForChildren(),
                  isClean());
          applicationSubmissionContext_ = null;
        }
        return applicationSubmissionContextBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.SubmitApplicationRequestProto)
    }

    static {
      defaultInstance = new SubmitApplicationRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.SubmitApplicationRequestProto)
  }

  public interface SubmitApplicationResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.yarn.SubmitApplicationResponseProto}
   */
  public static final class SubmitApplicationResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements SubmitApplicationResponseProtoOrBuilder {
    // Use SubmitApplicationResponseProto.newBuilder() to construct.
    private SubmitApplicationResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private SubmitApplicationResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final SubmitApplicationResponseProto defaultInstance;
    public static SubmitApplicationResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public SubmitApplicationResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private SubmitApplicationResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SubmitApplicationResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SubmitApplicationResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<SubmitApplicationResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<SubmitApplicationResponseProto>() {
      public SubmitApplicationResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SubmitApplicationResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<SubmitApplicationResponseProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.SubmitApplicationResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SubmitApplicationResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SubmitApplicationResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SubmitApplicationResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.SubmitApplicationResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.SubmitApplicationResponseProto)
    }

    static {
      defaultInstance = new SubmitApplicationResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.SubmitApplicationResponseProto)
  }

  public interface FailApplicationAttemptRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
     */
    boolean hasApplicationAttemptId();
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto getApplicationAttemptId();
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder getApplicationAttemptIdOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.FailApplicationAttemptRequestProto}
   */
  public static final class FailApplicationAttemptRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements FailApplicationAttemptRequestProtoOrBuilder {
    // Use FailApplicationAttemptRequestProto.newBuilder() to construct.
    private FailApplicationAttemptRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private FailApplicationAttemptRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final FailApplicationAttemptRequestProto defaultInstance;
    public static FailApplicationAttemptRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public FailApplicationAttemptRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private FailApplicationAttemptRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = applicationAttemptId_.toBuilder();
              }
              applicationAttemptId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationAttemptId_);
                applicationAttemptId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FailApplicationAttemptRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FailApplicationAttemptRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<FailApplicationAttemptRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<FailApplicationAttemptRequestProto>() {
      public FailApplicationAttemptRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new FailApplicationAttemptRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<FailApplicationAttemptRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;
    public static final int APPLICATION_ATTEMPT_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto applicationAttemptId_;
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
     */
    public boolean hasApplicationAttemptId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto getApplicationAttemptId() {
      return applicationAttemptId_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder getApplicationAttemptIdOrBuilder() {
      return applicationAttemptId_;
    }

    private void initFields() {
      applicationAttemptId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, applicationAttemptId_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, applicationAttemptId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto) obj;

      boolean result = true;
      result = result && (hasApplicationAttemptId() == other.hasApplicationAttemptId());
      if (hasApplicationAttemptId()) {
        result = result && getApplicationAttemptId()
            .equals(other.getApplicationAttemptId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasApplicationAttemptId()) {
        hash = (37 * hash) + APPLICATION_ATTEMPT_ID_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationAttemptId().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.FailApplicationAttemptRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FailApplicationAttemptRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FailApplicationAttemptRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getApplicationAttemptIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (applicationAttemptIdBuilder_ == null) {
          applicationAttemptId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance();
        } else {
          applicationAttemptIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FailApplicationAttemptRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (applicationAttemptIdBuilder_ == null) {
          result.applicationAttemptId_ = applicationAttemptId_;
        } else {
          result.applicationAttemptId_ = applicationAttemptIdBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto.getDefaultInstance()) return this;
        if (other.hasApplicationAttemptId()) {
          mergeApplicationAttemptId(other.getApplicationAttemptId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto applicationAttemptId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder> applicationAttemptIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public boolean hasApplicationAttemptId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto getApplicationAttemptId() {
        if (applicationAttemptIdBuilder_ == null) {
          return applicationAttemptId_;
        } else {
          return applicationAttemptIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public Builder setApplicationAttemptId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto value) {
        if (applicationAttemptIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationAttemptId_ = value;
          onChanged();
        } else {
          applicationAttemptIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public Builder setApplicationAttemptId(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder builderForValue) {
        if (applicationAttemptIdBuilder_ == null) {
          applicationAttemptId_ = builderForValue.build();
          onChanged();
        } else {
          applicationAttemptIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public Builder mergeApplicationAttemptId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto value) {
        if (applicationAttemptIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              applicationAttemptId_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance()) {
            applicationAttemptId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.newBuilder(applicationAttemptId_).mergeFrom(value).buildPartial();
          } else {
            applicationAttemptId_ = value;
          }
          onChanged();
        } else {
          applicationAttemptIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public Builder clearApplicationAttemptId() {
        if (applicationAttemptIdBuilder_ == null) {
          applicationAttemptId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance();
          onChanged();
        } else {
          applicationAttemptIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder getApplicationAttemptIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getApplicationAttemptIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder getApplicationAttemptIdOrBuilder() {
        if (applicationAttemptIdBuilder_ != null) {
          return applicationAttemptIdBuilder_.getMessageOrBuilder();
        } else {
          return applicationAttemptId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder> 
          getApplicationAttemptIdFieldBuilder() {
        if (applicationAttemptIdBuilder_ == null) {
          applicationAttemptIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder>(
                  applicationAttemptId_,
                  getParentForChildren(),
                  isClean());
          applicationAttemptId_ = null;
        }
        return applicationAttemptIdBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.FailApplicationAttemptRequestProto)
    }

    static {
      defaultInstance = new FailApplicationAttemptRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.FailApplicationAttemptRequestProto)
  }

  public interface FailApplicationAttemptResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.yarn.FailApplicationAttemptResponseProto}
   */
  public static final class FailApplicationAttemptResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements FailApplicationAttemptResponseProtoOrBuilder {
    // Use FailApplicationAttemptResponseProto.newBuilder() to construct.
    private FailApplicationAttemptResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private FailApplicationAttemptResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final FailApplicationAttemptResponseProto defaultInstance;
    public static FailApplicationAttemptResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public FailApplicationAttemptResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private FailApplicationAttemptResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FailApplicationAttemptResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FailApplicationAttemptResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<FailApplicationAttemptResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<FailApplicationAttemptResponseProto>() {
      public FailApplicationAttemptResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new FailApplicationAttemptResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<FailApplicationAttemptResponseProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.FailApplicationAttemptResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FailApplicationAttemptResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FailApplicationAttemptResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_FailApplicationAttemptResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.FailApplicationAttemptResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.FailApplicationAttemptResponseProto)
    }

    static {
      defaultInstance = new FailApplicationAttemptResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.FailApplicationAttemptResponseProto)
  }

  public interface KillApplicationRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ApplicationIdProto application_id = 1;
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    boolean hasApplicationId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder();

    // optional string diagnostics = 2;
    /**
     * <code>optional string diagnostics = 2;</code>
     */
    boolean hasDiagnostics();
    /**
     * <code>optional string diagnostics = 2;</code>
     */
    java.lang.String getDiagnostics();
    /**
     * <code>optional string diagnostics = 2;</code>
     */
    com.google.protobuf.ByteString
        getDiagnosticsBytes();
  }
  /**
   * Protobuf type {@code hadoop.yarn.KillApplicationRequestProto}
   */
  public static final class KillApplicationRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements KillApplicationRequestProtoOrBuilder {
    // Use KillApplicationRequestProto.newBuilder() to construct.
    private KillApplicationRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private KillApplicationRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final KillApplicationRequestProto defaultInstance;
    public static KillApplicationRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public KillApplicationRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private KillApplicationRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = applicationId_.toBuilder();
              }
              applicationId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationId_);
                applicationId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              diagnostics_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_KillApplicationRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_KillApplicationRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<KillApplicationRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<KillApplicationRequestProto>() {
      public KillApplicationRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new KillApplicationRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<KillApplicationRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ApplicationIdProto application_id = 1;
    public static final int APPLICATION_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto applicationId_;
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public boolean hasApplicationId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId() {
      return applicationId_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder() {
      return applicationId_;
    }

    // optional string diagnostics = 2;
    public static final int DIAGNOSTICS_FIELD_NUMBER = 2;
    private java.lang.Object diagnostics_;
    /**
     * <code>optional string diagnostics = 2;</code>
     */
    public boolean hasDiagnostics() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional string diagnostics = 2;</code>
     */
    public java.lang.String getDiagnostics() {
      java.lang.Object ref = diagnostics_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          diagnostics_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string diagnostics = 2;</code>
     */
    public com.google.protobuf.ByteString
        getDiagnosticsBytes() {
      java.lang.Object ref = diagnostics_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        diagnostics_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private void initFields() {
      applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
      diagnostics_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, applicationId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, getDiagnosticsBytes());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, applicationId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, getDiagnosticsBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto) obj;

      boolean result = true;
      result = result && (hasApplicationId() == other.hasApplicationId());
      if (hasApplicationId()) {
        result = result && getApplicationId()
            .equals(other.getApplicationId());
      }
      result = result && (hasDiagnostics() == other.hasDiagnostics());
      if (hasDiagnostics()) {
        result = result && getDiagnostics()
            .equals(other.getDiagnostics());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasApplicationId()) {
        hash = (37 * hash) + APPLICATION_ID_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationId().hashCode();
      }
      if (hasDiagnostics()) {
        hash = (37 * hash) + DIAGNOSTICS_FIELD_NUMBER;
        hash = (53 * hash) + getDiagnostics().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.KillApplicationRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_KillApplicationRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_KillApplicationRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getApplicationIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (applicationIdBuilder_ == null) {
          applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
        } else {
          applicationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        diagnostics_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_KillApplicationRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (applicationIdBuilder_ == null) {
          result.applicationId_ = applicationId_;
        } else {
          result.applicationId_ = applicationIdBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.diagnostics_ = diagnostics_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto.getDefaultInstance()) return this;
        if (other.hasApplicationId()) {
          mergeApplicationId(other.getApplicationId());
        }
        if (other.hasDiagnostics()) {
          bitField0_ |= 0x00000002;
          diagnostics_ = other.diagnostics_;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ApplicationIdProto application_id = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> applicationIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public boolean hasApplicationId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId() {
        if (applicationIdBuilder_ == null) {
          return applicationId_;
        } else {
          return applicationIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder setApplicationId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationId_ = value;
          onChanged();
        } else {
          applicationIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder setApplicationId(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder builderForValue) {
        if (applicationIdBuilder_ == null) {
          applicationId_ = builderForValue.build();
          onChanged();
        } else {
          applicationIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder mergeApplicationId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              applicationId_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance()) {
            applicationId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.newBuilder(applicationId_).mergeFrom(value).buildPartial();
          } else {
            applicationId_ = value;
          }
          onChanged();
        } else {
          applicationIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder clearApplicationId() {
        if (applicationIdBuilder_ == null) {
          applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
          onChanged();
        } else {
          applicationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder getApplicationIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getApplicationIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder() {
        if (applicationIdBuilder_ != null) {
          return applicationIdBuilder_.getMessageOrBuilder();
        } else {
          return applicationId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> 
          getApplicationIdFieldBuilder() {
        if (applicationIdBuilder_ == null) {
          applicationIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder>(
                  applicationId_,
                  getParentForChildren(),
                  isClean());
          applicationId_ = null;
        }
        return applicationIdBuilder_;
      }

      // optional string diagnostics = 2;
      private java.lang.Object diagnostics_ = "";
      /**
       * <code>optional string diagnostics = 2;</code>
       */
      public boolean hasDiagnostics() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional string diagnostics = 2;</code>
       */
      public java.lang.String getDiagnostics() {
        java.lang.Object ref = diagnostics_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          diagnostics_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string diagnostics = 2;</code>
       */
      public com.google.protobuf.ByteString
          getDiagnosticsBytes() {
        java.lang.Object ref = diagnostics_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          diagnostics_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string diagnostics = 2;</code>
       */
      public Builder setDiagnostics(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        diagnostics_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string diagnostics = 2;</code>
       */
      public Builder clearDiagnostics() {
        bitField0_ = (bitField0_ & ~0x00000002);
        diagnostics_ = getDefaultInstance().getDiagnostics();
        onChanged();
        return this;
      }
      /**
       * <code>optional string diagnostics = 2;</code>
       */
      public Builder setDiagnosticsBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        diagnostics_ = value;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.KillApplicationRequestProto)
    }

    static {
      defaultInstance = new KillApplicationRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.KillApplicationRequestProto)
  }

  public interface KillApplicationResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional bool is_kill_completed = 1 [default = false];
    /**
     * <code>optional bool is_kill_completed = 1 [default = false];</code>
     */
    boolean hasIsKillCompleted();
    /**
     * <code>optional bool is_kill_completed = 1 [default = false];</code>
     */
    boolean getIsKillCompleted();
  }
  /**
   * Protobuf type {@code hadoop.yarn.KillApplicationResponseProto}
   */
  public static final class KillApplicationResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements KillApplicationResponseProtoOrBuilder {
    // Use KillApplicationResponseProto.newBuilder() to construct.
    private KillApplicationResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private KillApplicationResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final KillApplicationResponseProto defaultInstance;
    public static KillApplicationResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public KillApplicationResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private KillApplicationResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              isKillCompleted_ = input.readBool();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_KillApplicationResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_KillApplicationResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<KillApplicationResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<KillApplicationResponseProto>() {
      public KillApplicationResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new KillApplicationResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<KillApplicationResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional bool is_kill_completed = 1 [default = false];
    public static final int IS_KILL_COMPLETED_FIELD_NUMBER = 1;
    private boolean isKillCompleted_;
    /**
     * <code>optional bool is_kill_completed = 1 [default = false];</code>
     */
    public boolean hasIsKillCompleted() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional bool is_kill_completed = 1 [default = false];</code>
     */
    public boolean getIsKillCompleted() {
      return isKillCompleted_;
    }

    private void initFields() {
      isKillCompleted_ = false;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBool(1, isKillCompleted_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(1, isKillCompleted_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto) obj;

      boolean result = true;
      result = result && (hasIsKillCompleted() == other.hasIsKillCompleted());
      if (hasIsKillCompleted()) {
        result = result && (getIsKillCompleted()
            == other.getIsKillCompleted());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasIsKillCompleted()) {
        hash = (37 * hash) + IS_KILL_COMPLETED_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getIsKillCompleted());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.KillApplicationResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_KillApplicationResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_KillApplicationResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        isKillCompleted_ = false;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_KillApplicationResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.isKillCompleted_ = isKillCompleted_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto.getDefaultInstance()) return this;
        if (other.hasIsKillCompleted()) {
          setIsKillCompleted(other.getIsKillCompleted());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.KillApplicationResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional bool is_kill_completed = 1 [default = false];
      private boolean isKillCompleted_ ;
      /**
       * <code>optional bool is_kill_completed = 1 [default = false];</code>
       */
      public boolean hasIsKillCompleted() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional bool is_kill_completed = 1 [default = false];</code>
       */
      public boolean getIsKillCompleted() {
        return isKillCompleted_;
      }
      /**
       * <code>optional bool is_kill_completed = 1 [default = false];</code>
       */
      public Builder setIsKillCompleted(boolean value) {
        bitField0_ |= 0x00000001;
        isKillCompleted_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool is_kill_completed = 1 [default = false];</code>
       */
      public Builder clearIsKillCompleted() {
        bitField0_ = (bitField0_ & ~0x00000001);
        isKillCompleted_ = false;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.KillApplicationResponseProto)
    }

    static {
      defaultInstance = new KillApplicationResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.KillApplicationResponseProto)
  }

  public interface GetClusterMetricsRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetClusterMetricsRequestProto}
   */
  public static final class GetClusterMetricsRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetClusterMetricsRequestProtoOrBuilder {
    // Use GetClusterMetricsRequestProto.newBuilder() to construct.
    private GetClusterMetricsRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetClusterMetricsRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetClusterMetricsRequestProto defaultInstance;
    public static GetClusterMetricsRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetClusterMetricsRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetClusterMetricsRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterMetricsRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterMetricsRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetClusterMetricsRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetClusterMetricsRequestProto>() {
      public GetClusterMetricsRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetClusterMetricsRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetClusterMetricsRequestProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetClusterMetricsRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterMetricsRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterMetricsRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterMetricsRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetClusterMetricsRequestProto)
    }

    static {
      defaultInstance = new GetClusterMetricsRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetClusterMetricsRequestProto)
  }

  public interface GetClusterMetricsResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;
    /**
     * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
     */
    boolean hasClusterMetrics();
    /**
     * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto getClusterMetrics();
    /**
     * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProtoOrBuilder getClusterMetricsOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetClusterMetricsResponseProto}
   */
  public static final class GetClusterMetricsResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetClusterMetricsResponseProtoOrBuilder {
    // Use GetClusterMetricsResponseProto.newBuilder() to construct.
    private GetClusterMetricsResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetClusterMetricsResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetClusterMetricsResponseProto defaultInstance;
    public static GetClusterMetricsResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetClusterMetricsResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetClusterMetricsResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = clusterMetrics_.toBuilder();
              }
              clusterMetrics_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(clusterMetrics_);
                clusterMetrics_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterMetricsResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterMetricsResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetClusterMetricsResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetClusterMetricsResponseProto>() {
      public GetClusterMetricsResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetClusterMetricsResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetClusterMetricsResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;
    public static final int CLUSTER_METRICS_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto clusterMetrics_;
    /**
     * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
     */
    public boolean hasClusterMetrics() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto getClusterMetrics() {
      return clusterMetrics_;
    }
    /**
     * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProtoOrBuilder getClusterMetricsOrBuilder() {
      return clusterMetrics_;
    }

    private void initFields() {
      clusterMetrics_ = org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, clusterMetrics_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, clusterMetrics_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto) obj;

      boolean result = true;
      result = result && (hasClusterMetrics() == other.hasClusterMetrics());
      if (hasClusterMetrics()) {
        result = result && getClusterMetrics()
            .equals(other.getClusterMetrics());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasClusterMetrics()) {
        hash = (37 * hash) + CLUSTER_METRICS_FIELD_NUMBER;
        hash = (53 * hash) + getClusterMetrics().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetClusterMetricsResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterMetricsResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterMetricsResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getClusterMetricsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (clusterMetricsBuilder_ == null) {
          clusterMetrics_ = org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.getDefaultInstance();
        } else {
          clusterMetricsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterMetricsResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (clusterMetricsBuilder_ == null) {
          result.clusterMetrics_ = clusterMetrics_;
        } else {
          result.clusterMetrics_ = clusterMetricsBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto.getDefaultInstance()) return this;
        if (other.hasClusterMetrics()) {
          mergeClusterMetrics(other.getClusterMetrics());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterMetricsResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto clusterMetrics_ = org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto, org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProtoOrBuilder> clusterMetricsBuilder_;
      /**
       * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
       */
      public boolean hasClusterMetrics() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto getClusterMetrics() {
        if (clusterMetricsBuilder_ == null) {
          return clusterMetrics_;
        } else {
          return clusterMetricsBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
       */
      public Builder setClusterMetrics(org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto value) {
        if (clusterMetricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          clusterMetrics_ = value;
          onChanged();
        } else {
          clusterMetricsBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
       */
      public Builder setClusterMetrics(
          org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.Builder builderForValue) {
        if (clusterMetricsBuilder_ == null) {
          clusterMetrics_ = builderForValue.build();
          onChanged();
        } else {
          clusterMetricsBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
       */
      public Builder mergeClusterMetrics(org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto value) {
        if (clusterMetricsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              clusterMetrics_ != org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.getDefaultInstance()) {
            clusterMetrics_ =
              org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.newBuilder(clusterMetrics_).mergeFrom(value).buildPartial();
          } else {
            clusterMetrics_ = value;
          }
          onChanged();
        } else {
          clusterMetricsBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
       */
      public Builder clearClusterMetrics() {
        if (clusterMetricsBuilder_ == null) {
          clusterMetrics_ = org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.getDefaultInstance();
          onChanged();
        } else {
          clusterMetricsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.Builder getClusterMetricsBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getClusterMetricsFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProtoOrBuilder getClusterMetricsOrBuilder() {
        if (clusterMetricsBuilder_ != null) {
          return clusterMetricsBuilder_.getMessageOrBuilder();
        } else {
          return clusterMetrics_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.YarnClusterMetricsProto cluster_metrics = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto, org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProtoOrBuilder> 
          getClusterMetricsFieldBuilder() {
        if (clusterMetricsBuilder_ == null) {
          clusterMetricsBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto, org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.YarnClusterMetricsProtoOrBuilder>(
                  clusterMetrics_,
                  getParentForChildren(),
                  isClean());
          clusterMetrics_ = null;
        }
        return clusterMetricsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetClusterMetricsResponseProto)
    }

    static {
      defaultInstance = new GetClusterMetricsResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetClusterMetricsResponseProto)
  }

  public interface MoveApplicationAcrossQueuesRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required .hadoop.yarn.ApplicationIdProto application_id = 1;
    /**
     * <code>required .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    boolean hasApplicationId();
    /**
     * <code>required .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId();
    /**
     * <code>required .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder();

    // required string target_queue = 2;
    /**
     * <code>required string target_queue = 2;</code>
     */
    boolean hasTargetQueue();
    /**
     * <code>required string target_queue = 2;</code>
     */
    java.lang.String getTargetQueue();
    /**
     * <code>required string target_queue = 2;</code>
     */
    com.google.protobuf.ByteString
        getTargetQueueBytes();
  }
  /**
   * Protobuf type {@code hadoop.yarn.MoveApplicationAcrossQueuesRequestProto}
   */
  public static final class MoveApplicationAcrossQueuesRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements MoveApplicationAcrossQueuesRequestProtoOrBuilder {
    // Use MoveApplicationAcrossQueuesRequestProto.newBuilder() to construct.
    private MoveApplicationAcrossQueuesRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private MoveApplicationAcrossQueuesRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final MoveApplicationAcrossQueuesRequestProto defaultInstance;
    public static MoveApplicationAcrossQueuesRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public MoveApplicationAcrossQueuesRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private MoveApplicationAcrossQueuesRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = applicationId_.toBuilder();
              }
              applicationId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationId_);
                applicationId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              targetQueue_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_MoveApplicationAcrossQueuesRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_MoveApplicationAcrossQueuesRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<MoveApplicationAcrossQueuesRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<MoveApplicationAcrossQueuesRequestProto>() {
      public MoveApplicationAcrossQueuesRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new MoveApplicationAcrossQueuesRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<MoveApplicationAcrossQueuesRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required .hadoop.yarn.ApplicationIdProto application_id = 1;
    public static final int APPLICATION_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto applicationId_;
    /**
     * <code>required .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public boolean hasApplicationId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId() {
      return applicationId_;
    }
    /**
     * <code>required .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder() {
      return applicationId_;
    }

    // required string target_queue = 2;
    public static final int TARGET_QUEUE_FIELD_NUMBER = 2;
    private java.lang.Object targetQueue_;
    /**
     * <code>required string target_queue = 2;</code>
     */
    public boolean hasTargetQueue() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required string target_queue = 2;</code>
     */
    public java.lang.String getTargetQueue() {
      java.lang.Object ref = targetQueue_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          targetQueue_ = s;
        }
        return s;
      }
    }
    /**
     * <code>required string target_queue = 2;</code>
     */
    public com.google.protobuf.ByteString
        getTargetQueueBytes() {
      java.lang.Object ref = targetQueue_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        targetQueue_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private void initFields() {
      applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
      targetQueue_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasApplicationId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasTargetQueue()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, applicationId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, getTargetQueueBytes());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, applicationId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, getTargetQueueBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto) obj;

      boolean result = true;
      result = result && (hasApplicationId() == other.hasApplicationId());
      if (hasApplicationId()) {
        result = result && getApplicationId()
            .equals(other.getApplicationId());
      }
      result = result && (hasTargetQueue() == other.hasTargetQueue());
      if (hasTargetQueue()) {
        result = result && getTargetQueue()
            .equals(other.getTargetQueue());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasApplicationId()) {
        hash = (37 * hash) + APPLICATION_ID_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationId().hashCode();
      }
      if (hasTargetQueue()) {
        hash = (37 * hash) + TARGET_QUEUE_FIELD_NUMBER;
        hash = (53 * hash) + getTargetQueue().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.MoveApplicationAcrossQueuesRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_MoveApplicationAcrossQueuesRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_MoveApplicationAcrossQueuesRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getApplicationIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (applicationIdBuilder_ == null) {
          applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
        } else {
          applicationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        targetQueue_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_MoveApplicationAcrossQueuesRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (applicationIdBuilder_ == null) {
          result.applicationId_ = applicationId_;
        } else {
          result.applicationId_ = applicationIdBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.targetQueue_ = targetQueue_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto.getDefaultInstance()) return this;
        if (other.hasApplicationId()) {
          mergeApplicationId(other.getApplicationId());
        }
        if (other.hasTargetQueue()) {
          bitField0_ |= 0x00000002;
          targetQueue_ = other.targetQueue_;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasApplicationId()) {
          
          return false;
        }
        if (!hasTargetQueue()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required .hadoop.yarn.ApplicationIdProto application_id = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> applicationIdBuilder_;
      /**
       * <code>required .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public boolean hasApplicationId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId() {
        if (applicationIdBuilder_ == null) {
          return applicationId_;
        } else {
          return applicationIdBuilder_.getMessage();
        }
      }
      /**
       * <code>required .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder setApplicationId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationId_ = value;
          onChanged();
        } else {
          applicationIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder setApplicationId(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder builderForValue) {
        if (applicationIdBuilder_ == null) {
          applicationId_ = builderForValue.build();
          onChanged();
        } else {
          applicationIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder mergeApplicationId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              applicationId_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance()) {
            applicationId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.newBuilder(applicationId_).mergeFrom(value).buildPartial();
          } else {
            applicationId_ = value;
          }
          onChanged();
        } else {
          applicationIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder clearApplicationId() {
        if (applicationIdBuilder_ == null) {
          applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
          onChanged();
        } else {
          applicationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>required .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder getApplicationIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getApplicationIdFieldBuilder().getBuilder();
      }
      /**
       * <code>required .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder() {
        if (applicationIdBuilder_ != null) {
          return applicationIdBuilder_.getMessageOrBuilder();
        } else {
          return applicationId_;
        }
      }
      /**
       * <code>required .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> 
          getApplicationIdFieldBuilder() {
        if (applicationIdBuilder_ == null) {
          applicationIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder>(
                  applicationId_,
                  getParentForChildren(),
                  isClean());
          applicationId_ = null;
        }
        return applicationIdBuilder_;
      }

      // required string target_queue = 2;
      private java.lang.Object targetQueue_ = "";
      /**
       * <code>required string target_queue = 2;</code>
       */
      public boolean hasTargetQueue() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required string target_queue = 2;</code>
       */
      public java.lang.String getTargetQueue() {
        java.lang.Object ref = targetQueue_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          targetQueue_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>required string target_queue = 2;</code>
       */
      public com.google.protobuf.ByteString
          getTargetQueueBytes() {
        java.lang.Object ref = targetQueue_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          targetQueue_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>required string target_queue = 2;</code>
       */
      public Builder setTargetQueue(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        targetQueue_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required string target_queue = 2;</code>
       */
      public Builder clearTargetQueue() {
        bitField0_ = (bitField0_ & ~0x00000002);
        targetQueue_ = getDefaultInstance().getTargetQueue();
        onChanged();
        return this;
      }
      /**
       * <code>required string target_queue = 2;</code>
       */
      public Builder setTargetQueueBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        targetQueue_ = value;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.MoveApplicationAcrossQueuesRequestProto)
    }

    static {
      defaultInstance = new MoveApplicationAcrossQueuesRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.MoveApplicationAcrossQueuesRequestProto)
  }

  public interface MoveApplicationAcrossQueuesResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.yarn.MoveApplicationAcrossQueuesResponseProto}
   */
  public static final class MoveApplicationAcrossQueuesResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements MoveApplicationAcrossQueuesResponseProtoOrBuilder {
    // Use MoveApplicationAcrossQueuesResponseProto.newBuilder() to construct.
    private MoveApplicationAcrossQueuesResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private MoveApplicationAcrossQueuesResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final MoveApplicationAcrossQueuesResponseProto defaultInstance;
    public static MoveApplicationAcrossQueuesResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public MoveApplicationAcrossQueuesResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private MoveApplicationAcrossQueuesResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_MoveApplicationAcrossQueuesResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_MoveApplicationAcrossQueuesResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<MoveApplicationAcrossQueuesResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<MoveApplicationAcrossQueuesResponseProto>() {
      public MoveApplicationAcrossQueuesResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new MoveApplicationAcrossQueuesResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<MoveApplicationAcrossQueuesResponseProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.MoveApplicationAcrossQueuesResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_MoveApplicationAcrossQueuesResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_MoveApplicationAcrossQueuesResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_MoveApplicationAcrossQueuesResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.MoveApplicationAcrossQueuesResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.MoveApplicationAcrossQueuesResponseProto)
    }

    static {
      defaultInstance = new MoveApplicationAcrossQueuesResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.MoveApplicationAcrossQueuesResponseProto)
  }

  public interface GetApplicationsRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated string application_types = 1;
    /**
     * <code>repeated string application_types = 1;</code>
     */
    java.util.List<java.lang.String>
    getApplicationTypesList();
    /**
     * <code>repeated string application_types = 1;</code>
     */
    int getApplicationTypesCount();
    /**
     * <code>repeated string application_types = 1;</code>
     */
    java.lang.String getApplicationTypes(int index);
    /**
     * <code>repeated string application_types = 1;</code>
     */
    com.google.protobuf.ByteString
        getApplicationTypesBytes(int index);

    // repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;
    /**
     * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto> getApplicationStatesList();
    /**
     * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
     */
    int getApplicationStatesCount();
    /**
     * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto getApplicationStates(int index);

    // repeated string users = 3;
    /**
     * <code>repeated string users = 3;</code>
     */
    java.util.List<java.lang.String>
    getUsersList();
    /**
     * <code>repeated string users = 3;</code>
     */
    int getUsersCount();
    /**
     * <code>repeated string users = 3;</code>
     */
    java.lang.String getUsers(int index);
    /**
     * <code>repeated string users = 3;</code>
     */
    com.google.protobuf.ByteString
        getUsersBytes(int index);

    // repeated string queues = 4;
    /**
     * <code>repeated string queues = 4;</code>
     */
    java.util.List<java.lang.String>
    getQueuesList();
    /**
     * <code>repeated string queues = 4;</code>
     */
    int getQueuesCount();
    /**
     * <code>repeated string queues = 4;</code>
     */
    java.lang.String getQueues(int index);
    /**
     * <code>repeated string queues = 4;</code>
     */
    com.google.protobuf.ByteString
        getQueuesBytes(int index);

    // optional int64 limit = 5;
    /**
     * <code>optional int64 limit = 5;</code>
     */
    boolean hasLimit();
    /**
     * <code>optional int64 limit = 5;</code>
     */
    long getLimit();

    // optional int64 start_begin = 6;
    /**
     * <code>optional int64 start_begin = 6;</code>
     */
    boolean hasStartBegin();
    /**
     * <code>optional int64 start_begin = 6;</code>
     */
    long getStartBegin();

    // optional int64 start_end = 7;
    /**
     * <code>optional int64 start_end = 7;</code>
     */
    boolean hasStartEnd();
    /**
     * <code>optional int64 start_end = 7;</code>
     */
    long getStartEnd();

    // optional int64 finish_begin = 8;
    /**
     * <code>optional int64 finish_begin = 8;</code>
     */
    boolean hasFinishBegin();
    /**
     * <code>optional int64 finish_begin = 8;</code>
     */
    long getFinishBegin();

    // optional int64 finish_end = 9;
    /**
     * <code>optional int64 finish_end = 9;</code>
     */
    boolean hasFinishEnd();
    /**
     * <code>optional int64 finish_end = 9;</code>
     */
    long getFinishEnd();

    // repeated string applicationTags = 10;
    /**
     * <code>repeated string applicationTags = 10;</code>
     */
    java.util.List<java.lang.String>
    getApplicationTagsList();
    /**
     * <code>repeated string applicationTags = 10;</code>
     */
    int getApplicationTagsCount();
    /**
     * <code>repeated string applicationTags = 10;</code>
     */
    java.lang.String getApplicationTags(int index);
    /**
     * <code>repeated string applicationTags = 10;</code>
     */
    com.google.protobuf.ByteString
        getApplicationTagsBytes(int index);

    // optional .hadoop.yarn.ApplicationsRequestScopeProto scope = 11 [default = ALL];
    /**
     * <code>optional .hadoop.yarn.ApplicationsRequestScopeProto scope = 11 [default = ALL];</code>
     */
    boolean hasScope();
    /**
     * <code>optional .hadoop.yarn.ApplicationsRequestScopeProto scope = 11 [default = ALL];</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.ApplicationsRequestScopeProto getScope();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetApplicationsRequestProto}
   */
  public static final class GetApplicationsRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetApplicationsRequestProtoOrBuilder {
    // Use GetApplicationsRequestProto.newBuilder() to construct.
    private GetApplicationsRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetApplicationsRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetApplicationsRequestProto defaultInstance;
    public static GetApplicationsRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetApplicationsRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetApplicationsRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                applicationTypes_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000001;
              }
              applicationTypes_.add(input.readBytes());
              break;
            }
            case 16: {
              int rawValue = input.readEnum();
              org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto value = org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(2, rawValue);
              } else {
                if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                  applicationStates_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto>();
                  mutable_bitField0_ |= 0x00000002;
                }
                applicationStates_.add(value);
              }
              break;
            }
            case 18: {
              int length = input.readRawVarint32();
              int oldLimit = input.pushLimit(length);
              while(input.getBytesUntilLimit() > 0) {
                int rawValue = input.readEnum();
                org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto value = org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto.valueOf(rawValue);
                if (value == null) {
                  unknownFields.mergeVarintField(2, rawValue);
                } else {
                  if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                    applicationStates_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto>();
                    mutable_bitField0_ |= 0x00000002;
                  }
                  applicationStates_.add(value);
                }
              }
              input.popLimit(oldLimit);
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
                users_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000004;
              }
              users_.add(input.readBytes());
              break;
            }
            case 34: {
              if (!((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
                queues_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000008;
              }
              queues_.add(input.readBytes());
              break;
            }
            case 40: {
              bitField0_ |= 0x00000001;
              limit_ = input.readInt64();
              break;
            }
            case 48: {
              bitField0_ |= 0x00000002;
              startBegin_ = input.readInt64();
              break;
            }
            case 56: {
              bitField0_ |= 0x00000004;
              startEnd_ = input.readInt64();
              break;
            }
            case 64: {
              bitField0_ |= 0x00000008;
              finishBegin_ = input.readInt64();
              break;
            }
            case 72: {
              bitField0_ |= 0x00000010;
              finishEnd_ = input.readInt64();
              break;
            }
            case 82: {
              if (!((mutable_bitField0_ & 0x00000200) == 0x00000200)) {
                applicationTags_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000200;
              }
              applicationTags_.add(input.readBytes());
              break;
            }
            case 88: {
              int rawValue = input.readEnum();
              org.apache.hadoop.yarn.proto.YarnServiceProtos.ApplicationsRequestScopeProto value = org.apache.hadoop.yarn.proto.YarnServiceProtos.ApplicationsRequestScopeProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(11, rawValue);
              } else {
                bitField0_ |= 0x00000020;
                scope_ = value;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          applicationTypes_ = new com.google.protobuf.UnmodifiableLazyStringList(applicationTypes_);
        }
        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
          applicationStates_ = java.util.Collections.unmodifiableList(applicationStates_);
        }
        if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
          users_ = new com.google.protobuf.UnmodifiableLazyStringList(users_);
        }
        if (((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
          queues_ = new com.google.protobuf.UnmodifiableLazyStringList(queues_);
        }
        if (((mutable_bitField0_ & 0x00000200) == 0x00000200)) {
          applicationTags_ = new com.google.protobuf.UnmodifiableLazyStringList(applicationTags_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationsRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationsRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetApplicationsRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetApplicationsRequestProto>() {
      public GetApplicationsRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetApplicationsRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetApplicationsRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // repeated string application_types = 1;
    public static final int APPLICATION_TYPES_FIELD_NUMBER = 1;
    private com.google.protobuf.LazyStringList applicationTypes_;
    /**
     * <code>repeated string application_types = 1;</code>
     */
    public java.util.List<java.lang.String>
        getApplicationTypesList() {
      return applicationTypes_;
    }
    /**
     * <code>repeated string application_types = 1;</code>
     */
    public int getApplicationTypesCount() {
      return applicationTypes_.size();
    }
    /**
     * <code>repeated string application_types = 1;</code>
     */
    public java.lang.String getApplicationTypes(int index) {
      return applicationTypes_.get(index);
    }
    /**
     * <code>repeated string application_types = 1;</code>
     */
    public com.google.protobuf.ByteString
        getApplicationTypesBytes(int index) {
      return applicationTypes_.getByteString(index);
    }

    // repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;
    public static final int APPLICATION_STATES_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto> applicationStates_;
    /**
     * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto> getApplicationStatesList() {
      return applicationStates_;
    }
    /**
     * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
     */
    public int getApplicationStatesCount() {
      return applicationStates_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto getApplicationStates(int index) {
      return applicationStates_.get(index);
    }

    // repeated string users = 3;
    public static final int USERS_FIELD_NUMBER = 3;
    private com.google.protobuf.LazyStringList users_;
    /**
     * <code>repeated string users = 3;</code>
     */
    public java.util.List<java.lang.String>
        getUsersList() {
      return users_;
    }
    /**
     * <code>repeated string users = 3;</code>
     */
    public int getUsersCount() {
      return users_.size();
    }
    /**
     * <code>repeated string users = 3;</code>
     */
    public java.lang.String getUsers(int index) {
      return users_.get(index);
    }
    /**
     * <code>repeated string users = 3;</code>
     */
    public com.google.protobuf.ByteString
        getUsersBytes(int index) {
      return users_.getByteString(index);
    }

    // repeated string queues = 4;
    public static final int QUEUES_FIELD_NUMBER = 4;
    private com.google.protobuf.LazyStringList queues_;
    /**
     * <code>repeated string queues = 4;</code>
     */
    public java.util.List<java.lang.String>
        getQueuesList() {
      return queues_;
    }
    /**
     * <code>repeated string queues = 4;</code>
     */
    public int getQueuesCount() {
      return queues_.size();
    }
    /**
     * <code>repeated string queues = 4;</code>
     */
    public java.lang.String getQueues(int index) {
      return queues_.get(index);
    }
    /**
     * <code>repeated string queues = 4;</code>
     */
    public com.google.protobuf.ByteString
        getQueuesBytes(int index) {
      return queues_.getByteString(index);
    }

    // optional int64 limit = 5;
    public static final int LIMIT_FIELD_NUMBER = 5;
    private long limit_;
    /**
     * <code>optional int64 limit = 5;</code>
     */
    public boolean hasLimit() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional int64 limit = 5;</code>
     */
    public long getLimit() {
      return limit_;
    }

    // optional int64 start_begin = 6;
    public static final int START_BEGIN_FIELD_NUMBER = 6;
    private long startBegin_;
    /**
     * <code>optional int64 start_begin = 6;</code>
     */
    public boolean hasStartBegin() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional int64 start_begin = 6;</code>
     */
    public long getStartBegin() {
      return startBegin_;
    }

    // optional int64 start_end = 7;
    public static final int START_END_FIELD_NUMBER = 7;
    private long startEnd_;
    /**
     * <code>optional int64 start_end = 7;</code>
     */
    public boolean hasStartEnd() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional int64 start_end = 7;</code>
     */
    public long getStartEnd() {
      return startEnd_;
    }

    // optional int64 finish_begin = 8;
    public static final int FINISH_BEGIN_FIELD_NUMBER = 8;
    private long finishBegin_;
    /**
     * <code>optional int64 finish_begin = 8;</code>
     */
    public boolean hasFinishBegin() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>optional int64 finish_begin = 8;</code>
     */
    public long getFinishBegin() {
      return finishBegin_;
    }

    // optional int64 finish_end = 9;
    public static final int FINISH_END_FIELD_NUMBER = 9;
    private long finishEnd_;
    /**
     * <code>optional int64 finish_end = 9;</code>
     */
    public boolean hasFinishEnd() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>optional int64 finish_end = 9;</code>
     */
    public long getFinishEnd() {
      return finishEnd_;
    }

    // repeated string applicationTags = 10;
    public static final int APPLICATIONTAGS_FIELD_NUMBER = 10;
    private com.google.protobuf.LazyStringList applicationTags_;
    /**
     * <code>repeated string applicationTags = 10;</code>
     */
    public java.util.List<java.lang.String>
        getApplicationTagsList() {
      return applicationTags_;
    }
    /**
     * <code>repeated string applicationTags = 10;</code>
     */
    public int getApplicationTagsCount() {
      return applicationTags_.size();
    }
    /**
     * <code>repeated string applicationTags = 10;</code>
     */
    public java.lang.String getApplicationTags(int index) {
      return applicationTags_.get(index);
    }
    /**
     * <code>repeated string applicationTags = 10;</code>
     */
    public com.google.protobuf.ByteString
        getApplicationTagsBytes(int index) {
      return applicationTags_.getByteString(index);
    }

    // optional .hadoop.yarn.ApplicationsRequestScopeProto scope = 11 [default = ALL];
    public static final int SCOPE_FIELD_NUMBER = 11;
    private org.apache.hadoop.yarn.proto.YarnServiceProtos.ApplicationsRequestScopeProto scope_;
    /**
     * <code>optional .hadoop.yarn.ApplicationsRequestScopeProto scope = 11 [default = ALL];</code>
     */
    public boolean hasScope() {
      return ((bitField0_ & 0x00000020) == 0x00000020);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationsRequestScopeProto scope = 11 [default = ALL];</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.ApplicationsRequestScopeProto getScope() {
      return scope_;
    }

    private void initFields() {
      applicationTypes_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      applicationStates_ = java.util.Collections.emptyList();
      users_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      queues_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      limit_ = 0L;
      startBegin_ = 0L;
      startEnd_ = 0L;
      finishBegin_ = 0L;
      finishEnd_ = 0L;
      applicationTags_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      scope_ = org.apache.hadoop.yarn.proto.YarnServiceProtos.ApplicationsRequestScopeProto.ALL;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < applicationTypes_.size(); i++) {
        output.writeBytes(1, applicationTypes_.getByteString(i));
      }
      for (int i = 0; i < applicationStates_.size(); i++) {
        output.writeEnum(2, applicationStates_.get(i).getNumber());
      }
      for (int i = 0; i < users_.size(); i++) {
        output.writeBytes(3, users_.getByteString(i));
      }
      for (int i = 0; i < queues_.size(); i++) {
        output.writeBytes(4, queues_.getByteString(i));
      }
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeInt64(5, limit_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeInt64(6, startBegin_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeInt64(7, startEnd_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeInt64(8, finishBegin_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeInt64(9, finishEnd_);
      }
      for (int i = 0; i < applicationTags_.size(); i++) {
        output.writeBytes(10, applicationTags_.getByteString(i));
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        output.writeEnum(11, scope_.getNumber());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        for (int i = 0; i < applicationTypes_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeBytesSizeNoTag(applicationTypes_.getByteString(i));
        }
        size += dataSize;
        size += 1 * getApplicationTypesList().size();
      }
      {
        int dataSize = 0;
        for (int i = 0; i < applicationStates_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeEnumSizeNoTag(applicationStates_.get(i).getNumber());
        }
        size += dataSize;
        size += 1 * applicationStates_.size();
      }
      {
        int dataSize = 0;
        for (int i = 0; i < users_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeBytesSizeNoTag(users_.getByteString(i));
        }
        size += dataSize;
        size += 1 * getUsersList().size();
      }
      {
        int dataSize = 0;
        for (int i = 0; i < queues_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeBytesSizeNoTag(queues_.getByteString(i));
        }
        size += dataSize;
        size += 1 * getQueuesList().size();
      }
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(5, limit_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(6, startBegin_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(7, startEnd_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(8, finishBegin_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(9, finishEnd_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < applicationTags_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeBytesSizeNoTag(applicationTags_.getByteString(i));
        }
        size += dataSize;
        size += 1 * getApplicationTagsList().size();
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(11, scope_.getNumber());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto) obj;

      boolean result = true;
      result = result && getApplicationTypesList()
          .equals(other.getApplicationTypesList());
      result = result && getApplicationStatesList()
          .equals(other.getApplicationStatesList());
      result = result && getUsersList()
          .equals(other.getUsersList());
      result = result && getQueuesList()
          .equals(other.getQueuesList());
      result = result && (hasLimit() == other.hasLimit());
      if (hasLimit()) {
        result = result && (getLimit()
            == other.getLimit());
      }
      result = result && (hasStartBegin() == other.hasStartBegin());
      if (hasStartBegin()) {
        result = result && (getStartBegin()
            == other.getStartBegin());
      }
      result = result && (hasStartEnd() == other.hasStartEnd());
      if (hasStartEnd()) {
        result = result && (getStartEnd()
            == other.getStartEnd());
      }
      result = result && (hasFinishBegin() == other.hasFinishBegin());
      if (hasFinishBegin()) {
        result = result && (getFinishBegin()
            == other.getFinishBegin());
      }
      result = result && (hasFinishEnd() == other.hasFinishEnd());
      if (hasFinishEnd()) {
        result = result && (getFinishEnd()
            == other.getFinishEnd());
      }
      result = result && getApplicationTagsList()
          .equals(other.getApplicationTagsList());
      result = result && (hasScope() == other.hasScope());
      if (hasScope()) {
        result = result &&
            (getScope() == other.getScope());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getApplicationTypesCount() > 0) {
        hash = (37 * hash) + APPLICATION_TYPES_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationTypesList().hashCode();
      }
      if (getApplicationStatesCount() > 0) {
        hash = (37 * hash) + APPLICATION_STATES_FIELD_NUMBER;
        hash = (53 * hash) + hashEnumList(getApplicationStatesList());
      }
      if (getUsersCount() > 0) {
        hash = (37 * hash) + USERS_FIELD_NUMBER;
        hash = (53 * hash) + getUsersList().hashCode();
      }
      if (getQueuesCount() > 0) {
        hash = (37 * hash) + QUEUES_FIELD_NUMBER;
        hash = (53 * hash) + getQueuesList().hashCode();
      }
      if (hasLimit()) {
        hash = (37 * hash) + LIMIT_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getLimit());
      }
      if (hasStartBegin()) {
        hash = (37 * hash) + START_BEGIN_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getStartBegin());
      }
      if (hasStartEnd()) {
        hash = (37 * hash) + START_END_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getStartEnd());
      }
      if (hasFinishBegin()) {
        hash = (37 * hash) + FINISH_BEGIN_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getFinishBegin());
      }
      if (hasFinishEnd()) {
        hash = (37 * hash) + FINISH_END_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getFinishEnd());
      }
      if (getApplicationTagsCount() > 0) {
        hash = (37 * hash) + APPLICATIONTAGS_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationTagsList().hashCode();
      }
      if (hasScope()) {
        hash = (37 * hash) + SCOPE_FIELD_NUMBER;
        hash = (53 * hash) + hashEnum(getScope());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetApplicationsRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationsRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationsRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        applicationTypes_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        applicationStates_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000002);
        users_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000004);
        queues_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000008);
        limit_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000010);
        startBegin_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000020);
        startEnd_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000040);
        finishBegin_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000080);
        finishEnd_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000100);
        applicationTags_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000200);
        scope_ = org.apache.hadoop.yarn.proto.YarnServiceProtos.ApplicationsRequestScopeProto.ALL;
        bitField0_ = (bitField0_ & ~0x00000400);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationsRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          applicationTypes_ = new com.google.protobuf.UnmodifiableLazyStringList(
              applicationTypes_);
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.applicationTypes_ = applicationTypes_;
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          applicationStates_ = java.util.Collections.unmodifiableList(applicationStates_);
          bitField0_ = (bitField0_ & ~0x00000002);
        }
        result.applicationStates_ = applicationStates_;
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          users_ = new com.google.protobuf.UnmodifiableLazyStringList(
              users_);
          bitField0_ = (bitField0_ & ~0x00000004);
        }
        result.users_ = users_;
        if (((bitField0_ & 0x00000008) == 0x00000008)) {
          queues_ = new com.google.protobuf.UnmodifiableLazyStringList(
              queues_);
          bitField0_ = (bitField0_ & ~0x00000008);
        }
        result.queues_ = queues_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000001;
        }
        result.limit_ = limit_;
        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
          to_bitField0_ |= 0x00000002;
        }
        result.startBegin_ = startBegin_;
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000004;
        }
        result.startEnd_ = startEnd_;
        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
          to_bitField0_ |= 0x00000008;
        }
        result.finishBegin_ = finishBegin_;
        if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
          to_bitField0_ |= 0x00000010;
        }
        result.finishEnd_ = finishEnd_;
        if (((bitField0_ & 0x00000200) == 0x00000200)) {
          applicationTags_ = new com.google.protobuf.UnmodifiableLazyStringList(
              applicationTags_);
          bitField0_ = (bitField0_ & ~0x00000200);
        }
        result.applicationTags_ = applicationTags_;
        if (((from_bitField0_ & 0x00000400) == 0x00000400)) {
          to_bitField0_ |= 0x00000020;
        }
        result.scope_ = scope_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto.getDefaultInstance()) return this;
        if (!other.applicationTypes_.isEmpty()) {
          if (applicationTypes_.isEmpty()) {
            applicationTypes_ = other.applicationTypes_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureApplicationTypesIsMutable();
            applicationTypes_.addAll(other.applicationTypes_);
          }
          onChanged();
        }
        if (!other.applicationStates_.isEmpty()) {
          if (applicationStates_.isEmpty()) {
            applicationStates_ = other.applicationStates_;
            bitField0_ = (bitField0_ & ~0x00000002);
          } else {
            ensureApplicationStatesIsMutable();
            applicationStates_.addAll(other.applicationStates_);
          }
          onChanged();
        }
        if (!other.users_.isEmpty()) {
          if (users_.isEmpty()) {
            users_ = other.users_;
            bitField0_ = (bitField0_ & ~0x00000004);
          } else {
            ensureUsersIsMutable();
            users_.addAll(other.users_);
          }
          onChanged();
        }
        if (!other.queues_.isEmpty()) {
          if (queues_.isEmpty()) {
            queues_ = other.queues_;
            bitField0_ = (bitField0_ & ~0x00000008);
          } else {
            ensureQueuesIsMutable();
            queues_.addAll(other.queues_);
          }
          onChanged();
        }
        if (other.hasLimit()) {
          setLimit(other.getLimit());
        }
        if (other.hasStartBegin()) {
          setStartBegin(other.getStartBegin());
        }
        if (other.hasStartEnd()) {
          setStartEnd(other.getStartEnd());
        }
        if (other.hasFinishBegin()) {
          setFinishBegin(other.getFinishBegin());
        }
        if (other.hasFinishEnd()) {
          setFinishEnd(other.getFinishEnd());
        }
        if (!other.applicationTags_.isEmpty()) {
          if (applicationTags_.isEmpty()) {
            applicationTags_ = other.applicationTags_;
            bitField0_ = (bitField0_ & ~0x00000200);
          } else {
            ensureApplicationTagsIsMutable();
            applicationTags_.addAll(other.applicationTags_);
          }
          onChanged();
        }
        if (other.hasScope()) {
          setScope(other.getScope());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated string application_types = 1;
      private com.google.protobuf.LazyStringList applicationTypes_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureApplicationTypesIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          applicationTypes_ = new com.google.protobuf.LazyStringArrayList(applicationTypes_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <code>repeated string application_types = 1;</code>
       */
      public java.util.List<java.lang.String>
          getApplicationTypesList() {
        return java.util.Collections.unmodifiableList(applicationTypes_);
      }
      /**
       * <code>repeated string application_types = 1;</code>
       */
      public int getApplicationTypesCount() {
        return applicationTypes_.size();
      }
      /**
       * <code>repeated string application_types = 1;</code>
       */
      public java.lang.String getApplicationTypes(int index) {
        return applicationTypes_.get(index);
      }
      /**
       * <code>repeated string application_types = 1;</code>
       */
      public com.google.protobuf.ByteString
          getApplicationTypesBytes(int index) {
        return applicationTypes_.getByteString(index);
      }
      /**
       * <code>repeated string application_types = 1;</code>
       */
      public Builder setApplicationTypes(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureApplicationTypesIsMutable();
        applicationTypes_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string application_types = 1;</code>
       */
      public Builder addApplicationTypes(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureApplicationTypesIsMutable();
        applicationTypes_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string application_types = 1;</code>
       */
      public Builder addAllApplicationTypes(
          java.lang.Iterable<java.lang.String> values) {
        ensureApplicationTypesIsMutable();
        super.addAll(values, applicationTypes_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string application_types = 1;</code>
       */
      public Builder clearApplicationTypes() {
        applicationTypes_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string application_types = 1;</code>
       */
      public Builder addApplicationTypesBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureApplicationTypesIsMutable();
        applicationTypes_.add(value);
        onChanged();
        return this;
      }

      // repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto> applicationStates_ =
        java.util.Collections.emptyList();
      private void ensureApplicationStatesIsMutable() {
        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
          applicationStates_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto>(applicationStates_);
          bitField0_ |= 0x00000002;
        }
      }
      /**
       * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto> getApplicationStatesList() {
        return java.util.Collections.unmodifiableList(applicationStates_);
      }
      /**
       * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
       */
      public int getApplicationStatesCount() {
        return applicationStates_.size();
      }
      /**
       * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto getApplicationStates(int index) {
        return applicationStates_.get(index);
      }
      /**
       * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
       */
      public Builder setApplicationStates(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensureApplicationStatesIsMutable();
        applicationStates_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
       */
      public Builder addApplicationStates(org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensureApplicationStatesIsMutable();
        applicationStates_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
       */
      public Builder addAllApplicationStates(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.YarnApplicationStateProto> values) {
        ensureApplicationStatesIsMutable();
        super.addAll(values, applicationStates_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.YarnApplicationStateProto application_states = 2;</code>
       */
      public Builder clearApplicationStates() {
        applicationStates_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000002);
        onChanged();
        return this;
      }

      // repeated string users = 3;
      private com.google.protobuf.LazyStringList users_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureUsersIsMutable() {
        if (!((bitField0_ & 0x00000004) == 0x00000004)) {
          users_ = new com.google.protobuf.LazyStringArrayList(users_);
          bitField0_ |= 0x00000004;
         }
      }
      /**
       * <code>repeated string users = 3;</code>
       */
      public java.util.List<java.lang.String>
          getUsersList() {
        return java.util.Collections.unmodifiableList(users_);
      }
      /**
       * <code>repeated string users = 3;</code>
       */
      public int getUsersCount() {
        return users_.size();
      }
      /**
       * <code>repeated string users = 3;</code>
       */
      public java.lang.String getUsers(int index) {
        return users_.get(index);
      }
      /**
       * <code>repeated string users = 3;</code>
       */
      public com.google.protobuf.ByteString
          getUsersBytes(int index) {
        return users_.getByteString(index);
      }
      /**
       * <code>repeated string users = 3;</code>
       */
      public Builder setUsers(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureUsersIsMutable();
        users_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string users = 3;</code>
       */
      public Builder addUsers(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureUsersIsMutable();
        users_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string users = 3;</code>
       */
      public Builder addAllUsers(
          java.lang.Iterable<java.lang.String> values) {
        ensureUsersIsMutable();
        super.addAll(values, users_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string users = 3;</code>
       */
      public Builder clearUsers() {
        users_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000004);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string users = 3;</code>
       */
      public Builder addUsersBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureUsersIsMutable();
        users_.add(value);
        onChanged();
        return this;
      }

      // repeated string queues = 4;
      private com.google.protobuf.LazyStringList queues_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureQueuesIsMutable() {
        if (!((bitField0_ & 0x00000008) == 0x00000008)) {
          queues_ = new com.google.protobuf.LazyStringArrayList(queues_);
          bitField0_ |= 0x00000008;
         }
      }
      /**
       * <code>repeated string queues = 4;</code>
       */
      public java.util.List<java.lang.String>
          getQueuesList() {
        return java.util.Collections.unmodifiableList(queues_);
      }
      /**
       * <code>repeated string queues = 4;</code>
       */
      public int getQueuesCount() {
        return queues_.size();
      }
      /**
       * <code>repeated string queues = 4;</code>
       */
      public java.lang.String getQueues(int index) {
        return queues_.get(index);
      }
      /**
       * <code>repeated string queues = 4;</code>
       */
      public com.google.protobuf.ByteString
          getQueuesBytes(int index) {
        return queues_.getByteString(index);
      }
      /**
       * <code>repeated string queues = 4;</code>
       */
      public Builder setQueues(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureQueuesIsMutable();
        queues_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string queues = 4;</code>
       */
      public Builder addQueues(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureQueuesIsMutable();
        queues_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string queues = 4;</code>
       */
      public Builder addAllQueues(
          java.lang.Iterable<java.lang.String> values) {
        ensureQueuesIsMutable();
        super.addAll(values, queues_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string queues = 4;</code>
       */
      public Builder clearQueues() {
        queues_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000008);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string queues = 4;</code>
       */
      public Builder addQueuesBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureQueuesIsMutable();
        queues_.add(value);
        onChanged();
        return this;
      }

      // optional int64 limit = 5;
      private long limit_ ;
      /**
       * <code>optional int64 limit = 5;</code>
       */
      public boolean hasLimit() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional int64 limit = 5;</code>
       */
      public long getLimit() {
        return limit_;
      }
      /**
       * <code>optional int64 limit = 5;</code>
       */
      public Builder setLimit(long value) {
        bitField0_ |= 0x00000010;
        limit_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 limit = 5;</code>
       */
      public Builder clearLimit() {
        bitField0_ = (bitField0_ & ~0x00000010);
        limit_ = 0L;
        onChanged();
        return this;
      }

      // optional int64 start_begin = 6;
      private long startBegin_ ;
      /**
       * <code>optional int64 start_begin = 6;</code>
       */
      public boolean hasStartBegin() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      /**
       * <code>optional int64 start_begin = 6;</code>
       */
      public long getStartBegin() {
        return startBegin_;
      }
      /**
       * <code>optional int64 start_begin = 6;</code>
       */
      public Builder setStartBegin(long value) {
        bitField0_ |= 0x00000020;
        startBegin_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 start_begin = 6;</code>
       */
      public Builder clearStartBegin() {
        bitField0_ = (bitField0_ & ~0x00000020);
        startBegin_ = 0L;
        onChanged();
        return this;
      }

      // optional int64 start_end = 7;
      private long startEnd_ ;
      /**
       * <code>optional int64 start_end = 7;</code>
       */
      public boolean hasStartEnd() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      /**
       * <code>optional int64 start_end = 7;</code>
       */
      public long getStartEnd() {
        return startEnd_;
      }
      /**
       * <code>optional int64 start_end = 7;</code>
       */
      public Builder setStartEnd(long value) {
        bitField0_ |= 0x00000040;
        startEnd_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 start_end = 7;</code>
       */
      public Builder clearStartEnd() {
        bitField0_ = (bitField0_ & ~0x00000040);
        startEnd_ = 0L;
        onChanged();
        return this;
      }

      // optional int64 finish_begin = 8;
      private long finishBegin_ ;
      /**
       * <code>optional int64 finish_begin = 8;</code>
       */
      public boolean hasFinishBegin() {
        return ((bitField0_ & 0x00000080) == 0x00000080);
      }
      /**
       * <code>optional int64 finish_begin = 8;</code>
       */
      public long getFinishBegin() {
        return finishBegin_;
      }
      /**
       * <code>optional int64 finish_begin = 8;</code>
       */
      public Builder setFinishBegin(long value) {
        bitField0_ |= 0x00000080;
        finishBegin_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 finish_begin = 8;</code>
       */
      public Builder clearFinishBegin() {
        bitField0_ = (bitField0_ & ~0x00000080);
        finishBegin_ = 0L;
        onChanged();
        return this;
      }

      // optional int64 finish_end = 9;
      private long finishEnd_ ;
      /**
       * <code>optional int64 finish_end = 9;</code>
       */
      public boolean hasFinishEnd() {
        return ((bitField0_ & 0x00000100) == 0x00000100);
      }
      /**
       * <code>optional int64 finish_end = 9;</code>
       */
      public long getFinishEnd() {
        return finishEnd_;
      }
      /**
       * <code>optional int64 finish_end = 9;</code>
       */
      public Builder setFinishEnd(long value) {
        bitField0_ |= 0x00000100;
        finishEnd_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 finish_end = 9;</code>
       */
      public Builder clearFinishEnd() {
        bitField0_ = (bitField0_ & ~0x00000100);
        finishEnd_ = 0L;
        onChanged();
        return this;
      }

      // repeated string applicationTags = 10;
      private com.google.protobuf.LazyStringList applicationTags_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureApplicationTagsIsMutable() {
        if (!((bitField0_ & 0x00000200) == 0x00000200)) {
          applicationTags_ = new com.google.protobuf.LazyStringArrayList(applicationTags_);
          bitField0_ |= 0x00000200;
         }
      }
      /**
       * <code>repeated string applicationTags = 10;</code>
       */
      public java.util.List<java.lang.String>
          getApplicationTagsList() {
        return java.util.Collections.unmodifiableList(applicationTags_);
      }
      /**
       * <code>repeated string applicationTags = 10;</code>
       */
      public int getApplicationTagsCount() {
        return applicationTags_.size();
      }
      /**
       * <code>repeated string applicationTags = 10;</code>
       */
      public java.lang.String getApplicationTags(int index) {
        return applicationTags_.get(index);
      }
      /**
       * <code>repeated string applicationTags = 10;</code>
       */
      public com.google.protobuf.ByteString
          getApplicationTagsBytes(int index) {
        return applicationTags_.getByteString(index);
      }
      /**
       * <code>repeated string applicationTags = 10;</code>
       */
      public Builder setApplicationTags(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureApplicationTagsIsMutable();
        applicationTags_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string applicationTags = 10;</code>
       */
      public Builder addApplicationTags(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureApplicationTagsIsMutable();
        applicationTags_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string applicationTags = 10;</code>
       */
      public Builder addAllApplicationTags(
          java.lang.Iterable<java.lang.String> values) {
        ensureApplicationTagsIsMutable();
        super.addAll(values, applicationTags_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string applicationTags = 10;</code>
       */
      public Builder clearApplicationTags() {
        applicationTags_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000200);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string applicationTags = 10;</code>
       */
      public Builder addApplicationTagsBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureApplicationTagsIsMutable();
        applicationTags_.add(value);
        onChanged();
        return this;
      }

      // optional .hadoop.yarn.ApplicationsRequestScopeProto scope = 11 [default = ALL];
      private org.apache.hadoop.yarn.proto.YarnServiceProtos.ApplicationsRequestScopeProto scope_ = org.apache.hadoop.yarn.proto.YarnServiceProtos.ApplicationsRequestScopeProto.ALL;
      /**
       * <code>optional .hadoop.yarn.ApplicationsRequestScopeProto scope = 11 [default = ALL];</code>
       */
      public boolean hasScope() {
        return ((bitField0_ & 0x00000400) == 0x00000400);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationsRequestScopeProto scope = 11 [default = ALL];</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ApplicationsRequestScopeProto getScope() {
        return scope_;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationsRequestScopeProto scope = 11 [default = ALL];</code>
       */
      public Builder setScope(org.apache.hadoop.yarn.proto.YarnServiceProtos.ApplicationsRequestScopeProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000400;
        scope_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationsRequestScopeProto scope = 11 [default = ALL];</code>
       */
      public Builder clearScope() {
        bitField0_ = (bitField0_ & ~0x00000400);
        scope_ = org.apache.hadoop.yarn.proto.YarnServiceProtos.ApplicationsRequestScopeProto.ALL;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetApplicationsRequestProto)
    }

    static {
      defaultInstance = new GetApplicationsRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetApplicationsRequestProto)
  }

  public interface GetApplicationsResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.ApplicationReportProto applications = 1;
    /**
     * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto> 
        getApplicationsList();
    /**
     * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto getApplications(int index);
    /**
     * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
     */
    int getApplicationsCount();
    /**
     * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder> 
        getApplicationsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder getApplicationsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetApplicationsResponseProto}
   */
  public static final class GetApplicationsResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetApplicationsResponseProtoOrBuilder {
    // Use GetApplicationsResponseProto.newBuilder() to construct.
    private GetApplicationsResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetApplicationsResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetApplicationsResponseProto defaultInstance;
    public static GetApplicationsResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetApplicationsResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetApplicationsResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                applications_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              applications_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          applications_ = java.util.Collections.unmodifiableList(applications_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationsResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationsResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetApplicationsResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetApplicationsResponseProto>() {
      public GetApplicationsResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetApplicationsResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetApplicationsResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.ApplicationReportProto applications = 1;
    public static final int APPLICATIONS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto> applications_;
    /**
     * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto> getApplicationsList() {
      return applications_;
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder> 
        getApplicationsOrBuilderList() {
      return applications_;
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
     */
    public int getApplicationsCount() {
      return applications_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto getApplications(int index) {
      return applications_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder getApplicationsOrBuilder(
        int index) {
      return applications_.get(index);
    }

    private void initFields() {
      applications_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      for (int i = 0; i < getApplicationsCount(); i++) {
        if (!getApplications(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < applications_.size(); i++) {
        output.writeMessage(1, applications_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < applications_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, applications_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto) obj;

      boolean result = true;
      result = result && getApplicationsList()
          .equals(other.getApplicationsList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getApplicationsCount() > 0) {
        hash = (37 * hash) + APPLICATIONS_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetApplicationsResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationsResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationsResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getApplicationsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (applicationsBuilder_ == null) {
          applications_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          applicationsBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationsResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (applicationsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            applications_ = java.util.Collections.unmodifiableList(applications_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.applications_ = applications_;
        } else {
          result.applications_ = applicationsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto.getDefaultInstance()) return this;
        if (applicationsBuilder_ == null) {
          if (!other.applications_.isEmpty()) {
            if (applications_.isEmpty()) {
              applications_ = other.applications_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureApplicationsIsMutable();
              applications_.addAll(other.applications_);
            }
            onChanged();
          }
        } else {
          if (!other.applications_.isEmpty()) {
            if (applicationsBuilder_.isEmpty()) {
              applicationsBuilder_.dispose();
              applicationsBuilder_ = null;
              applications_ = other.applications_;
              bitField0_ = (bitField0_ & ~0x00000001);
              applicationsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getApplicationsFieldBuilder() : null;
            } else {
              applicationsBuilder_.addAllMessages(other.applications_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        for (int i = 0; i < getApplicationsCount(); i++) {
          if (!getApplications(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationsResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.ApplicationReportProto applications = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto> applications_ =
        java.util.Collections.emptyList();
      private void ensureApplicationsIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          applications_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto>(applications_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder> applicationsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto> getApplicationsList() {
        if (applicationsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(applications_);
        } else {
          return applicationsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public int getApplicationsCount() {
        if (applicationsBuilder_ == null) {
          return applications_.size();
        } else {
          return applicationsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto getApplications(int index) {
        if (applicationsBuilder_ == null) {
          return applications_.get(index);
        } else {
          return applicationsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public Builder setApplications(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto value) {
        if (applicationsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureApplicationsIsMutable();
          applications_.set(index, value);
          onChanged();
        } else {
          applicationsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public Builder setApplications(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder builderForValue) {
        if (applicationsBuilder_ == null) {
          ensureApplicationsIsMutable();
          applications_.set(index, builderForValue.build());
          onChanged();
        } else {
          applicationsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public Builder addApplications(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto value) {
        if (applicationsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureApplicationsIsMutable();
          applications_.add(value);
          onChanged();
        } else {
          applicationsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public Builder addApplications(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto value) {
        if (applicationsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureApplicationsIsMutable();
          applications_.add(index, value);
          onChanged();
        } else {
          applicationsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public Builder addApplications(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder builderForValue) {
        if (applicationsBuilder_ == null) {
          ensureApplicationsIsMutable();
          applications_.add(builderForValue.build());
          onChanged();
        } else {
          applicationsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public Builder addApplications(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder builderForValue) {
        if (applicationsBuilder_ == null) {
          ensureApplicationsIsMutable();
          applications_.add(index, builderForValue.build());
          onChanged();
        } else {
          applicationsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public Builder addAllApplications(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto> values) {
        if (applicationsBuilder_ == null) {
          ensureApplicationsIsMutable();
          super.addAll(values, applications_);
          onChanged();
        } else {
          applicationsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public Builder clearApplications() {
        if (applicationsBuilder_ == null) {
          applications_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          applicationsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public Builder removeApplications(int index) {
        if (applicationsBuilder_ == null) {
          ensureApplicationsIsMutable();
          applications_.remove(index);
          onChanged();
        } else {
          applicationsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder getApplicationsBuilder(
          int index) {
        return getApplicationsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder getApplicationsOrBuilder(
          int index) {
        if (applicationsBuilder_ == null) {
          return applications_.get(index);  } else {
          return applicationsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder> 
           getApplicationsOrBuilderList() {
        if (applicationsBuilder_ != null) {
          return applicationsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(applications_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder addApplicationsBuilder() {
        return getApplicationsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder addApplicationsBuilder(
          int index) {
        return getApplicationsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationReportProto applications = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder> 
           getApplicationsBuilderList() {
        return getApplicationsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder> 
          getApplicationsFieldBuilder() {
        if (applicationsBuilder_ == null) {
          applicationsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProtoOrBuilder>(
                  applications_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          applications_ = null;
        }
        return applicationsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetApplicationsResponseProto)
    }

    static {
      defaultInstance = new GetApplicationsResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetApplicationsResponseProto)
  }

  public interface GetClusterNodesRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.NodeStateProto nodeStates = 1;
    /**
     * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto> getNodeStatesList();
    /**
     * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
     */
    int getNodeStatesCount();
    /**
     * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto getNodeStates(int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetClusterNodesRequestProto}
   */
  public static final class GetClusterNodesRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetClusterNodesRequestProtoOrBuilder {
    // Use GetClusterNodesRequestProto.newBuilder() to construct.
    private GetClusterNodesRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetClusterNodesRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetClusterNodesRequestProto defaultInstance;
    public static GetClusterNodesRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetClusterNodesRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetClusterNodesRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              int rawValue = input.readEnum();
              org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto value = org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(1, rawValue);
              } else {
                if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                  nodeStates_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto>();
                  mutable_bitField0_ |= 0x00000001;
                }
                nodeStates_.add(value);
              }
              break;
            }
            case 10: {
              int length = input.readRawVarint32();
              int oldLimit = input.pushLimit(length);
              while(input.getBytesUntilLimit() > 0) {
                int rawValue = input.readEnum();
                org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto value = org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto.valueOf(rawValue);
                if (value == null) {
                  unknownFields.mergeVarintField(1, rawValue);
                } else {
                  if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                    nodeStates_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto>();
                    mutable_bitField0_ |= 0x00000001;
                  }
                  nodeStates_.add(value);
                }
              }
              input.popLimit(oldLimit);
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          nodeStates_ = java.util.Collections.unmodifiableList(nodeStates_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodesRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodesRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetClusterNodesRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetClusterNodesRequestProto>() {
      public GetClusterNodesRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetClusterNodesRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetClusterNodesRequestProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.NodeStateProto nodeStates = 1;
    public static final int NODESTATES_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto> nodeStates_;
    /**
     * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto> getNodeStatesList() {
      return nodeStates_;
    }
    /**
     * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
     */
    public int getNodeStatesCount() {
      return nodeStates_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto getNodeStates(int index) {
      return nodeStates_.get(index);
    }

    private void initFields() {
      nodeStates_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < nodeStates_.size(); i++) {
        output.writeEnum(1, nodeStates_.get(i).getNumber());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        for (int i = 0; i < nodeStates_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeEnumSizeNoTag(nodeStates_.get(i).getNumber());
        }
        size += dataSize;
        size += 1 * nodeStates_.size();
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto) obj;

      boolean result = true;
      result = result && getNodeStatesList()
          .equals(other.getNodeStatesList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getNodeStatesCount() > 0) {
        hash = (37 * hash) + NODESTATES_FIELD_NUMBER;
        hash = (53 * hash) + hashEnumList(getNodeStatesList());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetClusterNodesRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodesRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodesRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        nodeStates_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodesRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto(this);
        int from_bitField0_ = bitField0_;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          nodeStates_ = java.util.Collections.unmodifiableList(nodeStates_);
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.nodeStates_ = nodeStates_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto.getDefaultInstance()) return this;
        if (!other.nodeStates_.isEmpty()) {
          if (nodeStates_.isEmpty()) {
            nodeStates_ = other.nodeStates_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureNodeStatesIsMutable();
            nodeStates_.addAll(other.nodeStates_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.NodeStateProto nodeStates = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto> nodeStates_ =
        java.util.Collections.emptyList();
      private void ensureNodeStatesIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          nodeStates_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto>(nodeStates_);
          bitField0_ |= 0x00000001;
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto> getNodeStatesList() {
        return java.util.Collections.unmodifiableList(nodeStates_);
      }
      /**
       * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
       */
      public int getNodeStatesCount() {
        return nodeStates_.size();
      }
      /**
       * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto getNodeStates(int index) {
        return nodeStates_.get(index);
      }
      /**
       * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
       */
      public Builder setNodeStates(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensureNodeStatesIsMutable();
        nodeStates_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
       */
      public Builder addNodeStates(org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensureNodeStatesIsMutable();
        nodeStates_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
       */
      public Builder addAllNodeStates(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeStateProto> values) {
        ensureNodeStatesIsMutable();
        super.addAll(values, nodeStates_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeStateProto nodeStates = 1;</code>
       */
      public Builder clearNodeStates() {
        nodeStates_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetClusterNodesRequestProto)
    }

    static {
      defaultInstance = new GetClusterNodesRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetClusterNodesRequestProto)
  }

  public interface GetClusterNodesResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.NodeReportProto nodeReports = 1;
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto> 
        getNodeReportsList();
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto getNodeReports(int index);
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
     */
    int getNodeReportsCount();
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder> 
        getNodeReportsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder getNodeReportsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetClusterNodesResponseProto}
   */
  public static final class GetClusterNodesResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetClusterNodesResponseProtoOrBuilder {
    // Use GetClusterNodesResponseProto.newBuilder() to construct.
    private GetClusterNodesResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetClusterNodesResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetClusterNodesResponseProto defaultInstance;
    public static GetClusterNodesResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetClusterNodesResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetClusterNodesResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                nodeReports_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              nodeReports_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          nodeReports_ = java.util.Collections.unmodifiableList(nodeReports_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodesResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodesResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetClusterNodesResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetClusterNodesResponseProto>() {
      public GetClusterNodesResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetClusterNodesResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetClusterNodesResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.NodeReportProto nodeReports = 1;
    public static final int NODEREPORTS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto> nodeReports_;
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto> getNodeReportsList() {
      return nodeReports_;
    }
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder> 
        getNodeReportsOrBuilderList() {
      return nodeReports_;
    }
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
     */
    public int getNodeReportsCount() {
      return nodeReports_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto getNodeReports(int index) {
      return nodeReports_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder getNodeReportsOrBuilder(
        int index) {
      return nodeReports_.get(index);
    }

    private void initFields() {
      nodeReports_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < nodeReports_.size(); i++) {
        output.writeMessage(1, nodeReports_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < nodeReports_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, nodeReports_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto) obj;

      boolean result = true;
      result = result && getNodeReportsList()
          .equals(other.getNodeReportsList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getNodeReportsCount() > 0) {
        hash = (37 * hash) + NODEREPORTS_FIELD_NUMBER;
        hash = (53 * hash) + getNodeReportsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetClusterNodesResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodesResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodesResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getNodeReportsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (nodeReportsBuilder_ == null) {
          nodeReports_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          nodeReportsBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodesResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (nodeReportsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            nodeReports_ = java.util.Collections.unmodifiableList(nodeReports_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.nodeReports_ = nodeReports_;
        } else {
          result.nodeReports_ = nodeReportsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto.getDefaultInstance()) return this;
        if (nodeReportsBuilder_ == null) {
          if (!other.nodeReports_.isEmpty()) {
            if (nodeReports_.isEmpty()) {
              nodeReports_ = other.nodeReports_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureNodeReportsIsMutable();
              nodeReports_.addAll(other.nodeReports_);
            }
            onChanged();
          }
        } else {
          if (!other.nodeReports_.isEmpty()) {
            if (nodeReportsBuilder_.isEmpty()) {
              nodeReportsBuilder_.dispose();
              nodeReportsBuilder_ = null;
              nodeReports_ = other.nodeReports_;
              bitField0_ = (bitField0_ & ~0x00000001);
              nodeReportsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getNodeReportsFieldBuilder() : null;
            } else {
              nodeReportsBuilder_.addAllMessages(other.nodeReports_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodesResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.NodeReportProto nodeReports = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto> nodeReports_ =
        java.util.Collections.emptyList();
      private void ensureNodeReportsIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          nodeReports_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto>(nodeReports_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder> nodeReportsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto> getNodeReportsList() {
        if (nodeReportsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(nodeReports_);
        } else {
          return nodeReportsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public int getNodeReportsCount() {
        if (nodeReportsBuilder_ == null) {
          return nodeReports_.size();
        } else {
          return nodeReportsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto getNodeReports(int index) {
        if (nodeReportsBuilder_ == null) {
          return nodeReports_.get(index);
        } else {
          return nodeReportsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public Builder setNodeReports(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto value) {
        if (nodeReportsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodeReportsIsMutable();
          nodeReports_.set(index, value);
          onChanged();
        } else {
          nodeReportsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public Builder setNodeReports(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder builderForValue) {
        if (nodeReportsBuilder_ == null) {
          ensureNodeReportsIsMutable();
          nodeReports_.set(index, builderForValue.build());
          onChanged();
        } else {
          nodeReportsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public Builder addNodeReports(org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto value) {
        if (nodeReportsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodeReportsIsMutable();
          nodeReports_.add(value);
          onChanged();
        } else {
          nodeReportsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public Builder addNodeReports(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto value) {
        if (nodeReportsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodeReportsIsMutable();
          nodeReports_.add(index, value);
          onChanged();
        } else {
          nodeReportsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public Builder addNodeReports(
          org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder builderForValue) {
        if (nodeReportsBuilder_ == null) {
          ensureNodeReportsIsMutable();
          nodeReports_.add(builderForValue.build());
          onChanged();
        } else {
          nodeReportsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public Builder addNodeReports(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder builderForValue) {
        if (nodeReportsBuilder_ == null) {
          ensureNodeReportsIsMutable();
          nodeReports_.add(index, builderForValue.build());
          onChanged();
        } else {
          nodeReportsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public Builder addAllNodeReports(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto> values) {
        if (nodeReportsBuilder_ == null) {
          ensureNodeReportsIsMutable();
          super.addAll(values, nodeReports_);
          onChanged();
        } else {
          nodeReportsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public Builder clearNodeReports() {
        if (nodeReportsBuilder_ == null) {
          nodeReports_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          nodeReportsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public Builder removeNodeReports(int index) {
        if (nodeReportsBuilder_ == null) {
          ensureNodeReportsIsMutable();
          nodeReports_.remove(index);
          onChanged();
        } else {
          nodeReportsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder getNodeReportsBuilder(
          int index) {
        return getNodeReportsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder getNodeReportsOrBuilder(
          int index) {
        if (nodeReportsBuilder_ == null) {
          return nodeReports_.get(index);  } else {
          return nodeReportsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder> 
           getNodeReportsOrBuilderList() {
        if (nodeReportsBuilder_ != null) {
          return nodeReportsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(nodeReports_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder addNodeReportsBuilder() {
        return getNodeReportsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder addNodeReportsBuilder(
          int index) {
        return getNodeReportsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.NodeReportProto nodeReports = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder> 
           getNodeReportsBuilderList() {
        return getNodeReportsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder> 
          getNodeReportsFieldBuilder() {
        if (nodeReportsBuilder_ == null) {
          nodeReportsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeReportProtoOrBuilder>(
                  nodeReports_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          nodeReports_ = null;
        }
        return nodeReportsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetClusterNodesResponseProto)
    }

    static {
      defaultInstance = new GetClusterNodesResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetClusterNodesResponseProto)
  }

  public interface GetQueueInfoRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional string queueName = 1;
    /**
     * <code>optional string queueName = 1;</code>
     */
    boolean hasQueueName();
    /**
     * <code>optional string queueName = 1;</code>
     */
    java.lang.String getQueueName();
    /**
     * <code>optional string queueName = 1;</code>
     */
    com.google.protobuf.ByteString
        getQueueNameBytes();

    // optional bool includeApplications = 2;
    /**
     * <code>optional bool includeApplications = 2;</code>
     */
    boolean hasIncludeApplications();
    /**
     * <code>optional bool includeApplications = 2;</code>
     */
    boolean getIncludeApplications();

    // optional bool includeChildQueues = 3;
    /**
     * <code>optional bool includeChildQueues = 3;</code>
     */
    boolean hasIncludeChildQueues();
    /**
     * <code>optional bool includeChildQueues = 3;</code>
     */
    boolean getIncludeChildQueues();

    // optional bool recursive = 4;
    /**
     * <code>optional bool recursive = 4;</code>
     */
    boolean hasRecursive();
    /**
     * <code>optional bool recursive = 4;</code>
     */
    boolean getRecursive();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetQueueInfoRequestProto}
   */
  public static final class GetQueueInfoRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetQueueInfoRequestProtoOrBuilder {
    // Use GetQueueInfoRequestProto.newBuilder() to construct.
    private GetQueueInfoRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetQueueInfoRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetQueueInfoRequestProto defaultInstance;
    public static GetQueueInfoRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetQueueInfoRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetQueueInfoRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              queueName_ = input.readBytes();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              includeApplications_ = input.readBool();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              includeChildQueues_ = input.readBool();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              recursive_ = input.readBool();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueInfoRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueInfoRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetQueueInfoRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetQueueInfoRequestProto>() {
      public GetQueueInfoRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetQueueInfoRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetQueueInfoRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional string queueName = 1;
    public static final int QUEUENAME_FIELD_NUMBER = 1;
    private java.lang.Object queueName_;
    /**
     * <code>optional string queueName = 1;</code>
     */
    public boolean hasQueueName() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional string queueName = 1;</code>
     */
    public java.lang.String getQueueName() {
      java.lang.Object ref = queueName_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          queueName_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string queueName = 1;</code>
     */
    public com.google.protobuf.ByteString
        getQueueNameBytes() {
      java.lang.Object ref = queueName_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        queueName_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional bool includeApplications = 2;
    public static final int INCLUDEAPPLICATIONS_FIELD_NUMBER = 2;
    private boolean includeApplications_;
    /**
     * <code>optional bool includeApplications = 2;</code>
     */
    public boolean hasIncludeApplications() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional bool includeApplications = 2;</code>
     */
    public boolean getIncludeApplications() {
      return includeApplications_;
    }

    // optional bool includeChildQueues = 3;
    public static final int INCLUDECHILDQUEUES_FIELD_NUMBER = 3;
    private boolean includeChildQueues_;
    /**
     * <code>optional bool includeChildQueues = 3;</code>
     */
    public boolean hasIncludeChildQueues() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional bool includeChildQueues = 3;</code>
     */
    public boolean getIncludeChildQueues() {
      return includeChildQueues_;
    }

    // optional bool recursive = 4;
    public static final int RECURSIVE_FIELD_NUMBER = 4;
    private boolean recursive_;
    /**
     * <code>optional bool recursive = 4;</code>
     */
    public boolean hasRecursive() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>optional bool recursive = 4;</code>
     */
    public boolean getRecursive() {
      return recursive_;
    }

    private void initFields() {
      queueName_ = "";
      includeApplications_ = false;
      includeChildQueues_ = false;
      recursive_ = false;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getQueueNameBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBool(2, includeApplications_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeBool(3, includeChildQueues_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeBool(4, recursive_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getQueueNameBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(2, includeApplications_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(3, includeChildQueues_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(4, recursive_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto) obj;

      boolean result = true;
      result = result && (hasQueueName() == other.hasQueueName());
      if (hasQueueName()) {
        result = result && getQueueName()
            .equals(other.getQueueName());
      }
      result = result && (hasIncludeApplications() == other.hasIncludeApplications());
      if (hasIncludeApplications()) {
        result = result && (getIncludeApplications()
            == other.getIncludeApplications());
      }
      result = result && (hasIncludeChildQueues() == other.hasIncludeChildQueues());
      if (hasIncludeChildQueues()) {
        result = result && (getIncludeChildQueues()
            == other.getIncludeChildQueues());
      }
      result = result && (hasRecursive() == other.hasRecursive());
      if (hasRecursive()) {
        result = result && (getRecursive()
            == other.getRecursive());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasQueueName()) {
        hash = (37 * hash) + QUEUENAME_FIELD_NUMBER;
        hash = (53 * hash) + getQueueName().hashCode();
      }
      if (hasIncludeApplications()) {
        hash = (37 * hash) + INCLUDEAPPLICATIONS_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getIncludeApplications());
      }
      if (hasIncludeChildQueues()) {
        hash = (37 * hash) + INCLUDECHILDQUEUES_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getIncludeChildQueues());
      }
      if (hasRecursive()) {
        hash = (37 * hash) + RECURSIVE_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getRecursive());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetQueueInfoRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueInfoRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueInfoRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        queueName_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        includeApplications_ = false;
        bitField0_ = (bitField0_ & ~0x00000002);
        includeChildQueues_ = false;
        bitField0_ = (bitField0_ & ~0x00000004);
        recursive_ = false;
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueInfoRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.queueName_ = queueName_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.includeApplications_ = includeApplications_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.includeChildQueues_ = includeChildQueues_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.recursive_ = recursive_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto.getDefaultInstance()) return this;
        if (other.hasQueueName()) {
          bitField0_ |= 0x00000001;
          queueName_ = other.queueName_;
          onChanged();
        }
        if (other.hasIncludeApplications()) {
          setIncludeApplications(other.getIncludeApplications());
        }
        if (other.hasIncludeChildQueues()) {
          setIncludeChildQueues(other.getIncludeChildQueues());
        }
        if (other.hasRecursive()) {
          setRecursive(other.getRecursive());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional string queueName = 1;
      private java.lang.Object queueName_ = "";
      /**
       * <code>optional string queueName = 1;</code>
       */
      public boolean hasQueueName() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional string queueName = 1;</code>
       */
      public java.lang.String getQueueName() {
        java.lang.Object ref = queueName_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          queueName_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string queueName = 1;</code>
       */
      public com.google.protobuf.ByteString
          getQueueNameBytes() {
        java.lang.Object ref = queueName_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          queueName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string queueName = 1;</code>
       */
      public Builder setQueueName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        queueName_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string queueName = 1;</code>
       */
      public Builder clearQueueName() {
        bitField0_ = (bitField0_ & ~0x00000001);
        queueName_ = getDefaultInstance().getQueueName();
        onChanged();
        return this;
      }
      /**
       * <code>optional string queueName = 1;</code>
       */
      public Builder setQueueNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        queueName_ = value;
        onChanged();
        return this;
      }

      // optional bool includeApplications = 2;
      private boolean includeApplications_ ;
      /**
       * <code>optional bool includeApplications = 2;</code>
       */
      public boolean hasIncludeApplications() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional bool includeApplications = 2;</code>
       */
      public boolean getIncludeApplications() {
        return includeApplications_;
      }
      /**
       * <code>optional bool includeApplications = 2;</code>
       */
      public Builder setIncludeApplications(boolean value) {
        bitField0_ |= 0x00000002;
        includeApplications_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool includeApplications = 2;</code>
       */
      public Builder clearIncludeApplications() {
        bitField0_ = (bitField0_ & ~0x00000002);
        includeApplications_ = false;
        onChanged();
        return this;
      }

      // optional bool includeChildQueues = 3;
      private boolean includeChildQueues_ ;
      /**
       * <code>optional bool includeChildQueues = 3;</code>
       */
      public boolean hasIncludeChildQueues() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional bool includeChildQueues = 3;</code>
       */
      public boolean getIncludeChildQueues() {
        return includeChildQueues_;
      }
      /**
       * <code>optional bool includeChildQueues = 3;</code>
       */
      public Builder setIncludeChildQueues(boolean value) {
        bitField0_ |= 0x00000004;
        includeChildQueues_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool includeChildQueues = 3;</code>
       */
      public Builder clearIncludeChildQueues() {
        bitField0_ = (bitField0_ & ~0x00000004);
        includeChildQueues_ = false;
        onChanged();
        return this;
      }

      // optional bool recursive = 4;
      private boolean recursive_ ;
      /**
       * <code>optional bool recursive = 4;</code>
       */
      public boolean hasRecursive() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional bool recursive = 4;</code>
       */
      public boolean getRecursive() {
        return recursive_;
      }
      /**
       * <code>optional bool recursive = 4;</code>
       */
      public Builder setRecursive(boolean value) {
        bitField0_ |= 0x00000008;
        recursive_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool recursive = 4;</code>
       */
      public Builder clearRecursive() {
        bitField0_ = (bitField0_ & ~0x00000008);
        recursive_ = false;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetQueueInfoRequestProto)
    }

    static {
      defaultInstance = new GetQueueInfoRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetQueueInfoRequestProto)
  }

  public interface GetQueueInfoResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.QueueInfoProto queueInfo = 1;
    /**
     * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
     */
    boolean hasQueueInfo();
    /**
     * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto getQueueInfo();
    /**
     * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProtoOrBuilder getQueueInfoOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetQueueInfoResponseProto}
   */
  public static final class GetQueueInfoResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetQueueInfoResponseProtoOrBuilder {
    // Use GetQueueInfoResponseProto.newBuilder() to construct.
    private GetQueueInfoResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetQueueInfoResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetQueueInfoResponseProto defaultInstance;
    public static GetQueueInfoResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetQueueInfoResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetQueueInfoResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = queueInfo_.toBuilder();
              }
              queueInfo_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(queueInfo_);
                queueInfo_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueInfoResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueInfoResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetQueueInfoResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetQueueInfoResponseProto>() {
      public GetQueueInfoResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetQueueInfoResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetQueueInfoResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.QueueInfoProto queueInfo = 1;
    public static final int QUEUEINFO_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto queueInfo_;
    /**
     * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
     */
    public boolean hasQueueInfo() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto getQueueInfo() {
      return queueInfo_;
    }
    /**
     * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProtoOrBuilder getQueueInfoOrBuilder() {
      return queueInfo_;
    }

    private void initFields() {
      queueInfo_ = org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (hasQueueInfo()) {
        if (!getQueueInfo().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, queueInfo_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, queueInfo_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto) obj;

      boolean result = true;
      result = result && (hasQueueInfo() == other.hasQueueInfo());
      if (hasQueueInfo()) {
        result = result && getQueueInfo()
            .equals(other.getQueueInfo());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasQueueInfo()) {
        hash = (37 * hash) + QUEUEINFO_FIELD_NUMBER;
        hash = (53 * hash) + getQueueInfo().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetQueueInfoResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueInfoResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueInfoResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getQueueInfoFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (queueInfoBuilder_ == null) {
          queueInfo_ = org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.getDefaultInstance();
        } else {
          queueInfoBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueInfoResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (queueInfoBuilder_ == null) {
          result.queueInfo_ = queueInfo_;
        } else {
          result.queueInfo_ = queueInfoBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto.getDefaultInstance()) return this;
        if (other.hasQueueInfo()) {
          mergeQueueInfo(other.getQueueInfo());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (hasQueueInfo()) {
          if (!getQueueInfo().isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueInfoResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.QueueInfoProto queueInfo = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto queueInfo_ = org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto, org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProtoOrBuilder> queueInfoBuilder_;
      /**
       * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
       */
      public boolean hasQueueInfo() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto getQueueInfo() {
        if (queueInfoBuilder_ == null) {
          return queueInfo_;
        } else {
          return queueInfoBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
       */
      public Builder setQueueInfo(org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto value) {
        if (queueInfoBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          queueInfo_ = value;
          onChanged();
        } else {
          queueInfoBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
       */
      public Builder setQueueInfo(
          org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.Builder builderForValue) {
        if (queueInfoBuilder_ == null) {
          queueInfo_ = builderForValue.build();
          onChanged();
        } else {
          queueInfoBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
       */
      public Builder mergeQueueInfo(org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto value) {
        if (queueInfoBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              queueInfo_ != org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.getDefaultInstance()) {
            queueInfo_ =
              org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.newBuilder(queueInfo_).mergeFrom(value).buildPartial();
          } else {
            queueInfo_ = value;
          }
          onChanged();
        } else {
          queueInfoBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
       */
      public Builder clearQueueInfo() {
        if (queueInfoBuilder_ == null) {
          queueInfo_ = org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.getDefaultInstance();
          onChanged();
        } else {
          queueInfoBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.Builder getQueueInfoBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getQueueInfoFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProtoOrBuilder getQueueInfoOrBuilder() {
        if (queueInfoBuilder_ != null) {
          return queueInfoBuilder_.getMessageOrBuilder();
        } else {
          return queueInfo_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.QueueInfoProto queueInfo = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto, org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProtoOrBuilder> 
          getQueueInfoFieldBuilder() {
        if (queueInfoBuilder_ == null) {
          queueInfoBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto, org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.QueueInfoProtoOrBuilder>(
                  queueInfo_,
                  getParentForChildren(),
                  isClean());
          queueInfo_ = null;
        }
        return queueInfoBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetQueueInfoResponseProto)
    }

    static {
      defaultInstance = new GetQueueInfoResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetQueueInfoResponseProto)
  }

  public interface GetQueueUserAclsInfoRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetQueueUserAclsInfoRequestProto}
   */
  public static final class GetQueueUserAclsInfoRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetQueueUserAclsInfoRequestProtoOrBuilder {
    // Use GetQueueUserAclsInfoRequestProto.newBuilder() to construct.
    private GetQueueUserAclsInfoRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetQueueUserAclsInfoRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetQueueUserAclsInfoRequestProto defaultInstance;
    public static GetQueueUserAclsInfoRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetQueueUserAclsInfoRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetQueueUserAclsInfoRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueUserAclsInfoRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueUserAclsInfoRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetQueueUserAclsInfoRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetQueueUserAclsInfoRequestProto>() {
      public GetQueueUserAclsInfoRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetQueueUserAclsInfoRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetQueueUserAclsInfoRequestProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetQueueUserAclsInfoRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueUserAclsInfoRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueUserAclsInfoRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueUserAclsInfoRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetQueueUserAclsInfoRequestProto)
    }

    static {
      defaultInstance = new GetQueueUserAclsInfoRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetQueueUserAclsInfoRequestProto)
  }

  public interface GetQueueUserAclsInfoResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;
    /**
     * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto> 
        getQueueUserAclsList();
    /**
     * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto getQueueUserAcls(int index);
    /**
     * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
     */
    int getQueueUserAclsCount();
    /**
     * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProtoOrBuilder> 
        getQueueUserAclsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProtoOrBuilder getQueueUserAclsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetQueueUserAclsInfoResponseProto}
   */
  public static final class GetQueueUserAclsInfoResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetQueueUserAclsInfoResponseProtoOrBuilder {
    // Use GetQueueUserAclsInfoResponseProto.newBuilder() to construct.
    private GetQueueUserAclsInfoResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetQueueUserAclsInfoResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetQueueUserAclsInfoResponseProto defaultInstance;
    public static GetQueueUserAclsInfoResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetQueueUserAclsInfoResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetQueueUserAclsInfoResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                queueUserAcls_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              queueUserAcls_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          queueUserAcls_ = java.util.Collections.unmodifiableList(queueUserAcls_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueUserAclsInfoResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueUserAclsInfoResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetQueueUserAclsInfoResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetQueueUserAclsInfoResponseProto>() {
      public GetQueueUserAclsInfoResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetQueueUserAclsInfoResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetQueueUserAclsInfoResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;
    public static final int QUEUEUSERACLS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto> queueUserAcls_;
    /**
     * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto> getQueueUserAclsList() {
      return queueUserAcls_;
    }
    /**
     * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProtoOrBuilder> 
        getQueueUserAclsOrBuilderList() {
      return queueUserAcls_;
    }
    /**
     * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
     */
    public int getQueueUserAclsCount() {
      return queueUserAcls_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto getQueueUserAcls(int index) {
      return queueUserAcls_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProtoOrBuilder getQueueUserAclsOrBuilder(
        int index) {
      return queueUserAcls_.get(index);
    }

    private void initFields() {
      queueUserAcls_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < queueUserAcls_.size(); i++) {
        output.writeMessage(1, queueUserAcls_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < queueUserAcls_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, queueUserAcls_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto) obj;

      boolean result = true;
      result = result && getQueueUserAclsList()
          .equals(other.getQueueUserAclsList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getQueueUserAclsCount() > 0) {
        hash = (37 * hash) + QUEUEUSERACLS_FIELD_NUMBER;
        hash = (53 * hash) + getQueueUserAclsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetQueueUserAclsInfoResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueUserAclsInfoResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueUserAclsInfoResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getQueueUserAclsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (queueUserAclsBuilder_ == null) {
          queueUserAcls_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          queueUserAclsBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetQueueUserAclsInfoResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (queueUserAclsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            queueUserAcls_ = java.util.Collections.unmodifiableList(queueUserAcls_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.queueUserAcls_ = queueUserAcls_;
        } else {
          result.queueUserAcls_ = queueUserAclsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto.getDefaultInstance()) return this;
        if (queueUserAclsBuilder_ == null) {
          if (!other.queueUserAcls_.isEmpty()) {
            if (queueUserAcls_.isEmpty()) {
              queueUserAcls_ = other.queueUserAcls_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureQueueUserAclsIsMutable();
              queueUserAcls_.addAll(other.queueUserAcls_);
            }
            onChanged();
          }
        } else {
          if (!other.queueUserAcls_.isEmpty()) {
            if (queueUserAclsBuilder_.isEmpty()) {
              queueUserAclsBuilder_.dispose();
              queueUserAclsBuilder_ = null;
              queueUserAcls_ = other.queueUserAcls_;
              bitField0_ = (bitField0_ & ~0x00000001);
              queueUserAclsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getQueueUserAclsFieldBuilder() : null;
            } else {
              queueUserAclsBuilder_.addAllMessages(other.queueUserAcls_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetQueueUserAclsInfoResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto> queueUserAcls_ =
        java.util.Collections.emptyList();
      private void ensureQueueUserAclsIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          queueUserAcls_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto>(queueUserAcls_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto, org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProtoOrBuilder> queueUserAclsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto> getQueueUserAclsList() {
        if (queueUserAclsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(queueUserAcls_);
        } else {
          return queueUserAclsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public int getQueueUserAclsCount() {
        if (queueUserAclsBuilder_ == null) {
          return queueUserAcls_.size();
        } else {
          return queueUserAclsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto getQueueUserAcls(int index) {
        if (queueUserAclsBuilder_ == null) {
          return queueUserAcls_.get(index);
        } else {
          return queueUserAclsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public Builder setQueueUserAcls(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto value) {
        if (queueUserAclsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureQueueUserAclsIsMutable();
          queueUserAcls_.set(index, value);
          onChanged();
        } else {
          queueUserAclsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public Builder setQueueUserAcls(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.Builder builderForValue) {
        if (queueUserAclsBuilder_ == null) {
          ensureQueueUserAclsIsMutable();
          queueUserAcls_.set(index, builderForValue.build());
          onChanged();
        } else {
          queueUserAclsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public Builder addQueueUserAcls(org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto value) {
        if (queueUserAclsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureQueueUserAclsIsMutable();
          queueUserAcls_.add(value);
          onChanged();
        } else {
          queueUserAclsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public Builder addQueueUserAcls(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto value) {
        if (queueUserAclsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureQueueUserAclsIsMutable();
          queueUserAcls_.add(index, value);
          onChanged();
        } else {
          queueUserAclsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public Builder addQueueUserAcls(
          org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.Builder builderForValue) {
        if (queueUserAclsBuilder_ == null) {
          ensureQueueUserAclsIsMutable();
          queueUserAcls_.add(builderForValue.build());
          onChanged();
        } else {
          queueUserAclsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public Builder addQueueUserAcls(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.Builder builderForValue) {
        if (queueUserAclsBuilder_ == null) {
          ensureQueueUserAclsIsMutable();
          queueUserAcls_.add(index, builderForValue.build());
          onChanged();
        } else {
          queueUserAclsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public Builder addAllQueueUserAcls(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto> values) {
        if (queueUserAclsBuilder_ == null) {
          ensureQueueUserAclsIsMutable();
          super.addAll(values, queueUserAcls_);
          onChanged();
        } else {
          queueUserAclsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public Builder clearQueueUserAcls() {
        if (queueUserAclsBuilder_ == null) {
          queueUserAcls_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          queueUserAclsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public Builder removeQueueUserAcls(int index) {
        if (queueUserAclsBuilder_ == null) {
          ensureQueueUserAclsIsMutable();
          queueUserAcls_.remove(index);
          onChanged();
        } else {
          queueUserAclsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.Builder getQueueUserAclsBuilder(
          int index) {
        return getQueueUserAclsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProtoOrBuilder getQueueUserAclsOrBuilder(
          int index) {
        if (queueUserAclsBuilder_ == null) {
          return queueUserAcls_.get(index);  } else {
          return queueUserAclsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProtoOrBuilder> 
           getQueueUserAclsOrBuilderList() {
        if (queueUserAclsBuilder_ != null) {
          return queueUserAclsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(queueUserAcls_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.Builder addQueueUserAclsBuilder() {
        return getQueueUserAclsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.Builder addQueueUserAclsBuilder(
          int index) {
        return getQueueUserAclsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.QueueUserACLInfoProto queueUserAcls = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.Builder> 
           getQueueUserAclsBuilderList() {
        return getQueueUserAclsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto, org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProtoOrBuilder> 
          getQueueUserAclsFieldBuilder() {
        if (queueUserAclsBuilder_ == null) {
          queueUserAclsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto, org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.QueueUserACLInfoProtoOrBuilder>(
                  queueUserAcls_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          queueUserAcls_ = null;
        }
        return queueUserAclsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetQueueUserAclsInfoResponseProto)
    }

    static {
      defaultInstance = new GetQueueUserAclsInfoResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetQueueUserAclsInfoResponseProto)
  }

  public interface GetNodesToLabelsRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetNodesToLabelsRequestProto}
   */
  public static final class GetNodesToLabelsRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetNodesToLabelsRequestProtoOrBuilder {
    // Use GetNodesToLabelsRequestProto.newBuilder() to construct.
    private GetNodesToLabelsRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetNodesToLabelsRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetNodesToLabelsRequestProto defaultInstance;
    public static GetNodesToLabelsRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetNodesToLabelsRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetNodesToLabelsRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNodesToLabelsRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNodesToLabelsRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetNodesToLabelsRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetNodesToLabelsRequestProto>() {
      public GetNodesToLabelsRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetNodesToLabelsRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetNodesToLabelsRequestProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetNodesToLabelsRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNodesToLabelsRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNodesToLabelsRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNodesToLabelsRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetNodesToLabelsRequestProto)
    }

    static {
      defaultInstance = new GetNodesToLabelsRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetNodesToLabelsRequestProto)
  }

  public interface GetNodesToLabelsResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;
    /**
     * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto> 
        getNodeToLabelsList();
    /**
     * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto getNodeToLabels(int index);
    /**
     * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
     */
    int getNodeToLabelsCount();
    /**
     * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProtoOrBuilder> 
        getNodeToLabelsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProtoOrBuilder getNodeToLabelsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetNodesToLabelsResponseProto}
   */
  public static final class GetNodesToLabelsResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetNodesToLabelsResponseProtoOrBuilder {
    // Use GetNodesToLabelsResponseProto.newBuilder() to construct.
    private GetNodesToLabelsResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetNodesToLabelsResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetNodesToLabelsResponseProto defaultInstance;
    public static GetNodesToLabelsResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetNodesToLabelsResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetNodesToLabelsResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                nodeToLabels_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              nodeToLabels_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          nodeToLabels_ = java.util.Collections.unmodifiableList(nodeToLabels_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNodesToLabelsResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNodesToLabelsResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetNodesToLabelsResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetNodesToLabelsResponseProto>() {
      public GetNodesToLabelsResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetNodesToLabelsResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetNodesToLabelsResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;
    public static final int NODETOLABELS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto> nodeToLabels_;
    /**
     * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto> getNodeToLabelsList() {
      return nodeToLabels_;
    }
    /**
     * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProtoOrBuilder> 
        getNodeToLabelsOrBuilderList() {
      return nodeToLabels_;
    }
    /**
     * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
     */
    public int getNodeToLabelsCount() {
      return nodeToLabels_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto getNodeToLabels(int index) {
      return nodeToLabels_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProtoOrBuilder getNodeToLabelsOrBuilder(
        int index) {
      return nodeToLabels_.get(index);
    }

    private void initFields() {
      nodeToLabels_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < nodeToLabels_.size(); i++) {
        output.writeMessage(1, nodeToLabels_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < nodeToLabels_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, nodeToLabels_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto) obj;

      boolean result = true;
      result = result && getNodeToLabelsList()
          .equals(other.getNodeToLabelsList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getNodeToLabelsCount() > 0) {
        hash = (37 * hash) + NODETOLABELS_FIELD_NUMBER;
        hash = (53 * hash) + getNodeToLabelsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetNodesToLabelsResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNodesToLabelsResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNodesToLabelsResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getNodeToLabelsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (nodeToLabelsBuilder_ == null) {
          nodeToLabels_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          nodeToLabelsBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNodesToLabelsResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (nodeToLabelsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            nodeToLabels_ = java.util.Collections.unmodifiableList(nodeToLabels_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.nodeToLabels_ = nodeToLabels_;
        } else {
          result.nodeToLabels_ = nodeToLabelsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto.getDefaultInstance()) return this;
        if (nodeToLabelsBuilder_ == null) {
          if (!other.nodeToLabels_.isEmpty()) {
            if (nodeToLabels_.isEmpty()) {
              nodeToLabels_ = other.nodeToLabels_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureNodeToLabelsIsMutable();
              nodeToLabels_.addAll(other.nodeToLabels_);
            }
            onChanged();
          }
        } else {
          if (!other.nodeToLabels_.isEmpty()) {
            if (nodeToLabelsBuilder_.isEmpty()) {
              nodeToLabelsBuilder_.dispose();
              nodeToLabelsBuilder_ = null;
              nodeToLabels_ = other.nodeToLabels_;
              bitField0_ = (bitField0_ & ~0x00000001);
              nodeToLabelsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getNodeToLabelsFieldBuilder() : null;
            } else {
              nodeToLabelsBuilder_.addAllMessages(other.nodeToLabels_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNodesToLabelsResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto> nodeToLabels_ =
        java.util.Collections.emptyList();
      private void ensureNodeToLabelsIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          nodeToLabels_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto>(nodeToLabels_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProtoOrBuilder> nodeToLabelsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto> getNodeToLabelsList() {
        if (nodeToLabelsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(nodeToLabels_);
        } else {
          return nodeToLabelsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
       */
      public int getNodeToLabelsCount() {
        if (nodeToLabelsBuilder_ == null) {
          return nodeToLabels_.size();
        } else {
          return nodeToLabelsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto getNodeToLabels(int index) {
        if (nodeToLabelsBuilder_ == null) {
          return nodeToLabels_.get(index);
        } else {
          return nodeToLabelsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
       */
      public Builder setNodeToLabels(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto value) {
        if (nodeToLabelsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodeToLabelsIsMutable();
          nodeToLabels_.set(index, value);
          onChanged();
        } else {
          nodeToLabelsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
       */
      public Builder setNodeToLabels(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto.Builder builderForValue) {
        if (nodeToLabelsBuilder_ == null) {
          ensureNodeToLabelsIsMutable();
          nodeToLabels_.set(index, builderForValue.build());
          onChanged();
        } else {
          nodeToLabelsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
       */
      public Builder addNodeToLabels(org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto value) {
        if (nodeToLabelsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodeToLabelsIsMutable();
          nodeToLabels_.add(value);
          onChanged();
        } else {
          nodeToLabelsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
       */
      public Builder addNodeToLabels(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto value) {
        if (nodeToLabelsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodeToLabelsIsMutable();
          nodeToLabels_.add(index, value);
          onChanged();
        } else {
          nodeToLabelsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
       */
      public Builder addNodeToLabels(
          org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto.Builder builderForValue) {
        if (nodeToLabelsBuilder_ == null) {
          ensureNodeToLabelsIsMutable();
          nodeToLabels_.add(builderForValue.build());
          onChanged();
        } else {
          nodeToLabelsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
       */
      public Builder addNodeToLabels(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto.Builder builderForValue) {
        if (nodeToLabelsBuilder_ == null) {
          ensureNodeToLabelsIsMutable();
          nodeToLabels_.add(index, builderForValue.build());
          onChanged();
        } else {
          nodeToLabelsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
       */
      public Builder addAllNodeToLabels(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto> values) {
        if (nodeToLabelsBuilder_ == null) {
          ensureNodeToLabelsIsMutable();
          super.addAll(values, nodeToLabels_);
          onChanged();
        } else {
          nodeToLabelsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
       */
      public Builder clearNodeToLabels() {
        if (nodeToLabelsBuilder_ == null) {
          nodeToLabels_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          nodeToLabelsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
       */
      public Builder removeNodeToLabels(int index) {
        if (nodeToLabelsBuilder_ == null) {
          ensureNodeToLabelsIsMutable();
          nodeToLabels_.remove(index);
          onChanged();
        } else {
          nodeToLabelsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto.Builder getNodeToLabelsBuilder(
          int index) {
        return getNodeToLabelsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProtoOrBuilder getNodeToLabelsOrBuilder(
          int index) {
        if (nodeToLabelsBuilder_ == null) {
          return nodeToLabels_.get(index);  } else {
          return nodeToLabelsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProtoOrBuilder> 
           getNodeToLabelsOrBuilderList() {
        if (nodeToLabelsBuilder_ != null) {
          return nodeToLabelsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(nodeToLabels_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto.Builder addNodeToLabelsBuilder() {
        return getNodeToLabelsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto.Builder addNodeToLabelsBuilder(
          int index) {
        return getNodeToLabelsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.NodeIdToLabelsProto nodeToLabels = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto.Builder> 
           getNodeToLabelsBuilderList() {
        return getNodeToLabelsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProtoOrBuilder> 
          getNodeToLabelsFieldBuilder() {
        if (nodeToLabelsBuilder_ == null) {
          nodeToLabelsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeIdToLabelsProtoOrBuilder>(
                  nodeToLabels_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          nodeToLabels_ = null;
        }
        return nodeToLabelsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetNodesToLabelsResponseProto)
    }

    static {
      defaultInstance = new GetNodesToLabelsResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetNodesToLabelsResponseProto)
  }

  public interface GetLabelsToNodesRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated string nodeLabels = 1;
    /**
     * <code>repeated string nodeLabels = 1;</code>
     */
    java.util.List<java.lang.String>
    getNodeLabelsList();
    /**
     * <code>repeated string nodeLabels = 1;</code>
     */
    int getNodeLabelsCount();
    /**
     * <code>repeated string nodeLabels = 1;</code>
     */
    java.lang.String getNodeLabels(int index);
    /**
     * <code>repeated string nodeLabels = 1;</code>
     */
    com.google.protobuf.ByteString
        getNodeLabelsBytes(int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetLabelsToNodesRequestProto}
   */
  public static final class GetLabelsToNodesRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetLabelsToNodesRequestProtoOrBuilder {
    // Use GetLabelsToNodesRequestProto.newBuilder() to construct.
    private GetLabelsToNodesRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetLabelsToNodesRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetLabelsToNodesRequestProto defaultInstance;
    public static GetLabelsToNodesRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetLabelsToNodesRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetLabelsToNodesRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                nodeLabels_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000001;
              }
              nodeLabels_.add(input.readBytes());
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          nodeLabels_ = new com.google.protobuf.UnmodifiableLazyStringList(nodeLabels_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetLabelsToNodesRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetLabelsToNodesRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetLabelsToNodesRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetLabelsToNodesRequestProto>() {
      public GetLabelsToNodesRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetLabelsToNodesRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetLabelsToNodesRequestProto> getParserForType() {
      return PARSER;
    }

    // repeated string nodeLabels = 1;
    public static final int NODELABELS_FIELD_NUMBER = 1;
    private com.google.protobuf.LazyStringList nodeLabels_;
    /**
     * <code>repeated string nodeLabels = 1;</code>
     */
    public java.util.List<java.lang.String>
        getNodeLabelsList() {
      return nodeLabels_;
    }
    /**
     * <code>repeated string nodeLabels = 1;</code>
     */
    public int getNodeLabelsCount() {
      return nodeLabels_.size();
    }
    /**
     * <code>repeated string nodeLabels = 1;</code>
     */
    public java.lang.String getNodeLabels(int index) {
      return nodeLabels_.get(index);
    }
    /**
     * <code>repeated string nodeLabels = 1;</code>
     */
    public com.google.protobuf.ByteString
        getNodeLabelsBytes(int index) {
      return nodeLabels_.getByteString(index);
    }

    private void initFields() {
      nodeLabels_ = com.google.protobuf.LazyStringArrayList.EMPTY;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < nodeLabels_.size(); i++) {
        output.writeBytes(1, nodeLabels_.getByteString(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        for (int i = 0; i < nodeLabels_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeBytesSizeNoTag(nodeLabels_.getByteString(i));
        }
        size += dataSize;
        size += 1 * getNodeLabelsList().size();
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto) obj;

      boolean result = true;
      result = result && getNodeLabelsList()
          .equals(other.getNodeLabelsList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getNodeLabelsCount() > 0) {
        hash = (37 * hash) + NODELABELS_FIELD_NUMBER;
        hash = (53 * hash) + getNodeLabelsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetLabelsToNodesRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetLabelsToNodesRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetLabelsToNodesRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        nodeLabels_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetLabelsToNodesRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto(this);
        int from_bitField0_ = bitField0_;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          nodeLabels_ = new com.google.protobuf.UnmodifiableLazyStringList(
              nodeLabels_);
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.nodeLabels_ = nodeLabels_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto.getDefaultInstance()) return this;
        if (!other.nodeLabels_.isEmpty()) {
          if (nodeLabels_.isEmpty()) {
            nodeLabels_ = other.nodeLabels_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureNodeLabelsIsMutable();
            nodeLabels_.addAll(other.nodeLabels_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated string nodeLabels = 1;
      private com.google.protobuf.LazyStringList nodeLabels_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureNodeLabelsIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          nodeLabels_ = new com.google.protobuf.LazyStringArrayList(nodeLabels_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <code>repeated string nodeLabels = 1;</code>
       */
      public java.util.List<java.lang.String>
          getNodeLabelsList() {
        return java.util.Collections.unmodifiableList(nodeLabels_);
      }
      /**
       * <code>repeated string nodeLabels = 1;</code>
       */
      public int getNodeLabelsCount() {
        return nodeLabels_.size();
      }
      /**
       * <code>repeated string nodeLabels = 1;</code>
       */
      public java.lang.String getNodeLabels(int index) {
        return nodeLabels_.get(index);
      }
      /**
       * <code>repeated string nodeLabels = 1;</code>
       */
      public com.google.protobuf.ByteString
          getNodeLabelsBytes(int index) {
        return nodeLabels_.getByteString(index);
      }
      /**
       * <code>repeated string nodeLabels = 1;</code>
       */
      public Builder setNodeLabels(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureNodeLabelsIsMutable();
        nodeLabels_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string nodeLabels = 1;</code>
       */
      public Builder addNodeLabels(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureNodeLabelsIsMutable();
        nodeLabels_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string nodeLabels = 1;</code>
       */
      public Builder addAllNodeLabels(
          java.lang.Iterable<java.lang.String> values) {
        ensureNodeLabelsIsMutable();
        super.addAll(values, nodeLabels_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string nodeLabels = 1;</code>
       */
      public Builder clearNodeLabels() {
        nodeLabels_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string nodeLabels = 1;</code>
       */
      public Builder addNodeLabelsBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureNodeLabelsIsMutable();
        nodeLabels_.add(value);
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetLabelsToNodesRequestProto)
    }

    static {
      defaultInstance = new GetLabelsToNodesRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetLabelsToNodesRequestProto)
  }

  public interface GetLabelsToNodesResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;
    /**
     * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto> 
        getLabelsToNodesList();
    /**
     * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto getLabelsToNodes(int index);
    /**
     * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
     */
    int getLabelsToNodesCount();
    /**
     * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProtoOrBuilder> 
        getLabelsToNodesOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProtoOrBuilder getLabelsToNodesOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetLabelsToNodesResponseProto}
   */
  public static final class GetLabelsToNodesResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetLabelsToNodesResponseProtoOrBuilder {
    // Use GetLabelsToNodesResponseProto.newBuilder() to construct.
    private GetLabelsToNodesResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetLabelsToNodesResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetLabelsToNodesResponseProto defaultInstance;
    public static GetLabelsToNodesResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetLabelsToNodesResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetLabelsToNodesResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                labelsToNodes_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              labelsToNodes_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          labelsToNodes_ = java.util.Collections.unmodifiableList(labelsToNodes_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetLabelsToNodesResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetLabelsToNodesResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetLabelsToNodesResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetLabelsToNodesResponseProto>() {
      public GetLabelsToNodesResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetLabelsToNodesResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetLabelsToNodesResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;
    public static final int LABELSTONODES_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto> labelsToNodes_;
    /**
     * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto> getLabelsToNodesList() {
      return labelsToNodes_;
    }
    /**
     * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProtoOrBuilder> 
        getLabelsToNodesOrBuilderList() {
      return labelsToNodes_;
    }
    /**
     * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
     */
    public int getLabelsToNodesCount() {
      return labelsToNodes_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto getLabelsToNodes(int index) {
      return labelsToNodes_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProtoOrBuilder getLabelsToNodesOrBuilder(
        int index) {
      return labelsToNodes_.get(index);
    }

    private void initFields() {
      labelsToNodes_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < labelsToNodes_.size(); i++) {
        output.writeMessage(1, labelsToNodes_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < labelsToNodes_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, labelsToNodes_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto) obj;

      boolean result = true;
      result = result && getLabelsToNodesList()
          .equals(other.getLabelsToNodesList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getLabelsToNodesCount() > 0) {
        hash = (37 * hash) + LABELSTONODES_FIELD_NUMBER;
        hash = (53 * hash) + getLabelsToNodesList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetLabelsToNodesResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetLabelsToNodesResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetLabelsToNodesResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getLabelsToNodesFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (labelsToNodesBuilder_ == null) {
          labelsToNodes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          labelsToNodesBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetLabelsToNodesResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (labelsToNodesBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            labelsToNodes_ = java.util.Collections.unmodifiableList(labelsToNodes_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.labelsToNodes_ = labelsToNodes_;
        } else {
          result.labelsToNodes_ = labelsToNodesBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto.getDefaultInstance()) return this;
        if (labelsToNodesBuilder_ == null) {
          if (!other.labelsToNodes_.isEmpty()) {
            if (labelsToNodes_.isEmpty()) {
              labelsToNodes_ = other.labelsToNodes_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureLabelsToNodesIsMutable();
              labelsToNodes_.addAll(other.labelsToNodes_);
            }
            onChanged();
          }
        } else {
          if (!other.labelsToNodes_.isEmpty()) {
            if (labelsToNodesBuilder_.isEmpty()) {
              labelsToNodesBuilder_.dispose();
              labelsToNodesBuilder_ = null;
              labelsToNodes_ = other.labelsToNodes_;
              bitField0_ = (bitField0_ & ~0x00000001);
              labelsToNodesBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getLabelsToNodesFieldBuilder() : null;
            } else {
              labelsToNodesBuilder_.addAllMessages(other.labelsToNodes_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetLabelsToNodesResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto> labelsToNodes_ =
        java.util.Collections.emptyList();
      private void ensureLabelsToNodesIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          labelsToNodes_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto>(labelsToNodes_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto, org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProtoOrBuilder> labelsToNodesBuilder_;

      /**
       * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto> getLabelsToNodesList() {
        if (labelsToNodesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(labelsToNodes_);
        } else {
          return labelsToNodesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
       */
      public int getLabelsToNodesCount() {
        if (labelsToNodesBuilder_ == null) {
          return labelsToNodes_.size();
        } else {
          return labelsToNodesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto getLabelsToNodes(int index) {
        if (labelsToNodesBuilder_ == null) {
          return labelsToNodes_.get(index);
        } else {
          return labelsToNodesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
       */
      public Builder setLabelsToNodes(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto value) {
        if (labelsToNodesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureLabelsToNodesIsMutable();
          labelsToNodes_.set(index, value);
          onChanged();
        } else {
          labelsToNodesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
       */
      public Builder setLabelsToNodes(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto.Builder builderForValue) {
        if (labelsToNodesBuilder_ == null) {
          ensureLabelsToNodesIsMutable();
          labelsToNodes_.set(index, builderForValue.build());
          onChanged();
        } else {
          labelsToNodesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
       */
      public Builder addLabelsToNodes(org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto value) {
        if (labelsToNodesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureLabelsToNodesIsMutable();
          labelsToNodes_.add(value);
          onChanged();
        } else {
          labelsToNodesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
       */
      public Builder addLabelsToNodes(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto value) {
        if (labelsToNodesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureLabelsToNodesIsMutable();
          labelsToNodes_.add(index, value);
          onChanged();
        } else {
          labelsToNodesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
       */
      public Builder addLabelsToNodes(
          org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto.Builder builderForValue) {
        if (labelsToNodesBuilder_ == null) {
          ensureLabelsToNodesIsMutable();
          labelsToNodes_.add(builderForValue.build());
          onChanged();
        } else {
          labelsToNodesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
       */
      public Builder addLabelsToNodes(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto.Builder builderForValue) {
        if (labelsToNodesBuilder_ == null) {
          ensureLabelsToNodesIsMutable();
          labelsToNodes_.add(index, builderForValue.build());
          onChanged();
        } else {
          labelsToNodesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
       */
      public Builder addAllLabelsToNodes(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto> values) {
        if (labelsToNodesBuilder_ == null) {
          ensureLabelsToNodesIsMutable();
          super.addAll(values, labelsToNodes_);
          onChanged();
        } else {
          labelsToNodesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
       */
      public Builder clearLabelsToNodes() {
        if (labelsToNodesBuilder_ == null) {
          labelsToNodes_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          labelsToNodesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
       */
      public Builder removeLabelsToNodes(int index) {
        if (labelsToNodesBuilder_ == null) {
          ensureLabelsToNodesIsMutable();
          labelsToNodes_.remove(index);
          onChanged();
        } else {
          labelsToNodesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto.Builder getLabelsToNodesBuilder(
          int index) {
        return getLabelsToNodesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProtoOrBuilder getLabelsToNodesOrBuilder(
          int index) {
        if (labelsToNodesBuilder_ == null) {
          return labelsToNodes_.get(index);  } else {
          return labelsToNodesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProtoOrBuilder> 
           getLabelsToNodesOrBuilderList() {
        if (labelsToNodesBuilder_ != null) {
          return labelsToNodesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(labelsToNodes_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto.Builder addLabelsToNodesBuilder() {
        return getLabelsToNodesFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto.Builder addLabelsToNodesBuilder(
          int index) {
        return getLabelsToNodesFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.LabelsToNodeIdsProto labelsToNodes = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto.Builder> 
           getLabelsToNodesBuilderList() {
        return getLabelsToNodesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto, org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProtoOrBuilder> 
          getLabelsToNodesFieldBuilder() {
        if (labelsToNodesBuilder_ == null) {
          labelsToNodesBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto, org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.LabelsToNodeIdsProtoOrBuilder>(
                  labelsToNodes_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          labelsToNodes_ = null;
        }
        return labelsToNodesBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetLabelsToNodesResponseProto)
    }

    static {
      defaultInstance = new GetLabelsToNodesResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetLabelsToNodesResponseProto)
  }

  public interface GetClusterNodeLabelsRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetClusterNodeLabelsRequestProto}
   */
  public static final class GetClusterNodeLabelsRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetClusterNodeLabelsRequestProtoOrBuilder {
    // Use GetClusterNodeLabelsRequestProto.newBuilder() to construct.
    private GetClusterNodeLabelsRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetClusterNodeLabelsRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetClusterNodeLabelsRequestProto defaultInstance;
    public static GetClusterNodeLabelsRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetClusterNodeLabelsRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetClusterNodeLabelsRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodeLabelsRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodeLabelsRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetClusterNodeLabelsRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetClusterNodeLabelsRequestProto>() {
      public GetClusterNodeLabelsRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetClusterNodeLabelsRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetClusterNodeLabelsRequestProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetClusterNodeLabelsRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodeLabelsRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodeLabelsRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodeLabelsRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetClusterNodeLabelsRequestProto)
    }

    static {
      defaultInstance = new GetClusterNodeLabelsRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetClusterNodeLabelsRequestProto)
  }

  public interface GetClusterNodeLabelsResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated string deprecatedNodeLabels = 1;
    /**
     * <code>repeated string deprecatedNodeLabels = 1;</code>
     */
    java.util.List<java.lang.String>
    getDeprecatedNodeLabelsList();
    /**
     * <code>repeated string deprecatedNodeLabels = 1;</code>
     */
    int getDeprecatedNodeLabelsCount();
    /**
     * <code>repeated string deprecatedNodeLabels = 1;</code>
     */
    java.lang.String getDeprecatedNodeLabels(int index);
    /**
     * <code>repeated string deprecatedNodeLabels = 1;</code>
     */
    com.google.protobuf.ByteString
        getDeprecatedNodeLabelsBytes(int index);

    // repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;
    /**
     * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto> 
        getNodeLabelsList();
    /**
     * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto getNodeLabels(int index);
    /**
     * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
     */
    int getNodeLabelsCount();
    /**
     * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProtoOrBuilder> 
        getNodeLabelsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProtoOrBuilder getNodeLabelsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetClusterNodeLabelsResponseProto}
   */
  public static final class GetClusterNodeLabelsResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetClusterNodeLabelsResponseProtoOrBuilder {
    // Use GetClusterNodeLabelsResponseProto.newBuilder() to construct.
    private GetClusterNodeLabelsResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetClusterNodeLabelsResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetClusterNodeLabelsResponseProto defaultInstance;
    public static GetClusterNodeLabelsResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetClusterNodeLabelsResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetClusterNodeLabelsResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                deprecatedNodeLabels_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000001;
              }
              deprecatedNodeLabels_.add(input.readBytes());
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                nodeLabels_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto>();
                mutable_bitField0_ |= 0x00000002;
              }
              nodeLabels_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          deprecatedNodeLabels_ = new com.google.protobuf.UnmodifiableLazyStringList(deprecatedNodeLabels_);
        }
        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
          nodeLabels_ = java.util.Collections.unmodifiableList(nodeLabels_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodeLabelsResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodeLabelsResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetClusterNodeLabelsResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetClusterNodeLabelsResponseProto>() {
      public GetClusterNodeLabelsResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetClusterNodeLabelsResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetClusterNodeLabelsResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated string deprecatedNodeLabels = 1;
    public static final int DEPRECATEDNODELABELS_FIELD_NUMBER = 1;
    private com.google.protobuf.LazyStringList deprecatedNodeLabels_;
    /**
     * <code>repeated string deprecatedNodeLabels = 1;</code>
     */
    public java.util.List<java.lang.String>
        getDeprecatedNodeLabelsList() {
      return deprecatedNodeLabels_;
    }
    /**
     * <code>repeated string deprecatedNodeLabels = 1;</code>
     */
    public int getDeprecatedNodeLabelsCount() {
      return deprecatedNodeLabels_.size();
    }
    /**
     * <code>repeated string deprecatedNodeLabels = 1;</code>
     */
    public java.lang.String getDeprecatedNodeLabels(int index) {
      return deprecatedNodeLabels_.get(index);
    }
    /**
     * <code>repeated string deprecatedNodeLabels = 1;</code>
     */
    public com.google.protobuf.ByteString
        getDeprecatedNodeLabelsBytes(int index) {
      return deprecatedNodeLabels_.getByteString(index);
    }

    // repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;
    public static final int NODELABELS_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto> nodeLabels_;
    /**
     * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto> getNodeLabelsList() {
      return nodeLabels_;
    }
    /**
     * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProtoOrBuilder> 
        getNodeLabelsOrBuilderList() {
      return nodeLabels_;
    }
    /**
     * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
     */
    public int getNodeLabelsCount() {
      return nodeLabels_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto getNodeLabels(int index) {
      return nodeLabels_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProtoOrBuilder getNodeLabelsOrBuilder(
        int index) {
      return nodeLabels_.get(index);
    }

    private void initFields() {
      deprecatedNodeLabels_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      nodeLabels_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < deprecatedNodeLabels_.size(); i++) {
        output.writeBytes(1, deprecatedNodeLabels_.getByteString(i));
      }
      for (int i = 0; i < nodeLabels_.size(); i++) {
        output.writeMessage(2, nodeLabels_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        for (int i = 0; i < deprecatedNodeLabels_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeBytesSizeNoTag(deprecatedNodeLabels_.getByteString(i));
        }
        size += dataSize;
        size += 1 * getDeprecatedNodeLabelsList().size();
      }
      for (int i = 0; i < nodeLabels_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, nodeLabels_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto) obj;

      boolean result = true;
      result = result && getDeprecatedNodeLabelsList()
          .equals(other.getDeprecatedNodeLabelsList());
      result = result && getNodeLabelsList()
          .equals(other.getNodeLabelsList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getDeprecatedNodeLabelsCount() > 0) {
        hash = (37 * hash) + DEPRECATEDNODELABELS_FIELD_NUMBER;
        hash = (53 * hash) + getDeprecatedNodeLabelsList().hashCode();
      }
      if (getNodeLabelsCount() > 0) {
        hash = (37 * hash) + NODELABELS_FIELD_NUMBER;
        hash = (53 * hash) + getNodeLabelsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetClusterNodeLabelsResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodeLabelsResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodeLabelsResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getNodeLabelsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        deprecatedNodeLabels_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        if (nodeLabelsBuilder_ == null) {
          nodeLabels_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          nodeLabelsBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetClusterNodeLabelsResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          deprecatedNodeLabels_ = new com.google.protobuf.UnmodifiableLazyStringList(
              deprecatedNodeLabels_);
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.deprecatedNodeLabels_ = deprecatedNodeLabels_;
        if (nodeLabelsBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002)) {
            nodeLabels_ = java.util.Collections.unmodifiableList(nodeLabels_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.nodeLabels_ = nodeLabels_;
        } else {
          result.nodeLabels_ = nodeLabelsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto.getDefaultInstance()) return this;
        if (!other.deprecatedNodeLabels_.isEmpty()) {
          if (deprecatedNodeLabels_.isEmpty()) {
            deprecatedNodeLabels_ = other.deprecatedNodeLabels_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureDeprecatedNodeLabelsIsMutable();
            deprecatedNodeLabels_.addAll(other.deprecatedNodeLabels_);
          }
          onChanged();
        }
        if (nodeLabelsBuilder_ == null) {
          if (!other.nodeLabels_.isEmpty()) {
            if (nodeLabels_.isEmpty()) {
              nodeLabels_ = other.nodeLabels_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureNodeLabelsIsMutable();
              nodeLabels_.addAll(other.nodeLabels_);
            }
            onChanged();
          }
        } else {
          if (!other.nodeLabels_.isEmpty()) {
            if (nodeLabelsBuilder_.isEmpty()) {
              nodeLabelsBuilder_.dispose();
              nodeLabelsBuilder_ = null;
              nodeLabels_ = other.nodeLabels_;
              bitField0_ = (bitField0_ & ~0x00000002);
              nodeLabelsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getNodeLabelsFieldBuilder() : null;
            } else {
              nodeLabelsBuilder_.addAllMessages(other.nodeLabels_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetClusterNodeLabelsResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated string deprecatedNodeLabels = 1;
      private com.google.protobuf.LazyStringList deprecatedNodeLabels_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureDeprecatedNodeLabelsIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          deprecatedNodeLabels_ = new com.google.protobuf.LazyStringArrayList(deprecatedNodeLabels_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <code>repeated string deprecatedNodeLabels = 1;</code>
       */
      public java.util.List<java.lang.String>
          getDeprecatedNodeLabelsList() {
        return java.util.Collections.unmodifiableList(deprecatedNodeLabels_);
      }
      /**
       * <code>repeated string deprecatedNodeLabels = 1;</code>
       */
      public int getDeprecatedNodeLabelsCount() {
        return deprecatedNodeLabels_.size();
      }
      /**
       * <code>repeated string deprecatedNodeLabels = 1;</code>
       */
      public java.lang.String getDeprecatedNodeLabels(int index) {
        return deprecatedNodeLabels_.get(index);
      }
      /**
       * <code>repeated string deprecatedNodeLabels = 1;</code>
       */
      public com.google.protobuf.ByteString
          getDeprecatedNodeLabelsBytes(int index) {
        return deprecatedNodeLabels_.getByteString(index);
      }
      /**
       * <code>repeated string deprecatedNodeLabels = 1;</code>
       */
      public Builder setDeprecatedNodeLabels(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureDeprecatedNodeLabelsIsMutable();
        deprecatedNodeLabels_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string deprecatedNodeLabels = 1;</code>
       */
      public Builder addDeprecatedNodeLabels(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureDeprecatedNodeLabelsIsMutable();
        deprecatedNodeLabels_.add(value);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string deprecatedNodeLabels = 1;</code>
       */
      public Builder addAllDeprecatedNodeLabels(
          java.lang.Iterable<java.lang.String> values) {
        ensureDeprecatedNodeLabelsIsMutable();
        super.addAll(values, deprecatedNodeLabels_);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string deprecatedNodeLabels = 1;</code>
       */
      public Builder clearDeprecatedNodeLabels() {
        deprecatedNodeLabels_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }
      /**
       * <code>repeated string deprecatedNodeLabels = 1;</code>
       */
      public Builder addDeprecatedNodeLabelsBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureDeprecatedNodeLabelsIsMutable();
        deprecatedNodeLabels_.add(value);
        onChanged();
        return this;
      }

      // repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto> nodeLabels_ =
        java.util.Collections.emptyList();
      private void ensureNodeLabelsIsMutable() {
        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
          nodeLabels_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto>(nodeLabels_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProtoOrBuilder> nodeLabelsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto> getNodeLabelsList() {
        if (nodeLabelsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(nodeLabels_);
        } else {
          return nodeLabelsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
       */
      public int getNodeLabelsCount() {
        if (nodeLabelsBuilder_ == null) {
          return nodeLabels_.size();
        } else {
          return nodeLabelsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto getNodeLabels(int index) {
        if (nodeLabelsBuilder_ == null) {
          return nodeLabels_.get(index);
        } else {
          return nodeLabelsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
       */
      public Builder setNodeLabels(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto value) {
        if (nodeLabelsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodeLabelsIsMutable();
          nodeLabels_.set(index, value);
          onChanged();
        } else {
          nodeLabelsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
       */
      public Builder setNodeLabels(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.Builder builderForValue) {
        if (nodeLabelsBuilder_ == null) {
          ensureNodeLabelsIsMutable();
          nodeLabels_.set(index, builderForValue.build());
          onChanged();
        } else {
          nodeLabelsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
       */
      public Builder addNodeLabels(org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto value) {
        if (nodeLabelsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodeLabelsIsMutable();
          nodeLabels_.add(value);
          onChanged();
        } else {
          nodeLabelsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
       */
      public Builder addNodeLabels(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto value) {
        if (nodeLabelsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureNodeLabelsIsMutable();
          nodeLabels_.add(index, value);
          onChanged();
        } else {
          nodeLabelsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
       */
      public Builder addNodeLabels(
          org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.Builder builderForValue) {
        if (nodeLabelsBuilder_ == null) {
          ensureNodeLabelsIsMutable();
          nodeLabels_.add(builderForValue.build());
          onChanged();
        } else {
          nodeLabelsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
       */
      public Builder addNodeLabels(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.Builder builderForValue) {
        if (nodeLabelsBuilder_ == null) {
          ensureNodeLabelsIsMutable();
          nodeLabels_.add(index, builderForValue.build());
          onChanged();
        } else {
          nodeLabelsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
       */
      public Builder addAllNodeLabels(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto> values) {
        if (nodeLabelsBuilder_ == null) {
          ensureNodeLabelsIsMutable();
          super.addAll(values, nodeLabels_);
          onChanged();
        } else {
          nodeLabelsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
       */
      public Builder clearNodeLabels() {
        if (nodeLabelsBuilder_ == null) {
          nodeLabels_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          nodeLabelsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
       */
      public Builder removeNodeLabels(int index) {
        if (nodeLabelsBuilder_ == null) {
          ensureNodeLabelsIsMutable();
          nodeLabels_.remove(index);
          onChanged();
        } else {
          nodeLabelsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.Builder getNodeLabelsBuilder(
          int index) {
        return getNodeLabelsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProtoOrBuilder getNodeLabelsOrBuilder(
          int index) {
        if (nodeLabelsBuilder_ == null) {
          return nodeLabels_.get(index);  } else {
          return nodeLabelsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProtoOrBuilder> 
           getNodeLabelsOrBuilderList() {
        if (nodeLabelsBuilder_ != null) {
          return nodeLabelsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(nodeLabels_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.Builder addNodeLabelsBuilder() {
        return getNodeLabelsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.Builder addNodeLabelsBuilder(
          int index) {
        return getNodeLabelsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.NodeLabelProto nodeLabels = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.Builder> 
           getNodeLabelsBuilderList() {
        return getNodeLabelsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProtoOrBuilder> 
          getNodeLabelsFieldBuilder() {
        if (nodeLabelsBuilder_ == null) {
          nodeLabelsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto, org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.NodeLabelProtoOrBuilder>(
                  nodeLabels_,
                  ((bitField0_ & 0x00000002) == 0x00000002),
                  getParentForChildren(),
                  isClean());
          nodeLabels_ = null;
        }
        return nodeLabelsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetClusterNodeLabelsResponseProto)
    }

    static {
      defaultInstance = new GetClusterNodeLabelsResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetClusterNodeLabelsResponseProto)
  }

  public interface UpdateApplicationPriorityRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required .hadoop.yarn.ApplicationIdProto applicationId = 1;
    /**
     * <code>required .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
     */
    boolean hasApplicationId();
    /**
     * <code>required .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId();
    /**
     * <code>required .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder();

    // required .hadoop.yarn.PriorityProto applicationPriority = 2;
    /**
     * <code>required .hadoop.yarn.PriorityProto applicationPriority = 2;</code>
     */
    boolean hasApplicationPriority();
    /**
     * <code>required .hadoop.yarn.PriorityProto applicationPriority = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto getApplicationPriority();
    /**
     * <code>required .hadoop.yarn.PriorityProto applicationPriority = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder getApplicationPriorityOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.UpdateApplicationPriorityRequestProto}
   */
  public static final class UpdateApplicationPriorityRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements UpdateApplicationPriorityRequestProtoOrBuilder {
    // Use UpdateApplicationPriorityRequestProto.newBuilder() to construct.
    private UpdateApplicationPriorityRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private UpdateApplicationPriorityRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final UpdateApplicationPriorityRequestProto defaultInstance;
    public static UpdateApplicationPriorityRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public UpdateApplicationPriorityRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private UpdateApplicationPriorityRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = applicationId_.toBuilder();
              }
              applicationId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationId_);
                applicationId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) == 0x00000002)) {
                subBuilder = applicationPriority_.toBuilder();
              }
              applicationPriority_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationPriority_);
                applicationPriority_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdateApplicationPriorityRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdateApplicationPriorityRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<UpdateApplicationPriorityRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<UpdateApplicationPriorityRequestProto>() {
      public UpdateApplicationPriorityRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new UpdateApplicationPriorityRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<UpdateApplicationPriorityRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required .hadoop.yarn.ApplicationIdProto applicationId = 1;
    public static final int APPLICATIONID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto applicationId_;
    /**
     * <code>required .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
     */
    public boolean hasApplicationId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId() {
      return applicationId_;
    }
    /**
     * <code>required .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder() {
      return applicationId_;
    }

    // required .hadoop.yarn.PriorityProto applicationPriority = 2;
    public static final int APPLICATIONPRIORITY_FIELD_NUMBER = 2;
    private org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto applicationPriority_;
    /**
     * <code>required .hadoop.yarn.PriorityProto applicationPriority = 2;</code>
     */
    public boolean hasApplicationPriority() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required .hadoop.yarn.PriorityProto applicationPriority = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto getApplicationPriority() {
      return applicationPriority_;
    }
    /**
     * <code>required .hadoop.yarn.PriorityProto applicationPriority = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder getApplicationPriorityOrBuilder() {
      return applicationPriority_;
    }

    private void initFields() {
      applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
      applicationPriority_ = org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasApplicationId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasApplicationPriority()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, applicationId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeMessage(2, applicationPriority_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, applicationId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, applicationPriority_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto) obj;

      boolean result = true;
      result = result && (hasApplicationId() == other.hasApplicationId());
      if (hasApplicationId()) {
        result = result && getApplicationId()
            .equals(other.getApplicationId());
      }
      result = result && (hasApplicationPriority() == other.hasApplicationPriority());
      if (hasApplicationPriority()) {
        result = result && getApplicationPriority()
            .equals(other.getApplicationPriority());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasApplicationId()) {
        hash = (37 * hash) + APPLICATIONID_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationId().hashCode();
      }
      if (hasApplicationPriority()) {
        hash = (37 * hash) + APPLICATIONPRIORITY_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationPriority().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.UpdateApplicationPriorityRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdateApplicationPriorityRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdateApplicationPriorityRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getApplicationIdFieldBuilder();
          getApplicationPriorityFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (applicationIdBuilder_ == null) {
          applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
        } else {
          applicationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (applicationPriorityBuilder_ == null) {
          applicationPriority_ = org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance();
        } else {
          applicationPriorityBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdateApplicationPriorityRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (applicationIdBuilder_ == null) {
          result.applicationId_ = applicationId_;
        } else {
          result.applicationId_ = applicationIdBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        if (applicationPriorityBuilder_ == null) {
          result.applicationPriority_ = applicationPriority_;
        } else {
          result.applicationPriority_ = applicationPriorityBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto.getDefaultInstance()) return this;
        if (other.hasApplicationId()) {
          mergeApplicationId(other.getApplicationId());
        }
        if (other.hasApplicationPriority()) {
          mergeApplicationPriority(other.getApplicationPriority());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasApplicationId()) {
          
          return false;
        }
        if (!hasApplicationPriority()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required .hadoop.yarn.ApplicationIdProto applicationId = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> applicationIdBuilder_;
      /**
       * <code>required .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      public boolean hasApplicationId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId() {
        if (applicationIdBuilder_ == null) {
          return applicationId_;
        } else {
          return applicationIdBuilder_.getMessage();
        }
      }
      /**
       * <code>required .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      public Builder setApplicationId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationId_ = value;
          onChanged();
        } else {
          applicationIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      public Builder setApplicationId(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder builderForValue) {
        if (applicationIdBuilder_ == null) {
          applicationId_ = builderForValue.build();
          onChanged();
        } else {
          applicationIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      public Builder mergeApplicationId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              applicationId_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance()) {
            applicationId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.newBuilder(applicationId_).mergeFrom(value).buildPartial();
          } else {
            applicationId_ = value;
          }
          onChanged();
        } else {
          applicationIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      public Builder clearApplicationId() {
        if (applicationIdBuilder_ == null) {
          applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
          onChanged();
        } else {
          applicationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>required .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder getApplicationIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getApplicationIdFieldBuilder().getBuilder();
      }
      /**
       * <code>required .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder() {
        if (applicationIdBuilder_ != null) {
          return applicationIdBuilder_.getMessageOrBuilder();
        } else {
          return applicationId_;
        }
      }
      /**
       * <code>required .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> 
          getApplicationIdFieldBuilder() {
        if (applicationIdBuilder_ == null) {
          applicationIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder>(
                  applicationId_,
                  getParentForChildren(),
                  isClean());
          applicationId_ = null;
        }
        return applicationIdBuilder_;
      }

      // required .hadoop.yarn.PriorityProto applicationPriority = 2;
      private org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto applicationPriority_ = org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder> applicationPriorityBuilder_;
      /**
       * <code>required .hadoop.yarn.PriorityProto applicationPriority = 2;</code>
       */
      public boolean hasApplicationPriority() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required .hadoop.yarn.PriorityProto applicationPriority = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto getApplicationPriority() {
        if (applicationPriorityBuilder_ == null) {
          return applicationPriority_;
        } else {
          return applicationPriorityBuilder_.getMessage();
        }
      }
      /**
       * <code>required .hadoop.yarn.PriorityProto applicationPriority = 2;</code>
       */
      public Builder setApplicationPriority(org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto value) {
        if (applicationPriorityBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationPriority_ = value;
          onChanged();
        } else {
          applicationPriorityBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .hadoop.yarn.PriorityProto applicationPriority = 2;</code>
       */
      public Builder setApplicationPriority(
          org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder builderForValue) {
        if (applicationPriorityBuilder_ == null) {
          applicationPriority_ = builderForValue.build();
          onChanged();
        } else {
          applicationPriorityBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .hadoop.yarn.PriorityProto applicationPriority = 2;</code>
       */
      public Builder mergeApplicationPriority(org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto value) {
        if (applicationPriorityBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002) &&
              applicationPriority_ != org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance()) {
            applicationPriority_ =
              org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.newBuilder(applicationPriority_).mergeFrom(value).buildPartial();
          } else {
            applicationPriority_ = value;
          }
          onChanged();
        } else {
          applicationPriorityBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>required .hadoop.yarn.PriorityProto applicationPriority = 2;</code>
       */
      public Builder clearApplicationPriority() {
        if (applicationPriorityBuilder_ == null) {
          applicationPriority_ = org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance();
          onChanged();
        } else {
          applicationPriorityBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>required .hadoop.yarn.PriorityProto applicationPriority = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder getApplicationPriorityBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getApplicationPriorityFieldBuilder().getBuilder();
      }
      /**
       * <code>required .hadoop.yarn.PriorityProto applicationPriority = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder getApplicationPriorityOrBuilder() {
        if (applicationPriorityBuilder_ != null) {
          return applicationPriorityBuilder_.getMessageOrBuilder();
        } else {
          return applicationPriority_;
        }
      }
      /**
       * <code>required .hadoop.yarn.PriorityProto applicationPriority = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder> 
          getApplicationPriorityFieldBuilder() {
        if (applicationPriorityBuilder_ == null) {
          applicationPriorityBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder>(
                  applicationPriority_,
                  getParentForChildren(),
                  isClean());
          applicationPriority_ = null;
        }
        return applicationPriorityBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.UpdateApplicationPriorityRequestProto)
    }

    static {
      defaultInstance = new UpdateApplicationPriorityRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.UpdateApplicationPriorityRequestProto)
  }

  public interface UpdateApplicationPriorityResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.PriorityProto applicationPriority = 1;
    /**
     * <code>optional .hadoop.yarn.PriorityProto applicationPriority = 1;</code>
     */
    boolean hasApplicationPriority();
    /**
     * <code>optional .hadoop.yarn.PriorityProto applicationPriority = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto getApplicationPriority();
    /**
     * <code>optional .hadoop.yarn.PriorityProto applicationPriority = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder getApplicationPriorityOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.UpdateApplicationPriorityResponseProto}
   */
  public static final class UpdateApplicationPriorityResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements UpdateApplicationPriorityResponseProtoOrBuilder {
    // Use UpdateApplicationPriorityResponseProto.newBuilder() to construct.
    private UpdateApplicationPriorityResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private UpdateApplicationPriorityResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final UpdateApplicationPriorityResponseProto defaultInstance;
    public static UpdateApplicationPriorityResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public UpdateApplicationPriorityResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private UpdateApplicationPriorityResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = applicationPriority_.toBuilder();
              }
              applicationPriority_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationPriority_);
                applicationPriority_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdateApplicationPriorityResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdateApplicationPriorityResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<UpdateApplicationPriorityResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<UpdateApplicationPriorityResponseProto>() {
      public UpdateApplicationPriorityResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new UpdateApplicationPriorityResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<UpdateApplicationPriorityResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.PriorityProto applicationPriority = 1;
    public static final int APPLICATIONPRIORITY_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto applicationPriority_;
    /**
     * <code>optional .hadoop.yarn.PriorityProto applicationPriority = 1;</code>
     */
    public boolean hasApplicationPriority() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.PriorityProto applicationPriority = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto getApplicationPriority() {
      return applicationPriority_;
    }
    /**
     * <code>optional .hadoop.yarn.PriorityProto applicationPriority = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder getApplicationPriorityOrBuilder() {
      return applicationPriority_;
    }

    private void initFields() {
      applicationPriority_ = org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, applicationPriority_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, applicationPriority_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto) obj;

      boolean result = true;
      result = result && (hasApplicationPriority() == other.hasApplicationPriority());
      if (hasApplicationPriority()) {
        result = result && getApplicationPriority()
            .equals(other.getApplicationPriority());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasApplicationPriority()) {
        hash = (37 * hash) + APPLICATIONPRIORITY_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationPriority().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.UpdateApplicationPriorityResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdateApplicationPriorityResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdateApplicationPriorityResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getApplicationPriorityFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (applicationPriorityBuilder_ == null) {
          applicationPriority_ = org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance();
        } else {
          applicationPriorityBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UpdateApplicationPriorityResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (applicationPriorityBuilder_ == null) {
          result.applicationPriority_ = applicationPriority_;
        } else {
          result.applicationPriority_ = applicationPriorityBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto.getDefaultInstance()) return this;
        if (other.hasApplicationPriority()) {
          mergeApplicationPriority(other.getApplicationPriority());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.UpdateApplicationPriorityResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.PriorityProto applicationPriority = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto applicationPriority_ = org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder> applicationPriorityBuilder_;
      /**
       * <code>optional .hadoop.yarn.PriorityProto applicationPriority = 1;</code>
       */
      public boolean hasApplicationPriority() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto applicationPriority = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto getApplicationPriority() {
        if (applicationPriorityBuilder_ == null) {
          return applicationPriority_;
        } else {
          return applicationPriorityBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto applicationPriority = 1;</code>
       */
      public Builder setApplicationPriority(org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto value) {
        if (applicationPriorityBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationPriority_ = value;
          onChanged();
        } else {
          applicationPriorityBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto applicationPriority = 1;</code>
       */
      public Builder setApplicationPriority(
          org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder builderForValue) {
        if (applicationPriorityBuilder_ == null) {
          applicationPriority_ = builderForValue.build();
          onChanged();
        } else {
          applicationPriorityBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto applicationPriority = 1;</code>
       */
      public Builder mergeApplicationPriority(org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto value) {
        if (applicationPriorityBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              applicationPriority_ != org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance()) {
            applicationPriority_ =
              org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.newBuilder(applicationPriority_).mergeFrom(value).buildPartial();
          } else {
            applicationPriority_ = value;
          }
          onChanged();
        } else {
          applicationPriorityBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto applicationPriority = 1;</code>
       */
      public Builder clearApplicationPriority() {
        if (applicationPriorityBuilder_ == null) {
          applicationPriority_ = org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.getDefaultInstance();
          onChanged();
        } else {
          applicationPriorityBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto applicationPriority = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder getApplicationPriorityBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getApplicationPriorityFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto applicationPriority = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder getApplicationPriorityOrBuilder() {
        if (applicationPriorityBuilder_ != null) {
          return applicationPriorityBuilder_.getMessageOrBuilder();
        } else {
          return applicationPriority_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.PriorityProto applicationPriority = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder> 
          getApplicationPriorityFieldBuilder() {
        if (applicationPriorityBuilder_ == null) {
          applicationPriorityBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.PriorityProtoOrBuilder>(
                  applicationPriority_,
                  getParentForChildren(),
                  isClean());
          applicationPriority_ = null;
        }
        return applicationPriorityBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.UpdateApplicationPriorityResponseProto)
    }

    static {
      defaultInstance = new UpdateApplicationPriorityResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.UpdateApplicationPriorityResponseProto)
  }

  public interface SignalContainerRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // required .hadoop.yarn.ContainerIdProto container_id = 1;
    /**
     * <code>required .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    boolean hasContainerId();
    /**
     * <code>required .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId();
    /**
     * <code>required .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder();

    // required .hadoop.yarn.SignalContainerCommandProto command = 2;
    /**
     * <code>required .hadoop.yarn.SignalContainerCommandProto command = 2;</code>
     */
    boolean hasCommand();
    /**
     * <code>required .hadoop.yarn.SignalContainerCommandProto command = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.SignalContainerCommandProto getCommand();
  }
  /**
   * Protobuf type {@code hadoop.yarn.SignalContainerRequestProto}
   */
  public static final class SignalContainerRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements SignalContainerRequestProtoOrBuilder {
    // Use SignalContainerRequestProto.newBuilder() to construct.
    private SignalContainerRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private SignalContainerRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final SignalContainerRequestProto defaultInstance;
    public static SignalContainerRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public SignalContainerRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private SignalContainerRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = containerId_.toBuilder();
              }
              containerId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(containerId_);
                containerId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 16: {
              int rawValue = input.readEnum();
              org.apache.hadoop.yarn.proto.YarnProtos.SignalContainerCommandProto value = org.apache.hadoop.yarn.proto.YarnProtos.SignalContainerCommandProto.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(2, rawValue);
              } else {
                bitField0_ |= 0x00000002;
                command_ = value;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SignalContainerRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SignalContainerRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<SignalContainerRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<SignalContainerRequestProto>() {
      public SignalContainerRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SignalContainerRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<SignalContainerRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // required .hadoop.yarn.ContainerIdProto container_id = 1;
    public static final int CONTAINER_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto containerId_;
    /**
     * <code>required .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public boolean hasContainerId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>required .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId() {
      return containerId_;
    }
    /**
     * <code>required .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder() {
      return containerId_;
    }

    // required .hadoop.yarn.SignalContainerCommandProto command = 2;
    public static final int COMMAND_FIELD_NUMBER = 2;
    private org.apache.hadoop.yarn.proto.YarnProtos.SignalContainerCommandProto command_;
    /**
     * <code>required .hadoop.yarn.SignalContainerCommandProto command = 2;</code>
     */
    public boolean hasCommand() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>required .hadoop.yarn.SignalContainerCommandProto command = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.SignalContainerCommandProto getCommand() {
      return command_;
    }

    private void initFields() {
      containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
      command_ = org.apache.hadoop.yarn.proto.YarnProtos.SignalContainerCommandProto.OUTPUT_THREAD_DUMP;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (!hasContainerId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasCommand()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, containerId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeEnum(2, command_.getNumber());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, containerId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(2, command_.getNumber());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto) obj;

      boolean result = true;
      result = result && (hasContainerId() == other.hasContainerId());
      if (hasContainerId()) {
        result = result && getContainerId()
            .equals(other.getContainerId());
      }
      result = result && (hasCommand() == other.hasCommand());
      if (hasCommand()) {
        result = result &&
            (getCommand() == other.getCommand());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasContainerId()) {
        hash = (37 * hash) + CONTAINER_ID_FIELD_NUMBER;
        hash = (53 * hash) + getContainerId().hashCode();
      }
      if (hasCommand()) {
        hash = (37 * hash) + COMMAND_FIELD_NUMBER;
        hash = (53 * hash) + hashEnum(getCommand());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.SignalContainerRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SignalContainerRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SignalContainerRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getContainerIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (containerIdBuilder_ == null) {
          containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
        } else {
          containerIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        command_ = org.apache.hadoop.yarn.proto.YarnProtos.SignalContainerCommandProto.OUTPUT_THREAD_DUMP;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SignalContainerRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (containerIdBuilder_ == null) {
          result.containerId_ = containerId_;
        } else {
          result.containerId_ = containerIdBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.command_ = command_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto.getDefaultInstance()) return this;
        if (other.hasContainerId()) {
          mergeContainerId(other.getContainerId());
        }
        if (other.hasCommand()) {
          setCommand(other.getCommand());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (!hasContainerId()) {
          
          return false;
        }
        if (!hasCommand()) {
          
          return false;
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // required .hadoop.yarn.ContainerIdProto container_id = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> containerIdBuilder_;
      /**
       * <code>required .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public boolean hasContainerId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>required .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId() {
        if (containerIdBuilder_ == null) {
          return containerId_;
        } else {
          return containerIdBuilder_.getMessage();
        }
      }
      /**
       * <code>required .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder setContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          containerId_ = value;
          onChanged();
        } else {
          containerIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder setContainerId(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containerIdBuilder_ == null) {
          containerId_ = builderForValue.build();
          onChanged();
        } else {
          containerIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder mergeContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              containerId_ != org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance()) {
            containerId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.newBuilder(containerId_).mergeFrom(value).buildPartial();
          } else {
            containerId_ = value;
          }
          onChanged();
        } else {
          containerIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>required .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder clearContainerId() {
        if (containerIdBuilder_ == null) {
          containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
          onChanged();
        } else {
          containerIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>required .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder getContainerIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getContainerIdFieldBuilder().getBuilder();
      }
      /**
       * <code>required .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder() {
        if (containerIdBuilder_ != null) {
          return containerIdBuilder_.getMessageOrBuilder();
        } else {
          return containerId_;
        }
      }
      /**
       * <code>required .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
          getContainerIdFieldBuilder() {
        if (containerIdBuilder_ == null) {
          containerIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder>(
                  containerId_,
                  getParentForChildren(),
                  isClean());
          containerId_ = null;
        }
        return containerIdBuilder_;
      }

      // required .hadoop.yarn.SignalContainerCommandProto command = 2;
      private org.apache.hadoop.yarn.proto.YarnProtos.SignalContainerCommandProto command_ = org.apache.hadoop.yarn.proto.YarnProtos.SignalContainerCommandProto.OUTPUT_THREAD_DUMP;
      /**
       * <code>required .hadoop.yarn.SignalContainerCommandProto command = 2;</code>
       */
      public boolean hasCommand() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>required .hadoop.yarn.SignalContainerCommandProto command = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.SignalContainerCommandProto getCommand() {
        return command_;
      }
      /**
       * <code>required .hadoop.yarn.SignalContainerCommandProto command = 2;</code>
       */
      public Builder setCommand(org.apache.hadoop.yarn.proto.YarnProtos.SignalContainerCommandProto value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000002;
        command_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>required .hadoop.yarn.SignalContainerCommandProto command = 2;</code>
       */
      public Builder clearCommand() {
        bitField0_ = (bitField0_ & ~0x00000002);
        command_ = org.apache.hadoop.yarn.proto.YarnProtos.SignalContainerCommandProto.OUTPUT_THREAD_DUMP;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.SignalContainerRequestProto)
    }

    static {
      defaultInstance = new SignalContainerRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.SignalContainerRequestProto)
  }

  public interface SignalContainerResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.yarn.SignalContainerResponseProto}
   */
  public static final class SignalContainerResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements SignalContainerResponseProtoOrBuilder {
    // Use SignalContainerResponseProto.newBuilder() to construct.
    private SignalContainerResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private SignalContainerResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final SignalContainerResponseProto defaultInstance;
    public static SignalContainerResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public SignalContainerResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private SignalContainerResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SignalContainerResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SignalContainerResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<SignalContainerResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<SignalContainerResponseProto>() {
      public SignalContainerResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new SignalContainerResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<SignalContainerResponseProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.SignalContainerResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SignalContainerResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SignalContainerResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_SignalContainerResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.SignalContainerResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.SignalContainerResponseProto)
    }

    static {
      defaultInstance = new SignalContainerResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.SignalContainerResponseProto)
  }

  public interface StartContainerRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;
    /**
     * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
     */
    boolean hasContainerLaunchContext();
    /**
     * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto getContainerLaunchContext();
    /**
     * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProtoOrBuilder getContainerLaunchContextOrBuilder();

    // optional .hadoop.common.TokenProto container_token = 2;
    /**
     * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
     */
    boolean hasContainerToken();
    /**
     * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
     */
    org.apache.hadoop.security.proto.SecurityProtos.TokenProto getContainerToken();
    /**
     * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
     */
    org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder getContainerTokenOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.StartContainerRequestProto}
   */
  public static final class StartContainerRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements StartContainerRequestProtoOrBuilder {
    // Use StartContainerRequestProto.newBuilder() to construct.
    private StartContainerRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private StartContainerRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final StartContainerRequestProto defaultInstance;
    public static StartContainerRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public StartContainerRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private StartContainerRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = containerLaunchContext_.toBuilder();
              }
              containerLaunchContext_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(containerLaunchContext_);
                containerLaunchContext_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) == 0x00000002)) {
                subBuilder = containerToken_.toBuilder();
              }
              containerToken_ = input.readMessage(org.apache.hadoop.security.proto.SecurityProtos.TokenProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(containerToken_);
                containerToken_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainerRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainerRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<StartContainerRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<StartContainerRequestProto>() {
      public StartContainerRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StartContainerRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<StartContainerRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;
    public static final int CONTAINER_LAUNCH_CONTEXT_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto containerLaunchContext_;
    /**
     * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
     */
    public boolean hasContainerLaunchContext() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto getContainerLaunchContext() {
      return containerLaunchContext_;
    }
    /**
     * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProtoOrBuilder getContainerLaunchContextOrBuilder() {
      return containerLaunchContext_;
    }

    // optional .hadoop.common.TokenProto container_token = 2;
    public static final int CONTAINER_TOKEN_FIELD_NUMBER = 2;
    private org.apache.hadoop.security.proto.SecurityProtos.TokenProto containerToken_;
    /**
     * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
     */
    public boolean hasContainerToken() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
     */
    public org.apache.hadoop.security.proto.SecurityProtos.TokenProto getContainerToken() {
      return containerToken_;
    }
    /**
     * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
     */
    public org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder getContainerTokenOrBuilder() {
      return containerToken_;
    }

    private void initFields() {
      containerLaunchContext_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.getDefaultInstance();
      containerToken_ = org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      if (hasContainerToken()) {
        if (!getContainerToken().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, containerLaunchContext_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeMessage(2, containerToken_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, containerLaunchContext_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, containerToken_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto) obj;

      boolean result = true;
      result = result && (hasContainerLaunchContext() == other.hasContainerLaunchContext());
      if (hasContainerLaunchContext()) {
        result = result && getContainerLaunchContext()
            .equals(other.getContainerLaunchContext());
      }
      result = result && (hasContainerToken() == other.hasContainerToken());
      if (hasContainerToken()) {
        result = result && getContainerToken()
            .equals(other.getContainerToken());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasContainerLaunchContext()) {
        hash = (37 * hash) + CONTAINER_LAUNCH_CONTEXT_FIELD_NUMBER;
        hash = (53 * hash) + getContainerLaunchContext().hashCode();
      }
      if (hasContainerToken()) {
        hash = (37 * hash) + CONTAINER_TOKEN_FIELD_NUMBER;
        hash = (53 * hash) + getContainerToken().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.StartContainerRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainerRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainerRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getContainerLaunchContextFieldBuilder();
          getContainerTokenFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (containerLaunchContextBuilder_ == null) {
          containerLaunchContext_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.getDefaultInstance();
        } else {
          containerLaunchContextBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (containerTokenBuilder_ == null) {
          containerToken_ = org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance();
        } else {
          containerTokenBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainerRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (containerLaunchContextBuilder_ == null) {
          result.containerLaunchContext_ = containerLaunchContext_;
        } else {
          result.containerLaunchContext_ = containerLaunchContextBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        if (containerTokenBuilder_ == null) {
          result.containerToken_ = containerToken_;
        } else {
          result.containerToken_ = containerTokenBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.getDefaultInstance()) return this;
        if (other.hasContainerLaunchContext()) {
          mergeContainerLaunchContext(other.getContainerLaunchContext());
        }
        if (other.hasContainerToken()) {
          mergeContainerToken(other.getContainerToken());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        if (hasContainerToken()) {
          if (!getContainerToken().isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto containerLaunchContext_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProtoOrBuilder> containerLaunchContextBuilder_;
      /**
       * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
       */
      public boolean hasContainerLaunchContext() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto getContainerLaunchContext() {
        if (containerLaunchContextBuilder_ == null) {
          return containerLaunchContext_;
        } else {
          return containerLaunchContextBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
       */
      public Builder setContainerLaunchContext(org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto value) {
        if (containerLaunchContextBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          containerLaunchContext_ = value;
          onChanged();
        } else {
          containerLaunchContextBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
       */
      public Builder setContainerLaunchContext(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.Builder builderForValue) {
        if (containerLaunchContextBuilder_ == null) {
          containerLaunchContext_ = builderForValue.build();
          onChanged();
        } else {
          containerLaunchContextBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
       */
      public Builder mergeContainerLaunchContext(org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto value) {
        if (containerLaunchContextBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              containerLaunchContext_ != org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.getDefaultInstance()) {
            containerLaunchContext_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.newBuilder(containerLaunchContext_).mergeFrom(value).buildPartial();
          } else {
            containerLaunchContext_ = value;
          }
          onChanged();
        } else {
          containerLaunchContextBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
       */
      public Builder clearContainerLaunchContext() {
        if (containerLaunchContextBuilder_ == null) {
          containerLaunchContext_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.getDefaultInstance();
          onChanged();
        } else {
          containerLaunchContextBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.Builder getContainerLaunchContextBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getContainerLaunchContextFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProtoOrBuilder getContainerLaunchContextOrBuilder() {
        if (containerLaunchContextBuilder_ != null) {
          return containerLaunchContextBuilder_.getMessageOrBuilder();
        } else {
          return containerLaunchContext_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerLaunchContextProto container_launch_context = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProtoOrBuilder> 
          getContainerLaunchContextFieldBuilder() {
        if (containerLaunchContextBuilder_ == null) {
          containerLaunchContextBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerLaunchContextProtoOrBuilder>(
                  containerLaunchContext_,
                  getParentForChildren(),
                  isClean());
          containerLaunchContext_ = null;
        }
        return containerLaunchContextBuilder_;
      }

      // optional .hadoop.common.TokenProto container_token = 2;
      private org.apache.hadoop.security.proto.SecurityProtos.TokenProto containerToken_ = org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.security.proto.SecurityProtos.TokenProto, org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder, org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder> containerTokenBuilder_;
      /**
       * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
       */
      public boolean hasContainerToken() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
       */
      public org.apache.hadoop.security.proto.SecurityProtos.TokenProto getContainerToken() {
        if (containerTokenBuilder_ == null) {
          return containerToken_;
        } else {
          return containerTokenBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
       */
      public Builder setContainerToken(org.apache.hadoop.security.proto.SecurityProtos.TokenProto value) {
        if (containerTokenBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          containerToken_ = value;
          onChanged();
        } else {
          containerTokenBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
       */
      public Builder setContainerToken(
          org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder builderForValue) {
        if (containerTokenBuilder_ == null) {
          containerToken_ = builderForValue.build();
          onChanged();
        } else {
          containerTokenBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
       */
      public Builder mergeContainerToken(org.apache.hadoop.security.proto.SecurityProtos.TokenProto value) {
        if (containerTokenBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002) &&
              containerToken_ != org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance()) {
            containerToken_ =
              org.apache.hadoop.security.proto.SecurityProtos.TokenProto.newBuilder(containerToken_).mergeFrom(value).buildPartial();
          } else {
            containerToken_ = value;
          }
          onChanged();
        } else {
          containerTokenBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
       */
      public Builder clearContainerToken() {
        if (containerTokenBuilder_ == null) {
          containerToken_ = org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance();
          onChanged();
        } else {
          containerTokenBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
       */
      public org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder getContainerTokenBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getContainerTokenFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
       */
      public org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder getContainerTokenOrBuilder() {
        if (containerTokenBuilder_ != null) {
          return containerTokenBuilder_.getMessageOrBuilder();
        } else {
          return containerToken_;
        }
      }
      /**
       * <code>optional .hadoop.common.TokenProto container_token = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.security.proto.SecurityProtos.TokenProto, org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder, org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder> 
          getContainerTokenFieldBuilder() {
        if (containerTokenBuilder_ == null) {
          containerTokenBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.security.proto.SecurityProtos.TokenProto, org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder, org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder>(
                  containerToken_,
                  getParentForChildren(),
                  isClean());
          containerToken_ = null;
        }
        return containerTokenBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.StartContainerRequestProto)
    }

    static {
      defaultInstance = new StartContainerRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.StartContainerRequestProto)
  }

  public interface StartContainerResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto> 
        getServicesMetaDataList();
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto getServicesMetaData(int index);
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    int getServicesMetaDataCount();
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder> 
        getServicesMetaDataOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder getServicesMetaDataOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.StartContainerResponseProto}
   */
  public static final class StartContainerResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements StartContainerResponseProtoOrBuilder {
    // Use StartContainerResponseProto.newBuilder() to construct.
    private StartContainerResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private StartContainerResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final StartContainerResponseProto defaultInstance;
    public static StartContainerResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public StartContainerResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private StartContainerResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                servicesMetaData_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              servicesMetaData_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          servicesMetaData_ = java.util.Collections.unmodifiableList(servicesMetaData_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainerResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainerResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<StartContainerResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<StartContainerResponseProto>() {
      public StartContainerResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StartContainerResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<StartContainerResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;
    public static final int SERVICES_META_DATA_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto> servicesMetaData_;
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto> getServicesMetaDataList() {
      return servicesMetaData_;
    }
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder> 
        getServicesMetaDataOrBuilderList() {
      return servicesMetaData_;
    }
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    public int getServicesMetaDataCount() {
      return servicesMetaData_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto getServicesMetaData(int index) {
      return servicesMetaData_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder getServicesMetaDataOrBuilder(
        int index) {
      return servicesMetaData_.get(index);
    }

    private void initFields() {
      servicesMetaData_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < servicesMetaData_.size(); i++) {
        output.writeMessage(1, servicesMetaData_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < servicesMetaData_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, servicesMetaData_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto) obj;

      boolean result = true;
      result = result && getServicesMetaDataList()
          .equals(other.getServicesMetaDataList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getServicesMetaDataCount() > 0) {
        hash = (37 * hash) + SERVICES_META_DATA_FIELD_NUMBER;
        hash = (53 * hash) + getServicesMetaDataList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.StartContainerResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainerResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainerResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getServicesMetaDataFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (servicesMetaDataBuilder_ == null) {
          servicesMetaData_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          servicesMetaDataBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainerResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (servicesMetaDataBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            servicesMetaData_ = java.util.Collections.unmodifiableList(servicesMetaData_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.servicesMetaData_ = servicesMetaData_;
        } else {
          result.servicesMetaData_ = servicesMetaDataBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto.getDefaultInstance()) return this;
        if (servicesMetaDataBuilder_ == null) {
          if (!other.servicesMetaData_.isEmpty()) {
            if (servicesMetaData_.isEmpty()) {
              servicesMetaData_ = other.servicesMetaData_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureServicesMetaDataIsMutable();
              servicesMetaData_.addAll(other.servicesMetaData_);
            }
            onChanged();
          }
        } else {
          if (!other.servicesMetaData_.isEmpty()) {
            if (servicesMetaDataBuilder_.isEmpty()) {
              servicesMetaDataBuilder_.dispose();
              servicesMetaDataBuilder_ = null;
              servicesMetaData_ = other.servicesMetaData_;
              bitField0_ = (bitField0_ & ~0x00000001);
              servicesMetaDataBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getServicesMetaDataFieldBuilder() : null;
            } else {
              servicesMetaDataBuilder_.addAllMessages(other.servicesMetaData_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto> servicesMetaData_ =
        java.util.Collections.emptyList();
      private void ensureServicesMetaDataIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          servicesMetaData_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto>(servicesMetaData_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder> servicesMetaDataBuilder_;

      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto> getServicesMetaDataList() {
        if (servicesMetaDataBuilder_ == null) {
          return java.util.Collections.unmodifiableList(servicesMetaData_);
        } else {
          return servicesMetaDataBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public int getServicesMetaDataCount() {
        if (servicesMetaDataBuilder_ == null) {
          return servicesMetaData_.size();
        } else {
          return servicesMetaDataBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto getServicesMetaData(int index) {
        if (servicesMetaDataBuilder_ == null) {
          return servicesMetaData_.get(index);
        } else {
          return servicesMetaDataBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder setServicesMetaData(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto value) {
        if (servicesMetaDataBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.set(index, value);
          onChanged();
        } else {
          servicesMetaDataBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder setServicesMetaData(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder builderForValue) {
        if (servicesMetaDataBuilder_ == null) {
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.set(index, builderForValue.build());
          onChanged();
        } else {
          servicesMetaDataBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder addServicesMetaData(org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto value) {
        if (servicesMetaDataBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.add(value);
          onChanged();
        } else {
          servicesMetaDataBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder addServicesMetaData(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto value) {
        if (servicesMetaDataBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.add(index, value);
          onChanged();
        } else {
          servicesMetaDataBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder addServicesMetaData(
          org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder builderForValue) {
        if (servicesMetaDataBuilder_ == null) {
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.add(builderForValue.build());
          onChanged();
        } else {
          servicesMetaDataBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder addServicesMetaData(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder builderForValue) {
        if (servicesMetaDataBuilder_ == null) {
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.add(index, builderForValue.build());
          onChanged();
        } else {
          servicesMetaDataBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder addAllServicesMetaData(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto> values) {
        if (servicesMetaDataBuilder_ == null) {
          ensureServicesMetaDataIsMutable();
          super.addAll(values, servicesMetaData_);
          onChanged();
        } else {
          servicesMetaDataBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder clearServicesMetaData() {
        if (servicesMetaDataBuilder_ == null) {
          servicesMetaData_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          servicesMetaDataBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder removeServicesMetaData(int index) {
        if (servicesMetaDataBuilder_ == null) {
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.remove(index);
          onChanged();
        } else {
          servicesMetaDataBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder getServicesMetaDataBuilder(
          int index) {
        return getServicesMetaDataFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder getServicesMetaDataOrBuilder(
          int index) {
        if (servicesMetaDataBuilder_ == null) {
          return servicesMetaData_.get(index);  } else {
          return servicesMetaDataBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder> 
           getServicesMetaDataOrBuilderList() {
        if (servicesMetaDataBuilder_ != null) {
          return servicesMetaDataBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(servicesMetaData_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder addServicesMetaDataBuilder() {
        return getServicesMetaDataFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder addServicesMetaDataBuilder(
          int index) {
        return getServicesMetaDataFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder> 
           getServicesMetaDataBuilderList() {
        return getServicesMetaDataFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder> 
          getServicesMetaDataFieldBuilder() {
        if (servicesMetaDataBuilder_ == null) {
          servicesMetaDataBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder>(
                  servicesMetaData_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          servicesMetaData_ = null;
        }
        return servicesMetaDataBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.StartContainerResponseProto)
    }

    static {
      defaultInstance = new StartContainerResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.StartContainerResponseProto)
  }

  public interface StopContainerRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ContainerIdProto container_id = 1;
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    boolean hasContainerId();
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId();
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.StopContainerRequestProto}
   */
  public static final class StopContainerRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements StopContainerRequestProtoOrBuilder {
    // Use StopContainerRequestProto.newBuilder() to construct.
    private StopContainerRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private StopContainerRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final StopContainerRequestProto defaultInstance;
    public static StopContainerRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public StopContainerRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private StopContainerRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = containerId_.toBuilder();
              }
              containerId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(containerId_);
                containerId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainerRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainerRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<StopContainerRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<StopContainerRequestProto>() {
      public StopContainerRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StopContainerRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<StopContainerRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ContainerIdProto container_id = 1;
    public static final int CONTAINER_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto containerId_;
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public boolean hasContainerId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId() {
      return containerId_;
    }
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder() {
      return containerId_;
    }

    private void initFields() {
      containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, containerId_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, containerId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto) obj;

      boolean result = true;
      result = result && (hasContainerId() == other.hasContainerId());
      if (hasContainerId()) {
        result = result && getContainerId()
            .equals(other.getContainerId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasContainerId()) {
        hash = (37 * hash) + CONTAINER_ID_FIELD_NUMBER;
        hash = (53 * hash) + getContainerId().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.StopContainerRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainerRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainerRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getContainerIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (containerIdBuilder_ == null) {
          containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
        } else {
          containerIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainerRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (containerIdBuilder_ == null) {
          result.containerId_ = containerId_;
        } else {
          result.containerId_ = containerIdBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto.getDefaultInstance()) return this;
        if (other.hasContainerId()) {
          mergeContainerId(other.getContainerId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ContainerIdProto container_id = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> containerIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public boolean hasContainerId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId() {
        if (containerIdBuilder_ == null) {
          return containerId_;
        } else {
          return containerIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder setContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          containerId_ = value;
          onChanged();
        } else {
          containerIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder setContainerId(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containerIdBuilder_ == null) {
          containerId_ = builderForValue.build();
          onChanged();
        } else {
          containerIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder mergeContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              containerId_ != org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance()) {
            containerId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.newBuilder(containerId_).mergeFrom(value).buildPartial();
          } else {
            containerId_ = value;
          }
          onChanged();
        } else {
          containerIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder clearContainerId() {
        if (containerIdBuilder_ == null) {
          containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
          onChanged();
        } else {
          containerIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder getContainerIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getContainerIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder() {
        if (containerIdBuilder_ != null) {
          return containerIdBuilder_.getMessageOrBuilder();
        } else {
          return containerId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
          getContainerIdFieldBuilder() {
        if (containerIdBuilder_ == null) {
          containerIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder>(
                  containerId_,
                  getParentForChildren(),
                  isClean());
          containerId_ = null;
        }
        return containerIdBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.StopContainerRequestProto)
    }

    static {
      defaultInstance = new StopContainerRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.StopContainerRequestProto)
  }

  public interface StopContainerResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.yarn.StopContainerResponseProto}
   */
  public static final class StopContainerResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements StopContainerResponseProtoOrBuilder {
    // Use StopContainerResponseProto.newBuilder() to construct.
    private StopContainerResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private StopContainerResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final StopContainerResponseProto defaultInstance;
    public static StopContainerResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public StopContainerResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private StopContainerResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainerResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainerResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<StopContainerResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<StopContainerResponseProto>() {
      public StopContainerResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StopContainerResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<StopContainerResponseProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.StopContainerResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainerResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainerResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainerResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainerResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.StopContainerResponseProto)
    }

    static {
      defaultInstance = new StopContainerResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.StopContainerResponseProto)
  }

  public interface StartContainersRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;
    /**
     * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto> 
        getStartContainerRequestList();
    /**
     * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto getStartContainerRequest(int index);
    /**
     * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
     */
    int getStartContainerRequestCount();
    /**
     * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProtoOrBuilder> 
        getStartContainerRequestOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProtoOrBuilder getStartContainerRequestOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.StartContainersRequestProto}
   *
   * <pre>
   *&#47;/ bulk API records
   * </pre>
   */
  public static final class StartContainersRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements StartContainersRequestProtoOrBuilder {
    // Use StartContainersRequestProto.newBuilder() to construct.
    private StartContainersRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private StartContainersRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final StartContainersRequestProto defaultInstance;
    public static StartContainersRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public StartContainersRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private StartContainersRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                startContainerRequest_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              startContainerRequest_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          startContainerRequest_ = java.util.Collections.unmodifiableList(startContainerRequest_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainersRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainersRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<StartContainersRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<StartContainersRequestProto>() {
      public StartContainersRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StartContainersRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<StartContainersRequestProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;
    public static final int START_CONTAINER_REQUEST_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto> startContainerRequest_;
    /**
     * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto> getStartContainerRequestList() {
      return startContainerRequest_;
    }
    /**
     * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProtoOrBuilder> 
        getStartContainerRequestOrBuilderList() {
      return startContainerRequest_;
    }
    /**
     * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
     */
    public int getStartContainerRequestCount() {
      return startContainerRequest_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto getStartContainerRequest(int index) {
      return startContainerRequest_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProtoOrBuilder getStartContainerRequestOrBuilder(
        int index) {
      return startContainerRequest_.get(index);
    }

    private void initFields() {
      startContainerRequest_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      for (int i = 0; i < getStartContainerRequestCount(); i++) {
        if (!getStartContainerRequest(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < startContainerRequest_.size(); i++) {
        output.writeMessage(1, startContainerRequest_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < startContainerRequest_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, startContainerRequest_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto) obj;

      boolean result = true;
      result = result && getStartContainerRequestList()
          .equals(other.getStartContainerRequestList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getStartContainerRequestCount() > 0) {
        hash = (37 * hash) + START_CONTAINER_REQUEST_FIELD_NUMBER;
        hash = (53 * hash) + getStartContainerRequestList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.StartContainersRequestProto}
     *
     * <pre>
     *&#47;/ bulk API records
     * </pre>
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainersRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainersRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getStartContainerRequestFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (startContainerRequestBuilder_ == null) {
          startContainerRequest_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          startContainerRequestBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainersRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto(this);
        int from_bitField0_ = bitField0_;
        if (startContainerRequestBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            startContainerRequest_ = java.util.Collections.unmodifiableList(startContainerRequest_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.startContainerRequest_ = startContainerRequest_;
        } else {
          result.startContainerRequest_ = startContainerRequestBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto.getDefaultInstance()) return this;
        if (startContainerRequestBuilder_ == null) {
          if (!other.startContainerRequest_.isEmpty()) {
            if (startContainerRequest_.isEmpty()) {
              startContainerRequest_ = other.startContainerRequest_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureStartContainerRequestIsMutable();
              startContainerRequest_.addAll(other.startContainerRequest_);
            }
            onChanged();
          }
        } else {
          if (!other.startContainerRequest_.isEmpty()) {
            if (startContainerRequestBuilder_.isEmpty()) {
              startContainerRequestBuilder_.dispose();
              startContainerRequestBuilder_ = null;
              startContainerRequest_ = other.startContainerRequest_;
              bitField0_ = (bitField0_ & ~0x00000001);
              startContainerRequestBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getStartContainerRequestFieldBuilder() : null;
            } else {
              startContainerRequestBuilder_.addAllMessages(other.startContainerRequest_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        for (int i = 0; i < getStartContainerRequestCount(); i++) {
          if (!getStartContainerRequest(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto> startContainerRequest_ =
        java.util.Collections.emptyList();
      private void ensureStartContainerRequestIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          startContainerRequest_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto>(startContainerRequest_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProtoOrBuilder> startContainerRequestBuilder_;

      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto> getStartContainerRequestList() {
        if (startContainerRequestBuilder_ == null) {
          return java.util.Collections.unmodifiableList(startContainerRequest_);
        } else {
          return startContainerRequestBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public int getStartContainerRequestCount() {
        if (startContainerRequestBuilder_ == null) {
          return startContainerRequest_.size();
        } else {
          return startContainerRequestBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto getStartContainerRequest(int index) {
        if (startContainerRequestBuilder_ == null) {
          return startContainerRequest_.get(index);
        } else {
          return startContainerRequestBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public Builder setStartContainerRequest(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto value) {
        if (startContainerRequestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStartContainerRequestIsMutable();
          startContainerRequest_.set(index, value);
          onChanged();
        } else {
          startContainerRequestBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public Builder setStartContainerRequest(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.Builder builderForValue) {
        if (startContainerRequestBuilder_ == null) {
          ensureStartContainerRequestIsMutable();
          startContainerRequest_.set(index, builderForValue.build());
          onChanged();
        } else {
          startContainerRequestBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public Builder addStartContainerRequest(org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto value) {
        if (startContainerRequestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStartContainerRequestIsMutable();
          startContainerRequest_.add(value);
          onChanged();
        } else {
          startContainerRequestBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public Builder addStartContainerRequest(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto value) {
        if (startContainerRequestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStartContainerRequestIsMutable();
          startContainerRequest_.add(index, value);
          onChanged();
        } else {
          startContainerRequestBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public Builder addStartContainerRequest(
          org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.Builder builderForValue) {
        if (startContainerRequestBuilder_ == null) {
          ensureStartContainerRequestIsMutable();
          startContainerRequest_.add(builderForValue.build());
          onChanged();
        } else {
          startContainerRequestBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public Builder addStartContainerRequest(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.Builder builderForValue) {
        if (startContainerRequestBuilder_ == null) {
          ensureStartContainerRequestIsMutable();
          startContainerRequest_.add(index, builderForValue.build());
          onChanged();
        } else {
          startContainerRequestBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public Builder addAllStartContainerRequest(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto> values) {
        if (startContainerRequestBuilder_ == null) {
          ensureStartContainerRequestIsMutable();
          super.addAll(values, startContainerRequest_);
          onChanged();
        } else {
          startContainerRequestBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public Builder clearStartContainerRequest() {
        if (startContainerRequestBuilder_ == null) {
          startContainerRequest_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          startContainerRequestBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public Builder removeStartContainerRequest(int index) {
        if (startContainerRequestBuilder_ == null) {
          ensureStartContainerRequestIsMutable();
          startContainerRequest_.remove(index);
          onChanged();
        } else {
          startContainerRequestBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.Builder getStartContainerRequestBuilder(
          int index) {
        return getStartContainerRequestFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProtoOrBuilder getStartContainerRequestOrBuilder(
          int index) {
        if (startContainerRequestBuilder_ == null) {
          return startContainerRequest_.get(index);  } else {
          return startContainerRequestBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProtoOrBuilder> 
           getStartContainerRequestOrBuilderList() {
        if (startContainerRequestBuilder_ != null) {
          return startContainerRequestBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(startContainerRequest_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.Builder addStartContainerRequestBuilder() {
        return getStartContainerRequestFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.Builder addStartContainerRequestBuilder(
          int index) {
        return getStartContainerRequestFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.StartContainerRequestProto start_container_request = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.Builder> 
           getStartContainerRequestBuilderList() {
        return getStartContainerRequestFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProtoOrBuilder> 
          getStartContainerRequestFieldBuilder() {
        if (startContainerRequestBuilder_ == null) {
          startContainerRequestBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainerRequestProtoOrBuilder>(
                  startContainerRequest_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          startContainerRequest_ = null;
        }
        return startContainerRequestBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.StartContainersRequestProto)
    }

    static {
      defaultInstance = new StartContainersRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.StartContainersRequestProto)
  }

  public interface ContainerExceptionMapProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ContainerIdProto container_id = 1;
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    boolean hasContainerId();
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId();
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder();

    // optional .hadoop.yarn.SerializedExceptionProto exception = 2;
    /**
     * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
     */
    boolean hasException();
    /**
     * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto getException();
    /**
     * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProtoOrBuilder getExceptionOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.ContainerExceptionMapProto}
   */
  public static final class ContainerExceptionMapProto extends
      com.google.protobuf.GeneratedMessage
      implements ContainerExceptionMapProtoOrBuilder {
    // Use ContainerExceptionMapProto.newBuilder() to construct.
    private ContainerExceptionMapProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private ContainerExceptionMapProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final ContainerExceptionMapProto defaultInstance;
    public static ContainerExceptionMapProto getDefaultInstance() {
      return defaultInstance;
    }

    public ContainerExceptionMapProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private ContainerExceptionMapProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = containerId_.toBuilder();
              }
              containerId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(containerId_);
                containerId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) == 0x00000002)) {
                subBuilder = exception_.toBuilder();
              }
              exception_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(exception_);
                exception_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ContainerExceptionMapProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ContainerExceptionMapProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder.class);
    }

    public static com.google.protobuf.Parser<ContainerExceptionMapProto> PARSER =
        new com.google.protobuf.AbstractParser<ContainerExceptionMapProto>() {
      public ContainerExceptionMapProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ContainerExceptionMapProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<ContainerExceptionMapProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ContainerIdProto container_id = 1;
    public static final int CONTAINER_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto containerId_;
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public boolean hasContainerId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId() {
      return containerId_;
    }
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder() {
      return containerId_;
    }

    // optional .hadoop.yarn.SerializedExceptionProto exception = 2;
    public static final int EXCEPTION_FIELD_NUMBER = 2;
    private org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto exception_;
    /**
     * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
     */
    public boolean hasException() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto getException() {
      return exception_;
    }
    /**
     * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProtoOrBuilder getExceptionOrBuilder() {
      return exception_;
    }

    private void initFields() {
      containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
      exception_ = org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, containerId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeMessage(2, exception_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, containerId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, exception_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto) obj;

      boolean result = true;
      result = result && (hasContainerId() == other.hasContainerId());
      if (hasContainerId()) {
        result = result && getContainerId()
            .equals(other.getContainerId());
      }
      result = result && (hasException() == other.hasException());
      if (hasException()) {
        result = result && getException()
            .equals(other.getException());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasContainerId()) {
        hash = (37 * hash) + CONTAINER_ID_FIELD_NUMBER;
        hash = (53 * hash) + getContainerId().hashCode();
      }
      if (hasException()) {
        hash = (37 * hash) + EXCEPTION_FIELD_NUMBER;
        hash = (53 * hash) + getException().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.ContainerExceptionMapProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ContainerExceptionMapProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ContainerExceptionMapProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getContainerIdFieldBuilder();
          getExceptionFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (containerIdBuilder_ == null) {
          containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
        } else {
          containerIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (exceptionBuilder_ == null) {
          exception_ = org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.getDefaultInstance();
        } else {
          exceptionBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ContainerExceptionMapProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (containerIdBuilder_ == null) {
          result.containerId_ = containerId_;
        } else {
          result.containerId_ = containerIdBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        if (exceptionBuilder_ == null) {
          result.exception_ = exception_;
        } else {
          result.exception_ = exceptionBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.getDefaultInstance()) return this;
        if (other.hasContainerId()) {
          mergeContainerId(other.getContainerId());
        }
        if (other.hasException()) {
          mergeException(other.getException());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ContainerIdProto container_id = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> containerIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public boolean hasContainerId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId() {
        if (containerIdBuilder_ == null) {
          return containerId_;
        } else {
          return containerIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder setContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          containerId_ = value;
          onChanged();
        } else {
          containerIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder setContainerId(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containerIdBuilder_ == null) {
          containerId_ = builderForValue.build();
          onChanged();
        } else {
          containerIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder mergeContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              containerId_ != org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance()) {
            containerId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.newBuilder(containerId_).mergeFrom(value).buildPartial();
          } else {
            containerId_ = value;
          }
          onChanged();
        } else {
          containerIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder clearContainerId() {
        if (containerIdBuilder_ == null) {
          containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
          onChanged();
        } else {
          containerIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder getContainerIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getContainerIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder() {
        if (containerIdBuilder_ != null) {
          return containerIdBuilder_.getMessageOrBuilder();
        } else {
          return containerId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
          getContainerIdFieldBuilder() {
        if (containerIdBuilder_ == null) {
          containerIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder>(
                  containerId_,
                  getParentForChildren(),
                  isClean());
          containerId_ = null;
        }
        return containerIdBuilder_;
      }

      // optional .hadoop.yarn.SerializedExceptionProto exception = 2;
      private org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto exception_ = org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto, org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProtoOrBuilder> exceptionBuilder_;
      /**
       * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
       */
      public boolean hasException() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto getException() {
        if (exceptionBuilder_ == null) {
          return exception_;
        } else {
          return exceptionBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
       */
      public Builder setException(org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto value) {
        if (exceptionBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          exception_ = value;
          onChanged();
        } else {
          exceptionBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
       */
      public Builder setException(
          org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.Builder builderForValue) {
        if (exceptionBuilder_ == null) {
          exception_ = builderForValue.build();
          onChanged();
        } else {
          exceptionBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
       */
      public Builder mergeException(org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto value) {
        if (exceptionBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002) &&
              exception_ != org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.getDefaultInstance()) {
            exception_ =
              org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.newBuilder(exception_).mergeFrom(value).buildPartial();
          } else {
            exception_ = value;
          }
          onChanged();
        } else {
          exceptionBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
       */
      public Builder clearException() {
        if (exceptionBuilder_ == null) {
          exception_ = org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.getDefaultInstance();
          onChanged();
        } else {
          exceptionBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.Builder getExceptionBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getExceptionFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProtoOrBuilder getExceptionOrBuilder() {
        if (exceptionBuilder_ != null) {
          return exceptionBuilder_.getMessageOrBuilder();
        } else {
          return exception_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.SerializedExceptionProto exception = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto, org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProtoOrBuilder> 
          getExceptionFieldBuilder() {
        if (exceptionBuilder_ == null) {
          exceptionBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto, org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.SerializedExceptionProtoOrBuilder>(
                  exception_,
                  getParentForChildren(),
                  isClean());
          exception_ = null;
        }
        return exceptionBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.ContainerExceptionMapProto)
    }

    static {
      defaultInstance = new ContainerExceptionMapProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.ContainerExceptionMapProto)
  }

  public interface StartContainersResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto> 
        getServicesMetaDataList();
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto getServicesMetaData(int index);
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    int getServicesMetaDataCount();
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder> 
        getServicesMetaDataOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder getServicesMetaDataOrBuilder(
        int index);

    // repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> 
        getSucceededRequestsList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getSucceededRequests(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
     */
    int getSucceededRequestsCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getSucceededRequestsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getSucceededRequestsOrBuilder(
        int index);

    // repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> 
        getFailedRequestsList();
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto getFailedRequests(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
     */
    int getFailedRequestsCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
        getFailedRequestsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder getFailedRequestsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.StartContainersResponseProto}
   */
  public static final class StartContainersResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements StartContainersResponseProtoOrBuilder {
    // Use StartContainersResponseProto.newBuilder() to construct.
    private StartContainersResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private StartContainersResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final StartContainersResponseProto defaultInstance;
    public static StartContainersResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public StartContainersResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private StartContainersResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                servicesMetaData_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              servicesMetaData_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.PARSER, extensionRegistry));
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                succeededRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>();
                mutable_bitField0_ |= 0x00000002;
              }
              succeededRequests_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.PARSER, extensionRegistry));
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
                failedRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto>();
                mutable_bitField0_ |= 0x00000004;
              }
              failedRequests_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          servicesMetaData_ = java.util.Collections.unmodifiableList(servicesMetaData_);
        }
        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
          succeededRequests_ = java.util.Collections.unmodifiableList(succeededRequests_);
        }
        if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
          failedRequests_ = java.util.Collections.unmodifiableList(failedRequests_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainersResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainersResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<StartContainersResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<StartContainersResponseProto>() {
      public StartContainersResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StartContainersResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<StartContainersResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;
    public static final int SERVICES_META_DATA_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto> servicesMetaData_;
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto> getServicesMetaDataList() {
      return servicesMetaData_;
    }
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder> 
        getServicesMetaDataOrBuilderList() {
      return servicesMetaData_;
    }
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    public int getServicesMetaDataCount() {
      return servicesMetaData_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto getServicesMetaData(int index) {
      return servicesMetaData_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder getServicesMetaDataOrBuilder(
        int index) {
      return servicesMetaData_.get(index);
    }

    // repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;
    public static final int SUCCEEDED_REQUESTS_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> succeededRequests_;
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getSucceededRequestsList() {
      return succeededRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getSucceededRequestsOrBuilderList() {
      return succeededRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
     */
    public int getSucceededRequestsCount() {
      return succeededRequests_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getSucceededRequests(int index) {
      return succeededRequests_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getSucceededRequestsOrBuilder(
        int index) {
      return succeededRequests_.get(index);
    }

    // repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;
    public static final int FAILED_REQUESTS_FIELD_NUMBER = 3;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> failedRequests_;
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> getFailedRequestsList() {
      return failedRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
        getFailedRequestsOrBuilderList() {
      return failedRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
     */
    public int getFailedRequestsCount() {
      return failedRequests_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto getFailedRequests(int index) {
      return failedRequests_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder getFailedRequestsOrBuilder(
        int index) {
      return failedRequests_.get(index);
    }

    private void initFields() {
      servicesMetaData_ = java.util.Collections.emptyList();
      succeededRequests_ = java.util.Collections.emptyList();
      failedRequests_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < servicesMetaData_.size(); i++) {
        output.writeMessage(1, servicesMetaData_.get(i));
      }
      for (int i = 0; i < succeededRequests_.size(); i++) {
        output.writeMessage(2, succeededRequests_.get(i));
      }
      for (int i = 0; i < failedRequests_.size(); i++) {
        output.writeMessage(3, failedRequests_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < servicesMetaData_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, servicesMetaData_.get(i));
      }
      for (int i = 0; i < succeededRequests_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, succeededRequests_.get(i));
      }
      for (int i = 0; i < failedRequests_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, failedRequests_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto) obj;

      boolean result = true;
      result = result && getServicesMetaDataList()
          .equals(other.getServicesMetaDataList());
      result = result && getSucceededRequestsList()
          .equals(other.getSucceededRequestsList());
      result = result && getFailedRequestsList()
          .equals(other.getFailedRequestsList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getServicesMetaDataCount() > 0) {
        hash = (37 * hash) + SERVICES_META_DATA_FIELD_NUMBER;
        hash = (53 * hash) + getServicesMetaDataList().hashCode();
      }
      if (getSucceededRequestsCount() > 0) {
        hash = (37 * hash) + SUCCEEDED_REQUESTS_FIELD_NUMBER;
        hash = (53 * hash) + getSucceededRequestsList().hashCode();
      }
      if (getFailedRequestsCount() > 0) {
        hash = (37 * hash) + FAILED_REQUESTS_FIELD_NUMBER;
        hash = (53 * hash) + getFailedRequestsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.StartContainersResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainersResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainersResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getServicesMetaDataFieldBuilder();
          getSucceededRequestsFieldBuilder();
          getFailedRequestsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (servicesMetaDataBuilder_ == null) {
          servicesMetaData_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          servicesMetaDataBuilder_.clear();
        }
        if (succeededRequestsBuilder_ == null) {
          succeededRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          succeededRequestsBuilder_.clear();
        }
        if (failedRequestsBuilder_ == null) {
          failedRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
        } else {
          failedRequestsBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StartContainersResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (servicesMetaDataBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            servicesMetaData_ = java.util.Collections.unmodifiableList(servicesMetaData_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.servicesMetaData_ = servicesMetaData_;
        } else {
          result.servicesMetaData_ = servicesMetaDataBuilder_.build();
        }
        if (succeededRequestsBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002)) {
            succeededRequests_ = java.util.Collections.unmodifiableList(succeededRequests_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.succeededRequests_ = succeededRequests_;
        } else {
          result.succeededRequests_ = succeededRequestsBuilder_.build();
        }
        if (failedRequestsBuilder_ == null) {
          if (((bitField0_ & 0x00000004) == 0x00000004)) {
            failedRequests_ = java.util.Collections.unmodifiableList(failedRequests_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.failedRequests_ = failedRequests_;
        } else {
          result.failedRequests_ = failedRequestsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto.getDefaultInstance()) return this;
        if (servicesMetaDataBuilder_ == null) {
          if (!other.servicesMetaData_.isEmpty()) {
            if (servicesMetaData_.isEmpty()) {
              servicesMetaData_ = other.servicesMetaData_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureServicesMetaDataIsMutable();
              servicesMetaData_.addAll(other.servicesMetaData_);
            }
            onChanged();
          }
        } else {
          if (!other.servicesMetaData_.isEmpty()) {
            if (servicesMetaDataBuilder_.isEmpty()) {
              servicesMetaDataBuilder_.dispose();
              servicesMetaDataBuilder_ = null;
              servicesMetaData_ = other.servicesMetaData_;
              bitField0_ = (bitField0_ & ~0x00000001);
              servicesMetaDataBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getServicesMetaDataFieldBuilder() : null;
            } else {
              servicesMetaDataBuilder_.addAllMessages(other.servicesMetaData_);
            }
          }
        }
        if (succeededRequestsBuilder_ == null) {
          if (!other.succeededRequests_.isEmpty()) {
            if (succeededRequests_.isEmpty()) {
              succeededRequests_ = other.succeededRequests_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureSucceededRequestsIsMutable();
              succeededRequests_.addAll(other.succeededRequests_);
            }
            onChanged();
          }
        } else {
          if (!other.succeededRequests_.isEmpty()) {
            if (succeededRequestsBuilder_.isEmpty()) {
              succeededRequestsBuilder_.dispose();
              succeededRequestsBuilder_ = null;
              succeededRequests_ = other.succeededRequests_;
              bitField0_ = (bitField0_ & ~0x00000002);
              succeededRequestsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getSucceededRequestsFieldBuilder() : null;
            } else {
              succeededRequestsBuilder_.addAllMessages(other.succeededRequests_);
            }
          }
        }
        if (failedRequestsBuilder_ == null) {
          if (!other.failedRequests_.isEmpty()) {
            if (failedRequests_.isEmpty()) {
              failedRequests_ = other.failedRequests_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureFailedRequestsIsMutable();
              failedRequests_.addAll(other.failedRequests_);
            }
            onChanged();
          }
        } else {
          if (!other.failedRequests_.isEmpty()) {
            if (failedRequestsBuilder_.isEmpty()) {
              failedRequestsBuilder_.dispose();
              failedRequestsBuilder_ = null;
              failedRequests_ = other.failedRequests_;
              bitField0_ = (bitField0_ & ~0x00000004);
              failedRequestsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getFailedRequestsFieldBuilder() : null;
            } else {
              failedRequestsBuilder_.addAllMessages(other.failedRequests_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StartContainersResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto> servicesMetaData_ =
        java.util.Collections.emptyList();
      private void ensureServicesMetaDataIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          servicesMetaData_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto>(servicesMetaData_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder> servicesMetaDataBuilder_;

      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto> getServicesMetaDataList() {
        if (servicesMetaDataBuilder_ == null) {
          return java.util.Collections.unmodifiableList(servicesMetaData_);
        } else {
          return servicesMetaDataBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public int getServicesMetaDataCount() {
        if (servicesMetaDataBuilder_ == null) {
          return servicesMetaData_.size();
        } else {
          return servicesMetaDataBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto getServicesMetaData(int index) {
        if (servicesMetaDataBuilder_ == null) {
          return servicesMetaData_.get(index);
        } else {
          return servicesMetaDataBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder setServicesMetaData(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto value) {
        if (servicesMetaDataBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.set(index, value);
          onChanged();
        } else {
          servicesMetaDataBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder setServicesMetaData(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder builderForValue) {
        if (servicesMetaDataBuilder_ == null) {
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.set(index, builderForValue.build());
          onChanged();
        } else {
          servicesMetaDataBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder addServicesMetaData(org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto value) {
        if (servicesMetaDataBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.add(value);
          onChanged();
        } else {
          servicesMetaDataBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder addServicesMetaData(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto value) {
        if (servicesMetaDataBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.add(index, value);
          onChanged();
        } else {
          servicesMetaDataBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder addServicesMetaData(
          org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder builderForValue) {
        if (servicesMetaDataBuilder_ == null) {
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.add(builderForValue.build());
          onChanged();
        } else {
          servicesMetaDataBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder addServicesMetaData(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder builderForValue) {
        if (servicesMetaDataBuilder_ == null) {
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.add(index, builderForValue.build());
          onChanged();
        } else {
          servicesMetaDataBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder addAllServicesMetaData(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto> values) {
        if (servicesMetaDataBuilder_ == null) {
          ensureServicesMetaDataIsMutable();
          super.addAll(values, servicesMetaData_);
          onChanged();
        } else {
          servicesMetaDataBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder clearServicesMetaData() {
        if (servicesMetaDataBuilder_ == null) {
          servicesMetaData_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          servicesMetaDataBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public Builder removeServicesMetaData(int index) {
        if (servicesMetaDataBuilder_ == null) {
          ensureServicesMetaDataIsMutable();
          servicesMetaData_.remove(index);
          onChanged();
        } else {
          servicesMetaDataBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder getServicesMetaDataBuilder(
          int index) {
        return getServicesMetaDataFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder getServicesMetaDataOrBuilder(
          int index) {
        if (servicesMetaDataBuilder_ == null) {
          return servicesMetaData_.get(index);  } else {
          return servicesMetaDataBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder> 
           getServicesMetaDataOrBuilderList() {
        if (servicesMetaDataBuilder_ != null) {
          return servicesMetaDataBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(servicesMetaData_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder addServicesMetaDataBuilder() {
        return getServicesMetaDataFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder addServicesMetaDataBuilder(
          int index) {
        return getServicesMetaDataFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.StringBytesMapProto services_meta_data = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder> 
           getServicesMetaDataBuilderList() {
        return getServicesMetaDataFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder> 
          getServicesMetaDataFieldBuilder() {
        if (servicesMetaDataBuilder_ == null) {
          servicesMetaDataBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.StringBytesMapProtoOrBuilder>(
                  servicesMetaData_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          servicesMetaData_ = null;
        }
        return servicesMetaDataBuilder_;
      }

      // repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> succeededRequests_ =
        java.util.Collections.emptyList();
      private void ensureSucceededRequestsIsMutable() {
        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
          succeededRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>(succeededRequests_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> succeededRequestsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getSucceededRequestsList() {
        if (succeededRequestsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(succeededRequests_);
        } else {
          return succeededRequestsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public int getSucceededRequestsCount() {
        if (succeededRequestsBuilder_ == null) {
          return succeededRequests_.size();
        } else {
          return succeededRequestsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getSucceededRequests(int index) {
        if (succeededRequestsBuilder_ == null) {
          return succeededRequests_.get(index);
        } else {
          return succeededRequestsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public Builder setSucceededRequests(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (succeededRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSucceededRequestsIsMutable();
          succeededRequests_.set(index, value);
          onChanged();
        } else {
          succeededRequestsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public Builder setSucceededRequests(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          succeededRequests_.set(index, builderForValue.build());
          onChanged();
        } else {
          succeededRequestsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public Builder addSucceededRequests(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (succeededRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSucceededRequestsIsMutable();
          succeededRequests_.add(value);
          onChanged();
        } else {
          succeededRequestsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public Builder addSucceededRequests(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (succeededRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSucceededRequestsIsMutable();
          succeededRequests_.add(index, value);
          onChanged();
        } else {
          succeededRequestsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public Builder addSucceededRequests(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          succeededRequests_.add(builderForValue.build());
          onChanged();
        } else {
          succeededRequestsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public Builder addSucceededRequests(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          succeededRequests_.add(index, builderForValue.build());
          onChanged();
        } else {
          succeededRequestsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public Builder addAllSucceededRequests(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> values) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          super.addAll(values, succeededRequests_);
          onChanged();
        } else {
          succeededRequestsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public Builder clearSucceededRequests() {
        if (succeededRequestsBuilder_ == null) {
          succeededRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          succeededRequestsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public Builder removeSucceededRequests(int index) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          succeededRequests_.remove(index);
          onChanged();
        } else {
          succeededRequestsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder getSucceededRequestsBuilder(
          int index) {
        return getSucceededRequestsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getSucceededRequestsOrBuilder(
          int index) {
        if (succeededRequestsBuilder_ == null) {
          return succeededRequests_.get(index);  } else {
          return succeededRequestsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
           getSucceededRequestsOrBuilderList() {
        if (succeededRequestsBuilder_ != null) {
          return succeededRequestsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(succeededRequests_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addSucceededRequestsBuilder() {
        return getSucceededRequestsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addSucceededRequestsBuilder(
          int index) {
        return getSucceededRequestsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder> 
           getSucceededRequestsBuilderList() {
        return getSucceededRequestsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
          getSucceededRequestsFieldBuilder() {
        if (succeededRequestsBuilder_ == null) {
          succeededRequestsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder>(
                  succeededRequests_,
                  ((bitField0_ & 0x00000002) == 0x00000002),
                  getParentForChildren(),
                  isClean());
          succeededRequests_ = null;
        }
        return succeededRequestsBuilder_;
      }

      // repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> failedRequests_ =
        java.util.Collections.emptyList();
      private void ensureFailedRequestsIsMutable() {
        if (!((bitField0_ & 0x00000004) == 0x00000004)) {
          failedRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto>(failedRequests_);
          bitField0_ |= 0x00000004;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> failedRequestsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> getFailedRequestsList() {
        if (failedRequestsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(failedRequests_);
        } else {
          return failedRequestsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public int getFailedRequestsCount() {
        if (failedRequestsBuilder_ == null) {
          return failedRequests_.size();
        } else {
          return failedRequestsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto getFailedRequests(int index) {
        if (failedRequestsBuilder_ == null) {
          return failedRequests_.get(index);
        } else {
          return failedRequestsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public Builder setFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto value) {
        if (failedRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureFailedRequestsIsMutable();
          failedRequests_.set(index, value);
          onChanged();
        } else {
          failedRequestsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public Builder setFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder builderForValue) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.set(index, builderForValue.build());
          onChanged();
        } else {
          failedRequestsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public Builder addFailedRequests(org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto value) {
        if (failedRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureFailedRequestsIsMutable();
          failedRequests_.add(value);
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public Builder addFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto value) {
        if (failedRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureFailedRequestsIsMutable();
          failedRequests_.add(index, value);
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public Builder addFailedRequests(
          org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder builderForValue) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.add(builderForValue.build());
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public Builder addFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder builderForValue) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.add(index, builderForValue.build());
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public Builder addAllFailedRequests(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> values) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          super.addAll(values, failedRequests_);
          onChanged();
        } else {
          failedRequestsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public Builder clearFailedRequests() {
        if (failedRequestsBuilder_ == null) {
          failedRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
        } else {
          failedRequestsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public Builder removeFailedRequests(int index) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.remove(index);
          onChanged();
        } else {
          failedRequestsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder getFailedRequestsBuilder(
          int index) {
        return getFailedRequestsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder getFailedRequestsOrBuilder(
          int index) {
        if (failedRequestsBuilder_ == null) {
          return failedRequests_.get(index);  } else {
          return failedRequestsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
           getFailedRequestsOrBuilderList() {
        if (failedRequestsBuilder_ != null) {
          return failedRequestsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(failedRequests_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder addFailedRequestsBuilder() {
        return getFailedRequestsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder addFailedRequestsBuilder(
          int index) {
        return getFailedRequestsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 3;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder> 
           getFailedRequestsBuilderList() {
        return getFailedRequestsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
          getFailedRequestsFieldBuilder() {
        if (failedRequestsBuilder_ == null) {
          failedRequestsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder>(
                  failedRequests_,
                  ((bitField0_ & 0x00000004) == 0x00000004),
                  getParentForChildren(),
                  isClean());
          failedRequests_ = null;
        }
        return failedRequestsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.StartContainersResponseProto)
    }

    static {
      defaultInstance = new StartContainersResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.StartContainersResponseProto)
  }

  public interface StopContainersRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.ContainerIdProto container_id = 1;
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> 
        getContainerIdList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    int getContainerIdCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getContainerIdOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.StopContainersRequestProto}
   */
  public static final class StopContainersRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements StopContainersRequestProtoOrBuilder {
    // Use StopContainersRequestProto.newBuilder() to construct.
    private StopContainersRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private StopContainersRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final StopContainersRequestProto defaultInstance;
    public static StopContainersRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public StopContainersRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private StopContainersRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                containerId_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              containerId_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          containerId_ = java.util.Collections.unmodifiableList(containerId_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainersRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainersRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<StopContainersRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<StopContainersRequestProto>() {
      public StopContainersRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StopContainersRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<StopContainersRequestProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.ContainerIdProto container_id = 1;
    public static final int CONTAINER_ID_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> containerId_;
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getContainerIdList() {
      return containerId_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getContainerIdOrBuilderList() {
      return containerId_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public int getContainerIdCount() {
      return containerId_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId(int index) {
      return containerId_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder(
        int index) {
      return containerId_.get(index);
    }

    private void initFields() {
      containerId_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < containerId_.size(); i++) {
        output.writeMessage(1, containerId_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < containerId_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, containerId_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto) obj;

      boolean result = true;
      result = result && getContainerIdList()
          .equals(other.getContainerIdList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getContainerIdCount() > 0) {
        hash = (37 * hash) + CONTAINER_ID_FIELD_NUMBER;
        hash = (53 * hash) + getContainerIdList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.StopContainersRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainersRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainersRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getContainerIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (containerIdBuilder_ == null) {
          containerId_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          containerIdBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainersRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto(this);
        int from_bitField0_ = bitField0_;
        if (containerIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            containerId_ = java.util.Collections.unmodifiableList(containerId_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.containerId_ = containerId_;
        } else {
          result.containerId_ = containerIdBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto.getDefaultInstance()) return this;
        if (containerIdBuilder_ == null) {
          if (!other.containerId_.isEmpty()) {
            if (containerId_.isEmpty()) {
              containerId_ = other.containerId_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureContainerIdIsMutable();
              containerId_.addAll(other.containerId_);
            }
            onChanged();
          }
        } else {
          if (!other.containerId_.isEmpty()) {
            if (containerIdBuilder_.isEmpty()) {
              containerIdBuilder_.dispose();
              containerIdBuilder_ = null;
              containerId_ = other.containerId_;
              bitField0_ = (bitField0_ & ~0x00000001);
              containerIdBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getContainerIdFieldBuilder() : null;
            } else {
              containerIdBuilder_.addAllMessages(other.containerId_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.ContainerIdProto container_id = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> containerId_ =
        java.util.Collections.emptyList();
      private void ensureContainerIdIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          containerId_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>(containerId_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> containerIdBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getContainerIdList() {
        if (containerIdBuilder_ == null) {
          return java.util.Collections.unmodifiableList(containerId_);
        } else {
          return containerIdBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public int getContainerIdCount() {
        if (containerIdBuilder_ == null) {
          return containerId_.size();
        } else {
          return containerIdBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId(int index) {
        if (containerIdBuilder_ == null) {
          return containerId_.get(index);
        } else {
          return containerIdBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder setContainerId(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainerIdIsMutable();
          containerId_.set(index, value);
          onChanged();
        } else {
          containerIdBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder setContainerId(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containerIdBuilder_ == null) {
          ensureContainerIdIsMutable();
          containerId_.set(index, builderForValue.build());
          onChanged();
        } else {
          containerIdBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder addContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainerIdIsMutable();
          containerId_.add(value);
          onChanged();
        } else {
          containerIdBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder addContainerId(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainerIdIsMutable();
          containerId_.add(index, value);
          onChanged();
        } else {
          containerIdBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder addContainerId(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containerIdBuilder_ == null) {
          ensureContainerIdIsMutable();
          containerId_.add(builderForValue.build());
          onChanged();
        } else {
          containerIdBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder addContainerId(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containerIdBuilder_ == null) {
          ensureContainerIdIsMutable();
          containerId_.add(index, builderForValue.build());
          onChanged();
        } else {
          containerIdBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder addAllContainerId(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> values) {
        if (containerIdBuilder_ == null) {
          ensureContainerIdIsMutable();
          super.addAll(values, containerId_);
          onChanged();
        } else {
          containerIdBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder clearContainerId() {
        if (containerIdBuilder_ == null) {
          containerId_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          containerIdBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder removeContainerId(int index) {
        if (containerIdBuilder_ == null) {
          ensureContainerIdIsMutable();
          containerId_.remove(index);
          onChanged();
        } else {
          containerIdBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder getContainerIdBuilder(
          int index) {
        return getContainerIdFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder(
          int index) {
        if (containerIdBuilder_ == null) {
          return containerId_.get(index);  } else {
          return containerIdBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
           getContainerIdOrBuilderList() {
        if (containerIdBuilder_ != null) {
          return containerIdBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(containerId_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addContainerIdBuilder() {
        return getContainerIdFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addContainerIdBuilder(
          int index) {
        return getContainerIdFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder> 
           getContainerIdBuilderList() {
        return getContainerIdFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
          getContainerIdFieldBuilder() {
        if (containerIdBuilder_ == null) {
          containerIdBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder>(
                  containerId_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          containerId_ = null;
        }
        return containerIdBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.StopContainersRequestProto)
    }

    static {
      defaultInstance = new StopContainersRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.StopContainersRequestProto)
  }

  public interface StopContainersResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> 
        getSucceededRequestsList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getSucceededRequests(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    int getSucceededRequestsCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getSucceededRequestsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getSucceededRequestsOrBuilder(
        int index);

    // repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> 
        getFailedRequestsList();
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto getFailedRequests(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    int getFailedRequestsCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
        getFailedRequestsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder getFailedRequestsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.StopContainersResponseProto}
   */
  public static final class StopContainersResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements StopContainersResponseProtoOrBuilder {
    // Use StopContainersResponseProto.newBuilder() to construct.
    private StopContainersResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private StopContainersResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final StopContainersResponseProto defaultInstance;
    public static StopContainersResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public StopContainersResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private StopContainersResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                succeededRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              succeededRequests_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.PARSER, extensionRegistry));
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                failedRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto>();
                mutable_bitField0_ |= 0x00000002;
              }
              failedRequests_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          succeededRequests_ = java.util.Collections.unmodifiableList(succeededRequests_);
        }
        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
          failedRequests_ = java.util.Collections.unmodifiableList(failedRequests_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainersResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainersResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<StopContainersResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<StopContainersResponseProto>() {
      public StopContainersResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new StopContainersResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<StopContainersResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;
    public static final int SUCCEEDED_REQUESTS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> succeededRequests_;
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getSucceededRequestsList() {
      return succeededRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getSucceededRequestsOrBuilderList() {
      return succeededRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    public int getSucceededRequestsCount() {
      return succeededRequests_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getSucceededRequests(int index) {
      return succeededRequests_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getSucceededRequestsOrBuilder(
        int index) {
      return succeededRequests_.get(index);
    }

    // repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;
    public static final int FAILED_REQUESTS_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> failedRequests_;
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> getFailedRequestsList() {
      return failedRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
        getFailedRequestsOrBuilderList() {
      return failedRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public int getFailedRequestsCount() {
      return failedRequests_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto getFailedRequests(int index) {
      return failedRequests_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder getFailedRequestsOrBuilder(
        int index) {
      return failedRequests_.get(index);
    }

    private void initFields() {
      succeededRequests_ = java.util.Collections.emptyList();
      failedRequests_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < succeededRequests_.size(); i++) {
        output.writeMessage(1, succeededRequests_.get(i));
      }
      for (int i = 0; i < failedRequests_.size(); i++) {
        output.writeMessage(2, failedRequests_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < succeededRequests_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, succeededRequests_.get(i));
      }
      for (int i = 0; i < failedRequests_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, failedRequests_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto) obj;

      boolean result = true;
      result = result && getSucceededRequestsList()
          .equals(other.getSucceededRequestsList());
      result = result && getFailedRequestsList()
          .equals(other.getFailedRequestsList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getSucceededRequestsCount() > 0) {
        hash = (37 * hash) + SUCCEEDED_REQUESTS_FIELD_NUMBER;
        hash = (53 * hash) + getSucceededRequestsList().hashCode();
      }
      if (getFailedRequestsCount() > 0) {
        hash = (37 * hash) + FAILED_REQUESTS_FIELD_NUMBER;
        hash = (53 * hash) + getFailedRequestsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.StopContainersResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainersResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainersResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getSucceededRequestsFieldBuilder();
          getFailedRequestsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (succeededRequestsBuilder_ == null) {
          succeededRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          succeededRequestsBuilder_.clear();
        }
        if (failedRequestsBuilder_ == null) {
          failedRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          failedRequestsBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_StopContainersResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (succeededRequestsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            succeededRequests_ = java.util.Collections.unmodifiableList(succeededRequests_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.succeededRequests_ = succeededRequests_;
        } else {
          result.succeededRequests_ = succeededRequestsBuilder_.build();
        }
        if (failedRequestsBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002)) {
            failedRequests_ = java.util.Collections.unmodifiableList(failedRequests_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.failedRequests_ = failedRequests_;
        } else {
          result.failedRequests_ = failedRequestsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto.getDefaultInstance()) return this;
        if (succeededRequestsBuilder_ == null) {
          if (!other.succeededRequests_.isEmpty()) {
            if (succeededRequests_.isEmpty()) {
              succeededRequests_ = other.succeededRequests_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureSucceededRequestsIsMutable();
              succeededRequests_.addAll(other.succeededRequests_);
            }
            onChanged();
          }
        } else {
          if (!other.succeededRequests_.isEmpty()) {
            if (succeededRequestsBuilder_.isEmpty()) {
              succeededRequestsBuilder_.dispose();
              succeededRequestsBuilder_ = null;
              succeededRequests_ = other.succeededRequests_;
              bitField0_ = (bitField0_ & ~0x00000001);
              succeededRequestsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getSucceededRequestsFieldBuilder() : null;
            } else {
              succeededRequestsBuilder_.addAllMessages(other.succeededRequests_);
            }
          }
        }
        if (failedRequestsBuilder_ == null) {
          if (!other.failedRequests_.isEmpty()) {
            if (failedRequests_.isEmpty()) {
              failedRequests_ = other.failedRequests_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureFailedRequestsIsMutable();
              failedRequests_.addAll(other.failedRequests_);
            }
            onChanged();
          }
        } else {
          if (!other.failedRequests_.isEmpty()) {
            if (failedRequestsBuilder_.isEmpty()) {
              failedRequestsBuilder_.dispose();
              failedRequestsBuilder_ = null;
              failedRequests_ = other.failedRequests_;
              bitField0_ = (bitField0_ & ~0x00000002);
              failedRequestsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getFailedRequestsFieldBuilder() : null;
            } else {
              failedRequestsBuilder_.addAllMessages(other.failedRequests_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.StopContainersResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> succeededRequests_ =
        java.util.Collections.emptyList();
      private void ensureSucceededRequestsIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          succeededRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>(succeededRequests_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> succeededRequestsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getSucceededRequestsList() {
        if (succeededRequestsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(succeededRequests_);
        } else {
          return succeededRequestsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public int getSucceededRequestsCount() {
        if (succeededRequestsBuilder_ == null) {
          return succeededRequests_.size();
        } else {
          return succeededRequestsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getSucceededRequests(int index) {
        if (succeededRequestsBuilder_ == null) {
          return succeededRequests_.get(index);
        } else {
          return succeededRequestsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder setSucceededRequests(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (succeededRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSucceededRequestsIsMutable();
          succeededRequests_.set(index, value);
          onChanged();
        } else {
          succeededRequestsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder setSucceededRequests(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          succeededRequests_.set(index, builderForValue.build());
          onChanged();
        } else {
          succeededRequestsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder addSucceededRequests(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (succeededRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSucceededRequestsIsMutable();
          succeededRequests_.add(value);
          onChanged();
        } else {
          succeededRequestsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder addSucceededRequests(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (succeededRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSucceededRequestsIsMutable();
          succeededRequests_.add(index, value);
          onChanged();
        } else {
          succeededRequestsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder addSucceededRequests(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          succeededRequests_.add(builderForValue.build());
          onChanged();
        } else {
          succeededRequestsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder addSucceededRequests(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          succeededRequests_.add(index, builderForValue.build());
          onChanged();
        } else {
          succeededRequestsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder addAllSucceededRequests(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> values) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          super.addAll(values, succeededRequests_);
          onChanged();
        } else {
          succeededRequestsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder clearSucceededRequests() {
        if (succeededRequestsBuilder_ == null) {
          succeededRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          succeededRequestsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder removeSucceededRequests(int index) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          succeededRequests_.remove(index);
          onChanged();
        } else {
          succeededRequestsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder getSucceededRequestsBuilder(
          int index) {
        return getSucceededRequestsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getSucceededRequestsOrBuilder(
          int index) {
        if (succeededRequestsBuilder_ == null) {
          return succeededRequests_.get(index);  } else {
          return succeededRequestsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
           getSucceededRequestsOrBuilderList() {
        if (succeededRequestsBuilder_ != null) {
          return succeededRequestsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(succeededRequests_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addSucceededRequestsBuilder() {
        return getSucceededRequestsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addSucceededRequestsBuilder(
          int index) {
        return getSucceededRequestsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder> 
           getSucceededRequestsBuilderList() {
        return getSucceededRequestsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
          getSucceededRequestsFieldBuilder() {
        if (succeededRequestsBuilder_ == null) {
          succeededRequestsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder>(
                  succeededRequests_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          succeededRequests_ = null;
        }
        return succeededRequestsBuilder_;
      }

      // repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> failedRequests_ =
        java.util.Collections.emptyList();
      private void ensureFailedRequestsIsMutable() {
        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
          failedRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto>(failedRequests_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> failedRequestsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> getFailedRequestsList() {
        if (failedRequestsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(failedRequests_);
        } else {
          return failedRequestsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public int getFailedRequestsCount() {
        if (failedRequestsBuilder_ == null) {
          return failedRequests_.size();
        } else {
          return failedRequestsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto getFailedRequests(int index) {
        if (failedRequestsBuilder_ == null) {
          return failedRequests_.get(index);
        } else {
          return failedRequestsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder setFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto value) {
        if (failedRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureFailedRequestsIsMutable();
          failedRequests_.set(index, value);
          onChanged();
        } else {
          failedRequestsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder setFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder builderForValue) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.set(index, builderForValue.build());
          onChanged();
        } else {
          failedRequestsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addFailedRequests(org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto value) {
        if (failedRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureFailedRequestsIsMutable();
          failedRequests_.add(value);
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto value) {
        if (failedRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureFailedRequestsIsMutable();
          failedRequests_.add(index, value);
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addFailedRequests(
          org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder builderForValue) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.add(builderForValue.build());
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder builderForValue) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.add(index, builderForValue.build());
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addAllFailedRequests(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> values) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          super.addAll(values, failedRequests_);
          onChanged();
        } else {
          failedRequestsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder clearFailedRequests() {
        if (failedRequestsBuilder_ == null) {
          failedRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          failedRequestsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder removeFailedRequests(int index) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.remove(index);
          onChanged();
        } else {
          failedRequestsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder getFailedRequestsBuilder(
          int index) {
        return getFailedRequestsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder getFailedRequestsOrBuilder(
          int index) {
        if (failedRequestsBuilder_ == null) {
          return failedRequests_.get(index);  } else {
          return failedRequestsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
           getFailedRequestsOrBuilderList() {
        if (failedRequestsBuilder_ != null) {
          return failedRequestsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(failedRequests_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder addFailedRequestsBuilder() {
        return getFailedRequestsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder addFailedRequestsBuilder(
          int index) {
        return getFailedRequestsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder> 
           getFailedRequestsBuilderList() {
        return getFailedRequestsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
          getFailedRequestsFieldBuilder() {
        if (failedRequestsBuilder_ == null) {
          failedRequestsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder>(
                  failedRequests_,
                  ((bitField0_ & 0x00000002) == 0x00000002),
                  getParentForChildren(),
                  isClean());
          failedRequests_ = null;
        }
        return failedRequestsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.StopContainersResponseProto)
    }

    static {
      defaultInstance = new StopContainersResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.StopContainersResponseProto)
  }

  public interface GetContainerStatusesRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.ContainerIdProto container_id = 1;
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> 
        getContainerIdList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    int getContainerIdCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getContainerIdOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetContainerStatusesRequestProto}
   */
  public static final class GetContainerStatusesRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetContainerStatusesRequestProtoOrBuilder {
    // Use GetContainerStatusesRequestProto.newBuilder() to construct.
    private GetContainerStatusesRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetContainerStatusesRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetContainerStatusesRequestProto defaultInstance;
    public static GetContainerStatusesRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetContainerStatusesRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetContainerStatusesRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                containerId_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              containerId_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          containerId_ = java.util.Collections.unmodifiableList(containerId_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusesRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusesRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetContainerStatusesRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetContainerStatusesRequestProto>() {
      public GetContainerStatusesRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetContainerStatusesRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetContainerStatusesRequestProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.ContainerIdProto container_id = 1;
    public static final int CONTAINER_ID_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> containerId_;
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getContainerIdList() {
      return containerId_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getContainerIdOrBuilderList() {
      return containerId_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public int getContainerIdCount() {
      return containerId_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId(int index) {
      return containerId_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder(
        int index) {
      return containerId_.get(index);
    }

    private void initFields() {
      containerId_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < containerId_.size(); i++) {
        output.writeMessage(1, containerId_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < containerId_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, containerId_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto) obj;

      boolean result = true;
      result = result && getContainerIdList()
          .equals(other.getContainerIdList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getContainerIdCount() > 0) {
        hash = (37 * hash) + CONTAINER_ID_FIELD_NUMBER;
        hash = (53 * hash) + getContainerIdList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetContainerStatusesRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusesRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusesRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getContainerIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (containerIdBuilder_ == null) {
          containerId_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          containerIdBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusesRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto(this);
        int from_bitField0_ = bitField0_;
        if (containerIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            containerId_ = java.util.Collections.unmodifiableList(containerId_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.containerId_ = containerId_;
        } else {
          result.containerId_ = containerIdBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto.getDefaultInstance()) return this;
        if (containerIdBuilder_ == null) {
          if (!other.containerId_.isEmpty()) {
            if (containerId_.isEmpty()) {
              containerId_ = other.containerId_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureContainerIdIsMutable();
              containerId_.addAll(other.containerId_);
            }
            onChanged();
          }
        } else {
          if (!other.containerId_.isEmpty()) {
            if (containerIdBuilder_.isEmpty()) {
              containerIdBuilder_.dispose();
              containerIdBuilder_ = null;
              containerId_ = other.containerId_;
              bitField0_ = (bitField0_ & ~0x00000001);
              containerIdBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getContainerIdFieldBuilder() : null;
            } else {
              containerIdBuilder_.addAllMessages(other.containerId_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.ContainerIdProto container_id = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> containerId_ =
        java.util.Collections.emptyList();
      private void ensureContainerIdIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          containerId_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>(containerId_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> containerIdBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getContainerIdList() {
        if (containerIdBuilder_ == null) {
          return java.util.Collections.unmodifiableList(containerId_);
        } else {
          return containerIdBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public int getContainerIdCount() {
        if (containerIdBuilder_ == null) {
          return containerId_.size();
        } else {
          return containerIdBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId(int index) {
        if (containerIdBuilder_ == null) {
          return containerId_.get(index);
        } else {
          return containerIdBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder setContainerId(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainerIdIsMutable();
          containerId_.set(index, value);
          onChanged();
        } else {
          containerIdBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder setContainerId(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containerIdBuilder_ == null) {
          ensureContainerIdIsMutable();
          containerId_.set(index, builderForValue.build());
          onChanged();
        } else {
          containerIdBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder addContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainerIdIsMutable();
          containerId_.add(value);
          onChanged();
        } else {
          containerIdBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder addContainerId(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainerIdIsMutable();
          containerId_.add(index, value);
          onChanged();
        } else {
          containerIdBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder addContainerId(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containerIdBuilder_ == null) {
          ensureContainerIdIsMutable();
          containerId_.add(builderForValue.build());
          onChanged();
        } else {
          containerIdBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder addContainerId(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containerIdBuilder_ == null) {
          ensureContainerIdIsMutable();
          containerId_.add(index, builderForValue.build());
          onChanged();
        } else {
          containerIdBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder addAllContainerId(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> values) {
        if (containerIdBuilder_ == null) {
          ensureContainerIdIsMutable();
          super.addAll(values, containerId_);
          onChanged();
        } else {
          containerIdBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder clearContainerId() {
        if (containerIdBuilder_ == null) {
          containerId_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          containerIdBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder removeContainerId(int index) {
        if (containerIdBuilder_ == null) {
          ensureContainerIdIsMutable();
          containerId_.remove(index);
          onChanged();
        } else {
          containerIdBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder getContainerIdBuilder(
          int index) {
        return getContainerIdFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder(
          int index) {
        if (containerIdBuilder_ == null) {
          return containerId_.get(index);  } else {
          return containerIdBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
           getContainerIdOrBuilderList() {
        if (containerIdBuilder_ != null) {
          return containerIdBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(containerId_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addContainerIdBuilder() {
        return getContainerIdFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addContainerIdBuilder(
          int index) {
        return getContainerIdFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder> 
           getContainerIdBuilderList() {
        return getContainerIdFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
          getContainerIdFieldBuilder() {
        if (containerIdBuilder_ == null) {
          containerIdBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder>(
                  containerId_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          containerId_ = null;
        }
        return containerIdBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetContainerStatusesRequestProto)
    }

    static {
      defaultInstance = new GetContainerStatusesRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetContainerStatusesRequestProto)
  }

  public interface GetContainerStatusesResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.ContainerStatusProto status = 1;
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto> 
        getStatusList();
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto getStatus(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    int getStatusCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder> 
        getStatusOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder getStatusOrBuilder(
        int index);

    // repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> 
        getFailedRequestsList();
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto getFailedRequests(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    int getFailedRequestsCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
        getFailedRequestsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder getFailedRequestsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetContainerStatusesResponseProto}
   */
  public static final class GetContainerStatusesResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetContainerStatusesResponseProtoOrBuilder {
    // Use GetContainerStatusesResponseProto.newBuilder() to construct.
    private GetContainerStatusesResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetContainerStatusesResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetContainerStatusesResponseProto defaultInstance;
    public static GetContainerStatusesResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetContainerStatusesResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetContainerStatusesResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                status_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              status_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.PARSER, extensionRegistry));
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                failedRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto>();
                mutable_bitField0_ |= 0x00000002;
              }
              failedRequests_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          status_ = java.util.Collections.unmodifiableList(status_);
        }
        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
          failedRequests_ = java.util.Collections.unmodifiableList(failedRequests_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusesResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusesResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetContainerStatusesResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetContainerStatusesResponseProto>() {
      public GetContainerStatusesResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetContainerStatusesResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetContainerStatusesResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.ContainerStatusProto status = 1;
    public static final int STATUS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto> status_;
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto> getStatusList() {
      return status_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder> 
        getStatusOrBuilderList() {
      return status_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    public int getStatusCount() {
      return status_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto getStatus(int index) {
      return status_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder getStatusOrBuilder(
        int index) {
      return status_.get(index);
    }

    // repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;
    public static final int FAILED_REQUESTS_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> failedRequests_;
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> getFailedRequestsList() {
      return failedRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
        getFailedRequestsOrBuilderList() {
      return failedRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public int getFailedRequestsCount() {
      return failedRequests_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto getFailedRequests(int index) {
      return failedRequests_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder getFailedRequestsOrBuilder(
        int index) {
      return failedRequests_.get(index);
    }

    private void initFields() {
      status_ = java.util.Collections.emptyList();
      failedRequests_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < status_.size(); i++) {
        output.writeMessage(1, status_.get(i));
      }
      for (int i = 0; i < failedRequests_.size(); i++) {
        output.writeMessage(2, failedRequests_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < status_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, status_.get(i));
      }
      for (int i = 0; i < failedRequests_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, failedRequests_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto) obj;

      boolean result = true;
      result = result && getStatusList()
          .equals(other.getStatusList());
      result = result && getFailedRequestsList()
          .equals(other.getFailedRequestsList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getStatusCount() > 0) {
        hash = (37 * hash) + STATUS_FIELD_NUMBER;
        hash = (53 * hash) + getStatusList().hashCode();
      }
      if (getFailedRequestsCount() > 0) {
        hash = (37 * hash) + FAILED_REQUESTS_FIELD_NUMBER;
        hash = (53 * hash) + getFailedRequestsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetContainerStatusesResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusesResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusesResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getStatusFieldBuilder();
          getFailedRequestsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (statusBuilder_ == null) {
          status_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          statusBuilder_.clear();
        }
        if (failedRequestsBuilder_ == null) {
          failedRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          failedRequestsBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerStatusesResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (statusBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            status_ = java.util.Collections.unmodifiableList(status_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.status_ = status_;
        } else {
          result.status_ = statusBuilder_.build();
        }
        if (failedRequestsBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002)) {
            failedRequests_ = java.util.Collections.unmodifiableList(failedRequests_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.failedRequests_ = failedRequests_;
        } else {
          result.failedRequests_ = failedRequestsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto.getDefaultInstance()) return this;
        if (statusBuilder_ == null) {
          if (!other.status_.isEmpty()) {
            if (status_.isEmpty()) {
              status_ = other.status_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureStatusIsMutable();
              status_.addAll(other.status_);
            }
            onChanged();
          }
        } else {
          if (!other.status_.isEmpty()) {
            if (statusBuilder_.isEmpty()) {
              statusBuilder_.dispose();
              statusBuilder_ = null;
              status_ = other.status_;
              bitField0_ = (bitField0_ & ~0x00000001);
              statusBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getStatusFieldBuilder() : null;
            } else {
              statusBuilder_.addAllMessages(other.status_);
            }
          }
        }
        if (failedRequestsBuilder_ == null) {
          if (!other.failedRequests_.isEmpty()) {
            if (failedRequests_.isEmpty()) {
              failedRequests_ = other.failedRequests_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureFailedRequestsIsMutable();
              failedRequests_.addAll(other.failedRequests_);
            }
            onChanged();
          }
        } else {
          if (!other.failedRequests_.isEmpty()) {
            if (failedRequestsBuilder_.isEmpty()) {
              failedRequestsBuilder_.dispose();
              failedRequestsBuilder_ = null;
              failedRequests_ = other.failedRequests_;
              bitField0_ = (bitField0_ & ~0x00000002);
              failedRequestsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getFailedRequestsFieldBuilder() : null;
            } else {
              failedRequestsBuilder_.addAllMessages(other.failedRequests_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerStatusesResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.ContainerStatusProto status = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto> status_ =
        java.util.Collections.emptyList();
      private void ensureStatusIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          status_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto>(status_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder> statusBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto> getStatusList() {
        if (statusBuilder_ == null) {
          return java.util.Collections.unmodifiableList(status_);
        } else {
          return statusBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public int getStatusCount() {
        if (statusBuilder_ == null) {
          return status_.size();
        } else {
          return statusBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto getStatus(int index) {
        if (statusBuilder_ == null) {
          return status_.get(index);
        } else {
          return statusBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public Builder setStatus(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto value) {
        if (statusBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStatusIsMutable();
          status_.set(index, value);
          onChanged();
        } else {
          statusBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public Builder setStatus(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder builderForValue) {
        if (statusBuilder_ == null) {
          ensureStatusIsMutable();
          status_.set(index, builderForValue.build());
          onChanged();
        } else {
          statusBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public Builder addStatus(org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto value) {
        if (statusBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStatusIsMutable();
          status_.add(value);
          onChanged();
        } else {
          statusBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public Builder addStatus(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto value) {
        if (statusBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStatusIsMutable();
          status_.add(index, value);
          onChanged();
        } else {
          statusBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public Builder addStatus(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder builderForValue) {
        if (statusBuilder_ == null) {
          ensureStatusIsMutable();
          status_.add(builderForValue.build());
          onChanged();
        } else {
          statusBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public Builder addStatus(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder builderForValue) {
        if (statusBuilder_ == null) {
          ensureStatusIsMutable();
          status_.add(index, builderForValue.build());
          onChanged();
        } else {
          statusBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public Builder addAllStatus(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto> values) {
        if (statusBuilder_ == null) {
          ensureStatusIsMutable();
          super.addAll(values, status_);
          onChanged();
        } else {
          statusBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public Builder clearStatus() {
        if (statusBuilder_ == null) {
          status_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          statusBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public Builder removeStatus(int index) {
        if (statusBuilder_ == null) {
          ensureStatusIsMutable();
          status_.remove(index);
          onChanged();
        } else {
          statusBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder getStatusBuilder(
          int index) {
        return getStatusFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder getStatusOrBuilder(
          int index) {
        if (statusBuilder_ == null) {
          return status_.get(index);  } else {
          return statusBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder> 
           getStatusOrBuilderList() {
        if (statusBuilder_ != null) {
          return statusBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(status_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder addStatusBuilder() {
        return getStatusFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder addStatusBuilder(
          int index) {
        return getStatusFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerStatusProto status = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder> 
           getStatusBuilderList() {
        return getStatusFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder> 
          getStatusFieldBuilder() {
        if (statusBuilder_ == null) {
          statusBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerStatusProtoOrBuilder>(
                  status_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          status_ = null;
        }
        return statusBuilder_;
      }

      // repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> failedRequests_ =
        java.util.Collections.emptyList();
      private void ensureFailedRequestsIsMutable() {
        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
          failedRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto>(failedRequests_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> failedRequestsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> getFailedRequestsList() {
        if (failedRequestsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(failedRequests_);
        } else {
          return failedRequestsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public int getFailedRequestsCount() {
        if (failedRequestsBuilder_ == null) {
          return failedRequests_.size();
        } else {
          return failedRequestsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto getFailedRequests(int index) {
        if (failedRequestsBuilder_ == null) {
          return failedRequests_.get(index);
        } else {
          return failedRequestsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder setFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto value) {
        if (failedRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureFailedRequestsIsMutable();
          failedRequests_.set(index, value);
          onChanged();
        } else {
          failedRequestsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder setFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder builderForValue) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.set(index, builderForValue.build());
          onChanged();
        } else {
          failedRequestsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addFailedRequests(org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto value) {
        if (failedRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureFailedRequestsIsMutable();
          failedRequests_.add(value);
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto value) {
        if (failedRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureFailedRequestsIsMutable();
          failedRequests_.add(index, value);
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addFailedRequests(
          org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder builderForValue) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.add(builderForValue.build());
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder builderForValue) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.add(index, builderForValue.build());
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addAllFailedRequests(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> values) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          super.addAll(values, failedRequests_);
          onChanged();
        } else {
          failedRequestsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder clearFailedRequests() {
        if (failedRequestsBuilder_ == null) {
          failedRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          failedRequestsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder removeFailedRequests(int index) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.remove(index);
          onChanged();
        } else {
          failedRequestsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder getFailedRequestsBuilder(
          int index) {
        return getFailedRequestsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder getFailedRequestsOrBuilder(
          int index) {
        if (failedRequestsBuilder_ == null) {
          return failedRequests_.get(index);  } else {
          return failedRequestsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
           getFailedRequestsOrBuilderList() {
        if (failedRequestsBuilder_ != null) {
          return failedRequestsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(failedRequests_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder addFailedRequestsBuilder() {
        return getFailedRequestsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder addFailedRequestsBuilder(
          int index) {
        return getFailedRequestsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder> 
           getFailedRequestsBuilderList() {
        return getFailedRequestsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
          getFailedRequestsFieldBuilder() {
        if (failedRequestsBuilder_ == null) {
          failedRequestsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder>(
                  failedRequests_,
                  ((bitField0_ & 0x00000002) == 0x00000002),
                  getParentForChildren(),
                  isClean());
          failedRequests_ = null;
        }
        return failedRequestsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetContainerStatusesResponseProto)
    }

    static {
      defaultInstance = new GetContainerStatusesResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetContainerStatusesResponseProto)
  }

  public interface IncreaseContainersResourceRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.common.TokenProto increase_containers = 1;
    /**
     * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
     */
    java.util.List<org.apache.hadoop.security.proto.SecurityProtos.TokenProto> 
        getIncreaseContainersList();
    /**
     * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
     */
    org.apache.hadoop.security.proto.SecurityProtos.TokenProto getIncreaseContainers(int index);
    /**
     * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
     */
    int getIncreaseContainersCount();
    /**
     * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder> 
        getIncreaseContainersOrBuilderList();
    /**
     * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
     */
    org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder getIncreaseContainersOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.IncreaseContainersResourceRequestProto}
   */
  public static final class IncreaseContainersResourceRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements IncreaseContainersResourceRequestProtoOrBuilder {
    // Use IncreaseContainersResourceRequestProto.newBuilder() to construct.
    private IncreaseContainersResourceRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private IncreaseContainersResourceRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final IncreaseContainersResourceRequestProto defaultInstance;
    public static IncreaseContainersResourceRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public IncreaseContainersResourceRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private IncreaseContainersResourceRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                increaseContainers_ = new java.util.ArrayList<org.apache.hadoop.security.proto.SecurityProtos.TokenProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              increaseContainers_.add(input.readMessage(org.apache.hadoop.security.proto.SecurityProtos.TokenProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          increaseContainers_ = java.util.Collections.unmodifiableList(increaseContainers_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_IncreaseContainersResourceRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_IncreaseContainersResourceRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<IncreaseContainersResourceRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<IncreaseContainersResourceRequestProto>() {
      public IncreaseContainersResourceRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new IncreaseContainersResourceRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<IncreaseContainersResourceRequestProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.common.TokenProto increase_containers = 1;
    public static final int INCREASE_CONTAINERS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.security.proto.SecurityProtos.TokenProto> increaseContainers_;
    /**
     * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
     */
    public java.util.List<org.apache.hadoop.security.proto.SecurityProtos.TokenProto> getIncreaseContainersList() {
      return increaseContainers_;
    }
    /**
     * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder> 
        getIncreaseContainersOrBuilderList() {
      return increaseContainers_;
    }
    /**
     * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
     */
    public int getIncreaseContainersCount() {
      return increaseContainers_.size();
    }
    /**
     * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
     */
    public org.apache.hadoop.security.proto.SecurityProtos.TokenProto getIncreaseContainers(int index) {
      return increaseContainers_.get(index);
    }
    /**
     * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
     */
    public org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder getIncreaseContainersOrBuilder(
        int index) {
      return increaseContainers_.get(index);
    }

    private void initFields() {
      increaseContainers_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      for (int i = 0; i < getIncreaseContainersCount(); i++) {
        if (!getIncreaseContainers(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < increaseContainers_.size(); i++) {
        output.writeMessage(1, increaseContainers_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < increaseContainers_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, increaseContainers_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto) obj;

      boolean result = true;
      result = result && getIncreaseContainersList()
          .equals(other.getIncreaseContainersList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getIncreaseContainersCount() > 0) {
        hash = (37 * hash) + INCREASE_CONTAINERS_FIELD_NUMBER;
        hash = (53 * hash) + getIncreaseContainersList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.IncreaseContainersResourceRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_IncreaseContainersResourceRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_IncreaseContainersResourceRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getIncreaseContainersFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (increaseContainersBuilder_ == null) {
          increaseContainers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          increaseContainersBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_IncreaseContainersResourceRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto(this);
        int from_bitField0_ = bitField0_;
        if (increaseContainersBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            increaseContainers_ = java.util.Collections.unmodifiableList(increaseContainers_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.increaseContainers_ = increaseContainers_;
        } else {
          result.increaseContainers_ = increaseContainersBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto.getDefaultInstance()) return this;
        if (increaseContainersBuilder_ == null) {
          if (!other.increaseContainers_.isEmpty()) {
            if (increaseContainers_.isEmpty()) {
              increaseContainers_ = other.increaseContainers_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureIncreaseContainersIsMutable();
              increaseContainers_.addAll(other.increaseContainers_);
            }
            onChanged();
          }
        } else {
          if (!other.increaseContainers_.isEmpty()) {
            if (increaseContainersBuilder_.isEmpty()) {
              increaseContainersBuilder_.dispose();
              increaseContainersBuilder_ = null;
              increaseContainers_ = other.increaseContainers_;
              bitField0_ = (bitField0_ & ~0x00000001);
              increaseContainersBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getIncreaseContainersFieldBuilder() : null;
            } else {
              increaseContainersBuilder_.addAllMessages(other.increaseContainers_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        for (int i = 0; i < getIncreaseContainersCount(); i++) {
          if (!getIncreaseContainers(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.common.TokenProto increase_containers = 1;
      private java.util.List<org.apache.hadoop.security.proto.SecurityProtos.TokenProto> increaseContainers_ =
        java.util.Collections.emptyList();
      private void ensureIncreaseContainersIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          increaseContainers_ = new java.util.ArrayList<org.apache.hadoop.security.proto.SecurityProtos.TokenProto>(increaseContainers_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.security.proto.SecurityProtos.TokenProto, org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder, org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder> increaseContainersBuilder_;

      /**
       * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
       */
      public java.util.List<org.apache.hadoop.security.proto.SecurityProtos.TokenProto> getIncreaseContainersList() {
        if (increaseContainersBuilder_ == null) {
          return java.util.Collections.unmodifiableList(increaseContainers_);
        } else {
          return increaseContainersBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
       */
      public int getIncreaseContainersCount() {
        if (increaseContainersBuilder_ == null) {
          return increaseContainers_.size();
        } else {
          return increaseContainersBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
       */
      public org.apache.hadoop.security.proto.SecurityProtos.TokenProto getIncreaseContainers(int index) {
        if (increaseContainersBuilder_ == null) {
          return increaseContainers_.get(index);
        } else {
          return increaseContainersBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
       */
      public Builder setIncreaseContainers(
          int index, org.apache.hadoop.security.proto.SecurityProtos.TokenProto value) {
        if (increaseContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureIncreaseContainersIsMutable();
          increaseContainers_.set(index, value);
          onChanged();
        } else {
          increaseContainersBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
       */
      public Builder setIncreaseContainers(
          int index, org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder builderForValue) {
        if (increaseContainersBuilder_ == null) {
          ensureIncreaseContainersIsMutable();
          increaseContainers_.set(index, builderForValue.build());
          onChanged();
        } else {
          increaseContainersBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
       */
      public Builder addIncreaseContainers(org.apache.hadoop.security.proto.SecurityProtos.TokenProto value) {
        if (increaseContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureIncreaseContainersIsMutable();
          increaseContainers_.add(value);
          onChanged();
        } else {
          increaseContainersBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
       */
      public Builder addIncreaseContainers(
          int index, org.apache.hadoop.security.proto.SecurityProtos.TokenProto value) {
        if (increaseContainersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureIncreaseContainersIsMutable();
          increaseContainers_.add(index, value);
          onChanged();
        } else {
          increaseContainersBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
       */
      public Builder addIncreaseContainers(
          org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder builderForValue) {
        if (increaseContainersBuilder_ == null) {
          ensureIncreaseContainersIsMutable();
          increaseContainers_.add(builderForValue.build());
          onChanged();
        } else {
          increaseContainersBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
       */
      public Builder addIncreaseContainers(
          int index, org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder builderForValue) {
        if (increaseContainersBuilder_ == null) {
          ensureIncreaseContainersIsMutable();
          increaseContainers_.add(index, builderForValue.build());
          onChanged();
        } else {
          increaseContainersBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
       */
      public Builder addAllIncreaseContainers(
          java.lang.Iterable<? extends org.apache.hadoop.security.proto.SecurityProtos.TokenProto> values) {
        if (increaseContainersBuilder_ == null) {
          ensureIncreaseContainersIsMutable();
          super.addAll(values, increaseContainers_);
          onChanged();
        } else {
          increaseContainersBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
       */
      public Builder clearIncreaseContainers() {
        if (increaseContainersBuilder_ == null) {
          increaseContainers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          increaseContainersBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
       */
      public Builder removeIncreaseContainers(int index) {
        if (increaseContainersBuilder_ == null) {
          ensureIncreaseContainersIsMutable();
          increaseContainers_.remove(index);
          onChanged();
        } else {
          increaseContainersBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
       */
      public org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder getIncreaseContainersBuilder(
          int index) {
        return getIncreaseContainersFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
       */
      public org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder getIncreaseContainersOrBuilder(
          int index) {
        if (increaseContainersBuilder_ == null) {
          return increaseContainers_.get(index);  } else {
          return increaseContainersBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder> 
           getIncreaseContainersOrBuilderList() {
        if (increaseContainersBuilder_ != null) {
          return increaseContainersBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(increaseContainers_);
        }
      }
      /**
       * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
       */
      public org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder addIncreaseContainersBuilder() {
        return getIncreaseContainersFieldBuilder().addBuilder(
            org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
       */
      public org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder addIncreaseContainersBuilder(
          int index) {
        return getIncreaseContainersFieldBuilder().addBuilder(
            index, org.apache.hadoop.security.proto.SecurityProtos.TokenProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.common.TokenProto increase_containers = 1;</code>
       */
      public java.util.List<org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder> 
           getIncreaseContainersBuilderList() {
        return getIncreaseContainersFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.security.proto.SecurityProtos.TokenProto, org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder, org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder> 
          getIncreaseContainersFieldBuilder() {
        if (increaseContainersBuilder_ == null) {
          increaseContainersBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.security.proto.SecurityProtos.TokenProto, org.apache.hadoop.security.proto.SecurityProtos.TokenProto.Builder, org.apache.hadoop.security.proto.SecurityProtos.TokenProtoOrBuilder>(
                  increaseContainers_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          increaseContainers_ = null;
        }
        return increaseContainersBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.IncreaseContainersResourceRequestProto)
    }

    static {
      defaultInstance = new IncreaseContainersResourceRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.IncreaseContainersResourceRequestProto)
  }

  public interface IncreaseContainersResourceResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> 
        getSucceededRequestsList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getSucceededRequests(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    int getSucceededRequestsCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getSucceededRequestsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getSucceededRequestsOrBuilder(
        int index);

    // repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> 
        getFailedRequestsList();
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto getFailedRequests(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    int getFailedRequestsCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
        getFailedRequestsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder getFailedRequestsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.IncreaseContainersResourceResponseProto}
   */
  public static final class IncreaseContainersResourceResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements IncreaseContainersResourceResponseProtoOrBuilder {
    // Use IncreaseContainersResourceResponseProto.newBuilder() to construct.
    private IncreaseContainersResourceResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private IncreaseContainersResourceResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final IncreaseContainersResourceResponseProto defaultInstance;
    public static IncreaseContainersResourceResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public IncreaseContainersResourceResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private IncreaseContainersResourceResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                succeededRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              succeededRequests_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.PARSER, extensionRegistry));
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                failedRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto>();
                mutable_bitField0_ |= 0x00000002;
              }
              failedRequests_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          succeededRequests_ = java.util.Collections.unmodifiableList(succeededRequests_);
        }
        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
          failedRequests_ = java.util.Collections.unmodifiableList(failedRequests_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_IncreaseContainersResourceResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_IncreaseContainersResourceResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<IncreaseContainersResourceResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<IncreaseContainersResourceResponseProto>() {
      public IncreaseContainersResourceResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new IncreaseContainersResourceResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<IncreaseContainersResourceResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;
    public static final int SUCCEEDED_REQUESTS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> succeededRequests_;
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getSucceededRequestsList() {
      return succeededRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
        getSucceededRequestsOrBuilderList() {
      return succeededRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    public int getSucceededRequestsCount() {
      return succeededRequests_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getSucceededRequests(int index) {
      return succeededRequests_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getSucceededRequestsOrBuilder(
        int index) {
      return succeededRequests_.get(index);
    }

    // repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;
    public static final int FAILED_REQUESTS_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> failedRequests_;
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> getFailedRequestsList() {
      return failedRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
        getFailedRequestsOrBuilderList() {
      return failedRequests_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public int getFailedRequestsCount() {
      return failedRequests_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto getFailedRequests(int index) {
      return failedRequests_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder getFailedRequestsOrBuilder(
        int index) {
      return failedRequests_.get(index);
    }

    private void initFields() {
      succeededRequests_ = java.util.Collections.emptyList();
      failedRequests_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < succeededRequests_.size(); i++) {
        output.writeMessage(1, succeededRequests_.get(i));
      }
      for (int i = 0; i < failedRequests_.size(); i++) {
        output.writeMessage(2, failedRequests_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < succeededRequests_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, succeededRequests_.get(i));
      }
      for (int i = 0; i < failedRequests_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, failedRequests_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto) obj;

      boolean result = true;
      result = result && getSucceededRequestsList()
          .equals(other.getSucceededRequestsList());
      result = result && getFailedRequestsList()
          .equals(other.getFailedRequestsList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getSucceededRequestsCount() > 0) {
        hash = (37 * hash) + SUCCEEDED_REQUESTS_FIELD_NUMBER;
        hash = (53 * hash) + getSucceededRequestsList().hashCode();
      }
      if (getFailedRequestsCount() > 0) {
        hash = (37 * hash) + FAILED_REQUESTS_FIELD_NUMBER;
        hash = (53 * hash) + getFailedRequestsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.IncreaseContainersResourceResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_IncreaseContainersResourceResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_IncreaseContainersResourceResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getSucceededRequestsFieldBuilder();
          getFailedRequestsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (succeededRequestsBuilder_ == null) {
          succeededRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          succeededRequestsBuilder_.clear();
        }
        if (failedRequestsBuilder_ == null) {
          failedRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          failedRequestsBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_IncreaseContainersResourceResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (succeededRequestsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            succeededRequests_ = java.util.Collections.unmodifiableList(succeededRequests_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.succeededRequests_ = succeededRequests_;
        } else {
          result.succeededRequests_ = succeededRequestsBuilder_.build();
        }
        if (failedRequestsBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002)) {
            failedRequests_ = java.util.Collections.unmodifiableList(failedRequests_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.failedRequests_ = failedRequests_;
        } else {
          result.failedRequests_ = failedRequestsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto.getDefaultInstance()) return this;
        if (succeededRequestsBuilder_ == null) {
          if (!other.succeededRequests_.isEmpty()) {
            if (succeededRequests_.isEmpty()) {
              succeededRequests_ = other.succeededRequests_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureSucceededRequestsIsMutable();
              succeededRequests_.addAll(other.succeededRequests_);
            }
            onChanged();
          }
        } else {
          if (!other.succeededRequests_.isEmpty()) {
            if (succeededRequestsBuilder_.isEmpty()) {
              succeededRequestsBuilder_.dispose();
              succeededRequestsBuilder_ = null;
              succeededRequests_ = other.succeededRequests_;
              bitField0_ = (bitField0_ & ~0x00000001);
              succeededRequestsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getSucceededRequestsFieldBuilder() : null;
            } else {
              succeededRequestsBuilder_.addAllMessages(other.succeededRequests_);
            }
          }
        }
        if (failedRequestsBuilder_ == null) {
          if (!other.failedRequests_.isEmpty()) {
            if (failedRequests_.isEmpty()) {
              failedRequests_ = other.failedRequests_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureFailedRequestsIsMutable();
              failedRequests_.addAll(other.failedRequests_);
            }
            onChanged();
          }
        } else {
          if (!other.failedRequests_.isEmpty()) {
            if (failedRequestsBuilder_.isEmpty()) {
              failedRequestsBuilder_.dispose();
              failedRequestsBuilder_ = null;
              failedRequests_ = other.failedRequests_;
              bitField0_ = (bitField0_ & ~0x00000002);
              failedRequestsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getFailedRequestsFieldBuilder() : null;
            } else {
              failedRequestsBuilder_.addAllMessages(other.failedRequests_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.IncreaseContainersResourceResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> succeededRequests_ =
        java.util.Collections.emptyList();
      private void ensureSucceededRequestsIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          succeededRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto>(succeededRequests_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> succeededRequestsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> getSucceededRequestsList() {
        if (succeededRequestsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(succeededRequests_);
        } else {
          return succeededRequestsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public int getSucceededRequestsCount() {
        if (succeededRequestsBuilder_ == null) {
          return succeededRequests_.size();
        } else {
          return succeededRequestsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getSucceededRequests(int index) {
        if (succeededRequestsBuilder_ == null) {
          return succeededRequests_.get(index);
        } else {
          return succeededRequestsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder setSucceededRequests(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (succeededRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSucceededRequestsIsMutable();
          succeededRequests_.set(index, value);
          onChanged();
        } else {
          succeededRequestsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder setSucceededRequests(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          succeededRequests_.set(index, builderForValue.build());
          onChanged();
        } else {
          succeededRequestsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder addSucceededRequests(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (succeededRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSucceededRequestsIsMutable();
          succeededRequests_.add(value);
          onChanged();
        } else {
          succeededRequestsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder addSucceededRequests(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (succeededRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSucceededRequestsIsMutable();
          succeededRequests_.add(index, value);
          onChanged();
        } else {
          succeededRequestsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder addSucceededRequests(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          succeededRequests_.add(builderForValue.build());
          onChanged();
        } else {
          succeededRequestsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder addSucceededRequests(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          succeededRequests_.add(index, builderForValue.build());
          onChanged();
        } else {
          succeededRequestsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder addAllSucceededRequests(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto> values) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          super.addAll(values, succeededRequests_);
          onChanged();
        } else {
          succeededRequestsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder clearSucceededRequests() {
        if (succeededRequestsBuilder_ == null) {
          succeededRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          succeededRequestsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public Builder removeSucceededRequests(int index) {
        if (succeededRequestsBuilder_ == null) {
          ensureSucceededRequestsIsMutable();
          succeededRequests_.remove(index);
          onChanged();
        } else {
          succeededRequestsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder getSucceededRequestsBuilder(
          int index) {
        return getSucceededRequestsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getSucceededRequestsOrBuilder(
          int index) {
        if (succeededRequestsBuilder_ == null) {
          return succeededRequests_.get(index);  } else {
          return succeededRequestsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
           getSucceededRequestsOrBuilderList() {
        if (succeededRequestsBuilder_ != null) {
          return succeededRequestsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(succeededRequests_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addSucceededRequestsBuilder() {
        return getSucceededRequestsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder addSucceededRequestsBuilder(
          int index) {
        return getSucceededRequestsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerIdProto succeeded_requests = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder> 
           getSucceededRequestsBuilderList() {
        return getSucceededRequestsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
          getSucceededRequestsFieldBuilder() {
        if (succeededRequestsBuilder_ == null) {
          succeededRequestsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder>(
                  succeededRequests_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          succeededRequests_ = null;
        }
        return succeededRequestsBuilder_;
      }

      // repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> failedRequests_ =
        java.util.Collections.emptyList();
      private void ensureFailedRequestsIsMutable() {
        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
          failedRequests_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto>(failedRequests_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> failedRequestsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> getFailedRequestsList() {
        if (failedRequestsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(failedRequests_);
        } else {
          return failedRequestsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public int getFailedRequestsCount() {
        if (failedRequestsBuilder_ == null) {
          return failedRequests_.size();
        } else {
          return failedRequestsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto getFailedRequests(int index) {
        if (failedRequestsBuilder_ == null) {
          return failedRequests_.get(index);
        } else {
          return failedRequestsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder setFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto value) {
        if (failedRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureFailedRequestsIsMutable();
          failedRequests_.set(index, value);
          onChanged();
        } else {
          failedRequestsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder setFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder builderForValue) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.set(index, builderForValue.build());
          onChanged();
        } else {
          failedRequestsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addFailedRequests(org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto value) {
        if (failedRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureFailedRequestsIsMutable();
          failedRequests_.add(value);
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto value) {
        if (failedRequestsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureFailedRequestsIsMutable();
          failedRequests_.add(index, value);
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addFailedRequests(
          org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder builderForValue) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.add(builderForValue.build());
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addFailedRequests(
          int index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder builderForValue) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.add(index, builderForValue.build());
          onChanged();
        } else {
          failedRequestsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder addAllFailedRequests(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto> values) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          super.addAll(values, failedRequests_);
          onChanged();
        } else {
          failedRequestsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder clearFailedRequests() {
        if (failedRequestsBuilder_ == null) {
          failedRequests_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          failedRequestsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public Builder removeFailedRequests(int index) {
        if (failedRequestsBuilder_ == null) {
          ensureFailedRequestsIsMutable();
          failedRequests_.remove(index);
          onChanged();
        } else {
          failedRequestsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder getFailedRequestsBuilder(
          int index) {
        return getFailedRequestsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder getFailedRequestsOrBuilder(
          int index) {
        if (failedRequestsBuilder_ == null) {
          return failedRequests_.get(index);  } else {
          return failedRequestsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
           getFailedRequestsOrBuilderList() {
        if (failedRequestsBuilder_ != null) {
          return failedRequestsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(failedRequests_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder addFailedRequestsBuilder() {
        return getFailedRequestsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder addFailedRequestsBuilder(
          int index) {
        return getFailedRequestsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerExceptionMapProto failed_requests = 2;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder> 
           getFailedRequestsBuilderList() {
        return getFailedRequestsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder> 
          getFailedRequestsFieldBuilder() {
        if (failedRequestsBuilder_ == null) {
          failedRequestsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProto.Builder, org.apache.hadoop.yarn.proto.YarnServiceProtos.ContainerExceptionMapProtoOrBuilder>(
                  failedRequests_,
                  ((bitField0_ & 0x00000002) == 0x00000002),
                  getParentForChildren(),
                  isClean());
          failedRequests_ = null;
        }
        return failedRequestsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.IncreaseContainersResourceResponseProto)
    }

    static {
      defaultInstance = new IncreaseContainersResourceResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.IncreaseContainersResourceResponseProto)
  }

  public interface GetApplicationAttemptReportRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
     */
    boolean hasApplicationAttemptId();
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto getApplicationAttemptId();
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder getApplicationAttemptIdOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetApplicationAttemptReportRequestProto}
   */
  public static final class GetApplicationAttemptReportRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetApplicationAttemptReportRequestProtoOrBuilder {
    // Use GetApplicationAttemptReportRequestProto.newBuilder() to construct.
    private GetApplicationAttemptReportRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetApplicationAttemptReportRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetApplicationAttemptReportRequestProto defaultInstance;
    public static GetApplicationAttemptReportRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetApplicationAttemptReportRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetApplicationAttemptReportRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = applicationAttemptId_.toBuilder();
              }
              applicationAttemptId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationAttemptId_);
                applicationAttemptId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationAttemptReportRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationAttemptReportRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetApplicationAttemptReportRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetApplicationAttemptReportRequestProto>() {
      public GetApplicationAttemptReportRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetApplicationAttemptReportRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetApplicationAttemptReportRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;
    public static final int APPLICATION_ATTEMPT_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto applicationAttemptId_;
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
     */
    public boolean hasApplicationAttemptId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto getApplicationAttemptId() {
      return applicationAttemptId_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder getApplicationAttemptIdOrBuilder() {
      return applicationAttemptId_;
    }

    private void initFields() {
      applicationAttemptId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, applicationAttemptId_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, applicationAttemptId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto) obj;

      boolean result = true;
      result = result && (hasApplicationAttemptId() == other.hasApplicationAttemptId());
      if (hasApplicationAttemptId()) {
        result = result && getApplicationAttemptId()
            .equals(other.getApplicationAttemptId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasApplicationAttemptId()) {
        hash = (37 * hash) + APPLICATION_ATTEMPT_ID_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationAttemptId().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetApplicationAttemptReportRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationAttemptReportRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationAttemptReportRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getApplicationAttemptIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (applicationAttemptIdBuilder_ == null) {
          applicationAttemptId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance();
        } else {
          applicationAttemptIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationAttemptReportRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (applicationAttemptIdBuilder_ == null) {
          result.applicationAttemptId_ = applicationAttemptId_;
        } else {
          result.applicationAttemptId_ = applicationAttemptIdBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto.getDefaultInstance()) return this;
        if (other.hasApplicationAttemptId()) {
          mergeApplicationAttemptId(other.getApplicationAttemptId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto applicationAttemptId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder> applicationAttemptIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public boolean hasApplicationAttemptId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto getApplicationAttemptId() {
        if (applicationAttemptIdBuilder_ == null) {
          return applicationAttemptId_;
        } else {
          return applicationAttemptIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public Builder setApplicationAttemptId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto value) {
        if (applicationAttemptIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationAttemptId_ = value;
          onChanged();
        } else {
          applicationAttemptIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public Builder setApplicationAttemptId(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder builderForValue) {
        if (applicationAttemptIdBuilder_ == null) {
          applicationAttemptId_ = builderForValue.build();
          onChanged();
        } else {
          applicationAttemptIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public Builder mergeApplicationAttemptId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto value) {
        if (applicationAttemptIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              applicationAttemptId_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance()) {
            applicationAttemptId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.newBuilder(applicationAttemptId_).mergeFrom(value).buildPartial();
          } else {
            applicationAttemptId_ = value;
          }
          onChanged();
        } else {
          applicationAttemptIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public Builder clearApplicationAttemptId() {
        if (applicationAttemptIdBuilder_ == null) {
          applicationAttemptId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance();
          onChanged();
        } else {
          applicationAttemptIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder getApplicationAttemptIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getApplicationAttemptIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder getApplicationAttemptIdOrBuilder() {
        if (applicationAttemptIdBuilder_ != null) {
          return applicationAttemptIdBuilder_.getMessageOrBuilder();
        } else {
          return applicationAttemptId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder> 
          getApplicationAttemptIdFieldBuilder() {
        if (applicationAttemptIdBuilder_ == null) {
          applicationAttemptIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder>(
                  applicationAttemptId_,
                  getParentForChildren(),
                  isClean());
          applicationAttemptId_ = null;
        }
        return applicationAttemptIdBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetApplicationAttemptReportRequestProto)
    }

    static {
      defaultInstance = new GetApplicationAttemptReportRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetApplicationAttemptReportRequestProto)
  }

  public interface GetApplicationAttemptReportResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ApplicationAttemptReportProto application_attempt_report = 1;
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptReportProto application_attempt_report = 1;</code>
     */
    boolean hasApplicationAttemptReport();
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptReportProto application_attempt_report = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto getApplicationAttemptReport();
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptReportProto application_attempt_report = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProtoOrBuilder getApplicationAttemptReportOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetApplicationAttemptReportResponseProto}
   */
  public static final class GetApplicationAttemptReportResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetApplicationAttemptReportResponseProtoOrBuilder {
    // Use GetApplicationAttemptReportResponseProto.newBuilder() to construct.
    private GetApplicationAttemptReportResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetApplicationAttemptReportResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetApplicationAttemptReportResponseProto defaultInstance;
    public static GetApplicationAttemptReportResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetApplicationAttemptReportResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetApplicationAttemptReportResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = applicationAttemptReport_.toBuilder();
              }
              applicationAttemptReport_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationAttemptReport_);
                applicationAttemptReport_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationAttemptReportResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationAttemptReportResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetApplicationAttemptReportResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetApplicationAttemptReportResponseProto>() {
      public GetApplicationAttemptReportResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetApplicationAttemptReportResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetApplicationAttemptReportResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ApplicationAttemptReportProto application_attempt_report = 1;
    public static final int APPLICATION_ATTEMPT_REPORT_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto applicationAttemptReport_;
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptReportProto application_attempt_report = 1;</code>
     */
    public boolean hasApplicationAttemptReport() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptReportProto application_attempt_report = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto getApplicationAttemptReport() {
      return applicationAttemptReport_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptReportProto application_attempt_report = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProtoOrBuilder getApplicationAttemptReportOrBuilder() {
      return applicationAttemptReport_;
    }

    private void initFields() {
      applicationAttemptReport_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, applicationAttemptReport_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, applicationAttemptReport_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto) obj;

      boolean result = true;
      result = result && (hasApplicationAttemptReport() == other.hasApplicationAttemptReport());
      if (hasApplicationAttemptReport()) {
        result = result && getApplicationAttemptReport()
            .equals(other.getApplicationAttemptReport());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasApplicationAttemptReport()) {
        hash = (37 * hash) + APPLICATION_ATTEMPT_REPORT_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationAttemptReport().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetApplicationAttemptReportResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationAttemptReportResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationAttemptReportResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getApplicationAttemptReportFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (applicationAttemptReportBuilder_ == null) {
          applicationAttemptReport_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.getDefaultInstance();
        } else {
          applicationAttemptReportBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationAttemptReportResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (applicationAttemptReportBuilder_ == null) {
          result.applicationAttemptReport_ = applicationAttemptReport_;
        } else {
          result.applicationAttemptReport_ = applicationAttemptReportBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto.getDefaultInstance()) return this;
        if (other.hasApplicationAttemptReport()) {
          mergeApplicationAttemptReport(other.getApplicationAttemptReport());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptReportResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ApplicationAttemptReportProto application_attempt_report = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto applicationAttemptReport_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProtoOrBuilder> applicationAttemptReportBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptReportProto application_attempt_report = 1;</code>
       */
      public boolean hasApplicationAttemptReport() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptReportProto application_attempt_report = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto getApplicationAttemptReport() {
        if (applicationAttemptReportBuilder_ == null) {
          return applicationAttemptReport_;
        } else {
          return applicationAttemptReportBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptReportProto application_attempt_report = 1;</code>
       */
      public Builder setApplicationAttemptReport(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto value) {
        if (applicationAttemptReportBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationAttemptReport_ = value;
          onChanged();
        } else {
          applicationAttemptReportBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptReportProto application_attempt_report = 1;</code>
       */
      public Builder setApplicationAttemptReport(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.Builder builderForValue) {
        if (applicationAttemptReportBuilder_ == null) {
          applicationAttemptReport_ = builderForValue.build();
          onChanged();
        } else {
          applicationAttemptReportBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptReportProto application_attempt_report = 1;</code>
       */
      public Builder mergeApplicationAttemptReport(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto value) {
        if (applicationAttemptReportBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              applicationAttemptReport_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.getDefaultInstance()) {
            applicationAttemptReport_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.newBuilder(applicationAttemptReport_).mergeFrom(value).buildPartial();
          } else {
            applicationAttemptReport_ = value;
          }
          onChanged();
        } else {
          applicationAttemptReportBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptReportProto application_attempt_report = 1;</code>
       */
      public Builder clearApplicationAttemptReport() {
        if (applicationAttemptReportBuilder_ == null) {
          applicationAttemptReport_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.getDefaultInstance();
          onChanged();
        } else {
          applicationAttemptReportBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptReportProto application_attempt_report = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.Builder getApplicationAttemptReportBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getApplicationAttemptReportFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptReportProto application_attempt_report = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProtoOrBuilder getApplicationAttemptReportOrBuilder() {
        if (applicationAttemptReportBuilder_ != null) {
          return applicationAttemptReportBuilder_.getMessageOrBuilder();
        } else {
          return applicationAttemptReport_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptReportProto application_attempt_report = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProtoOrBuilder> 
          getApplicationAttemptReportFieldBuilder() {
        if (applicationAttemptReportBuilder_ == null) {
          applicationAttemptReportBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProtoOrBuilder>(
                  applicationAttemptReport_,
                  getParentForChildren(),
                  isClean());
          applicationAttemptReport_ = null;
        }
        return applicationAttemptReportBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetApplicationAttemptReportResponseProto)
    }

    static {
      defaultInstance = new GetApplicationAttemptReportResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetApplicationAttemptReportResponseProto)
  }

  public interface GetApplicationAttemptsRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ApplicationIdProto application_id = 1;
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    boolean hasApplicationId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetApplicationAttemptsRequestProto}
   */
  public static final class GetApplicationAttemptsRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetApplicationAttemptsRequestProtoOrBuilder {
    // Use GetApplicationAttemptsRequestProto.newBuilder() to construct.
    private GetApplicationAttemptsRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetApplicationAttemptsRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetApplicationAttemptsRequestProto defaultInstance;
    public static GetApplicationAttemptsRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetApplicationAttemptsRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetApplicationAttemptsRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = applicationId_.toBuilder();
              }
              applicationId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationId_);
                applicationId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationAttemptsRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationAttemptsRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetApplicationAttemptsRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetApplicationAttemptsRequestProto>() {
      public GetApplicationAttemptsRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetApplicationAttemptsRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetApplicationAttemptsRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ApplicationIdProto application_id = 1;
    public static final int APPLICATION_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto applicationId_;
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public boolean hasApplicationId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId() {
      return applicationId_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder() {
      return applicationId_;
    }

    private void initFields() {
      applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, applicationId_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, applicationId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto) obj;

      boolean result = true;
      result = result && (hasApplicationId() == other.hasApplicationId());
      if (hasApplicationId()) {
        result = result && getApplicationId()
            .equals(other.getApplicationId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasApplicationId()) {
        hash = (37 * hash) + APPLICATION_ID_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationId().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetApplicationAttemptsRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationAttemptsRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationAttemptsRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getApplicationIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (applicationIdBuilder_ == null) {
          applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
        } else {
          applicationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationAttemptsRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (applicationIdBuilder_ == null) {
          result.applicationId_ = applicationId_;
        } else {
          result.applicationId_ = applicationIdBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto.getDefaultInstance()) return this;
        if (other.hasApplicationId()) {
          mergeApplicationId(other.getApplicationId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ApplicationIdProto application_id = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> applicationIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public boolean hasApplicationId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId() {
        if (applicationIdBuilder_ == null) {
          return applicationId_;
        } else {
          return applicationIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder setApplicationId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationId_ = value;
          onChanged();
        } else {
          applicationIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder setApplicationId(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder builderForValue) {
        if (applicationIdBuilder_ == null) {
          applicationId_ = builderForValue.build();
          onChanged();
        } else {
          applicationIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder mergeApplicationId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              applicationId_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance()) {
            applicationId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.newBuilder(applicationId_).mergeFrom(value).buildPartial();
          } else {
            applicationId_ = value;
          }
          onChanged();
        } else {
          applicationIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public Builder clearApplicationId() {
        if (applicationIdBuilder_ == null) {
          applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
          onChanged();
        } else {
          applicationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder getApplicationIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getApplicationIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder() {
        if (applicationIdBuilder_ != null) {
          return applicationIdBuilder_.getMessageOrBuilder();
        } else {
          return applicationId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto application_id = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> 
          getApplicationIdFieldBuilder() {
        if (applicationIdBuilder_ == null) {
          applicationIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder>(
                  applicationId_,
                  getParentForChildren(),
                  isClean());
          applicationId_ = null;
        }
        return applicationIdBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetApplicationAttemptsRequestProto)
    }

    static {
      defaultInstance = new GetApplicationAttemptsRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetApplicationAttemptsRequestProto)
  }

  public interface GetApplicationAttemptsResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;
    /**
     * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto> 
        getApplicationAttemptsList();
    /**
     * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto getApplicationAttempts(int index);
    /**
     * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
     */
    int getApplicationAttemptsCount();
    /**
     * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProtoOrBuilder> 
        getApplicationAttemptsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProtoOrBuilder getApplicationAttemptsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetApplicationAttemptsResponseProto}
   */
  public static final class GetApplicationAttemptsResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetApplicationAttemptsResponseProtoOrBuilder {
    // Use GetApplicationAttemptsResponseProto.newBuilder() to construct.
    private GetApplicationAttemptsResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetApplicationAttemptsResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetApplicationAttemptsResponseProto defaultInstance;
    public static GetApplicationAttemptsResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetApplicationAttemptsResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetApplicationAttemptsResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                applicationAttempts_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              applicationAttempts_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          applicationAttempts_ = java.util.Collections.unmodifiableList(applicationAttempts_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationAttemptsResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationAttemptsResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetApplicationAttemptsResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetApplicationAttemptsResponseProto>() {
      public GetApplicationAttemptsResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetApplicationAttemptsResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetApplicationAttemptsResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;
    public static final int APPLICATION_ATTEMPTS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto> applicationAttempts_;
    /**
     * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto> getApplicationAttemptsList() {
      return applicationAttempts_;
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProtoOrBuilder> 
        getApplicationAttemptsOrBuilderList() {
      return applicationAttempts_;
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
     */
    public int getApplicationAttemptsCount() {
      return applicationAttempts_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto getApplicationAttempts(int index) {
      return applicationAttempts_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProtoOrBuilder getApplicationAttemptsOrBuilder(
        int index) {
      return applicationAttempts_.get(index);
    }

    private void initFields() {
      applicationAttempts_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < applicationAttempts_.size(); i++) {
        output.writeMessage(1, applicationAttempts_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < applicationAttempts_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, applicationAttempts_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto) obj;

      boolean result = true;
      result = result && getApplicationAttemptsList()
          .equals(other.getApplicationAttemptsList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getApplicationAttemptsCount() > 0) {
        hash = (37 * hash) + APPLICATION_ATTEMPTS_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationAttemptsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetApplicationAttemptsResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationAttemptsResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationAttemptsResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getApplicationAttemptsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (applicationAttemptsBuilder_ == null) {
          applicationAttempts_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          applicationAttemptsBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetApplicationAttemptsResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (applicationAttemptsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            applicationAttempts_ = java.util.Collections.unmodifiableList(applicationAttempts_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.applicationAttempts_ = applicationAttempts_;
        } else {
          result.applicationAttempts_ = applicationAttemptsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto.getDefaultInstance()) return this;
        if (applicationAttemptsBuilder_ == null) {
          if (!other.applicationAttempts_.isEmpty()) {
            if (applicationAttempts_.isEmpty()) {
              applicationAttempts_ = other.applicationAttempts_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureApplicationAttemptsIsMutable();
              applicationAttempts_.addAll(other.applicationAttempts_);
            }
            onChanged();
          }
        } else {
          if (!other.applicationAttempts_.isEmpty()) {
            if (applicationAttemptsBuilder_.isEmpty()) {
              applicationAttemptsBuilder_.dispose();
              applicationAttemptsBuilder_ = null;
              applicationAttempts_ = other.applicationAttempts_;
              bitField0_ = (bitField0_ & ~0x00000001);
              applicationAttemptsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getApplicationAttemptsFieldBuilder() : null;
            } else {
              applicationAttemptsBuilder_.addAllMessages(other.applicationAttempts_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetApplicationAttemptsResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto> applicationAttempts_ =
        java.util.Collections.emptyList();
      private void ensureApplicationAttemptsIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          applicationAttempts_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto>(applicationAttempts_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProtoOrBuilder> applicationAttemptsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto> getApplicationAttemptsList() {
        if (applicationAttemptsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(applicationAttempts_);
        } else {
          return applicationAttemptsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
       */
      public int getApplicationAttemptsCount() {
        if (applicationAttemptsBuilder_ == null) {
          return applicationAttempts_.size();
        } else {
          return applicationAttemptsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto getApplicationAttempts(int index) {
        if (applicationAttemptsBuilder_ == null) {
          return applicationAttempts_.get(index);
        } else {
          return applicationAttemptsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
       */
      public Builder setApplicationAttempts(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto value) {
        if (applicationAttemptsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureApplicationAttemptsIsMutable();
          applicationAttempts_.set(index, value);
          onChanged();
        } else {
          applicationAttemptsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
       */
      public Builder setApplicationAttempts(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.Builder builderForValue) {
        if (applicationAttemptsBuilder_ == null) {
          ensureApplicationAttemptsIsMutable();
          applicationAttempts_.set(index, builderForValue.build());
          onChanged();
        } else {
          applicationAttemptsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
       */
      public Builder addApplicationAttempts(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto value) {
        if (applicationAttemptsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureApplicationAttemptsIsMutable();
          applicationAttempts_.add(value);
          onChanged();
        } else {
          applicationAttemptsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
       */
      public Builder addApplicationAttempts(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto value) {
        if (applicationAttemptsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureApplicationAttemptsIsMutable();
          applicationAttempts_.add(index, value);
          onChanged();
        } else {
          applicationAttemptsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
       */
      public Builder addApplicationAttempts(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.Builder builderForValue) {
        if (applicationAttemptsBuilder_ == null) {
          ensureApplicationAttemptsIsMutable();
          applicationAttempts_.add(builderForValue.build());
          onChanged();
        } else {
          applicationAttemptsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
       */
      public Builder addApplicationAttempts(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.Builder builderForValue) {
        if (applicationAttemptsBuilder_ == null) {
          ensureApplicationAttemptsIsMutable();
          applicationAttempts_.add(index, builderForValue.build());
          onChanged();
        } else {
          applicationAttemptsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
       */
      public Builder addAllApplicationAttempts(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto> values) {
        if (applicationAttemptsBuilder_ == null) {
          ensureApplicationAttemptsIsMutable();
          super.addAll(values, applicationAttempts_);
          onChanged();
        } else {
          applicationAttemptsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
       */
      public Builder clearApplicationAttempts() {
        if (applicationAttemptsBuilder_ == null) {
          applicationAttempts_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          applicationAttemptsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
       */
      public Builder removeApplicationAttempts(int index) {
        if (applicationAttemptsBuilder_ == null) {
          ensureApplicationAttemptsIsMutable();
          applicationAttempts_.remove(index);
          onChanged();
        } else {
          applicationAttemptsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.Builder getApplicationAttemptsBuilder(
          int index) {
        return getApplicationAttemptsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProtoOrBuilder getApplicationAttemptsOrBuilder(
          int index) {
        if (applicationAttemptsBuilder_ == null) {
          return applicationAttempts_.get(index);  } else {
          return applicationAttemptsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProtoOrBuilder> 
           getApplicationAttemptsOrBuilderList() {
        if (applicationAttemptsBuilder_ != null) {
          return applicationAttemptsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(applicationAttempts_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.Builder addApplicationAttemptsBuilder() {
        return getApplicationAttemptsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.Builder addApplicationAttemptsBuilder(
          int index) {
        return getApplicationAttemptsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ApplicationAttemptReportProto application_attempts = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.Builder> 
           getApplicationAttemptsBuilderList() {
        return getApplicationAttemptsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProtoOrBuilder> 
          getApplicationAttemptsFieldBuilder() {
        if (applicationAttemptsBuilder_ == null) {
          applicationAttemptsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptReportProtoOrBuilder>(
                  applicationAttempts_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          applicationAttempts_ = null;
        }
        return applicationAttemptsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetApplicationAttemptsResponseProto)
    }

    static {
      defaultInstance = new GetApplicationAttemptsResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetApplicationAttemptsResponseProto)
  }

  public interface GetContainerReportRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ContainerIdProto container_id = 1;
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    boolean hasContainerId();
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId();
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetContainerReportRequestProto}
   */
  public static final class GetContainerReportRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetContainerReportRequestProtoOrBuilder {
    // Use GetContainerReportRequestProto.newBuilder() to construct.
    private GetContainerReportRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetContainerReportRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetContainerReportRequestProto defaultInstance;
    public static GetContainerReportRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetContainerReportRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetContainerReportRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = containerId_.toBuilder();
              }
              containerId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(containerId_);
                containerId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerReportRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerReportRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetContainerReportRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetContainerReportRequestProto>() {
      public GetContainerReportRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetContainerReportRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetContainerReportRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ContainerIdProto container_id = 1;
    public static final int CONTAINER_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto containerId_;
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public boolean hasContainerId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId() {
      return containerId_;
    }
    /**
     * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder() {
      return containerId_;
    }

    private void initFields() {
      containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, containerId_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, containerId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto) obj;

      boolean result = true;
      result = result && (hasContainerId() == other.hasContainerId());
      if (hasContainerId()) {
        result = result && getContainerId()
            .equals(other.getContainerId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasContainerId()) {
        hash = (37 * hash) + CONTAINER_ID_FIELD_NUMBER;
        hash = (53 * hash) + getContainerId().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetContainerReportRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerReportRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerReportRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getContainerIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (containerIdBuilder_ == null) {
          containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
        } else {
          containerIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerReportRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (containerIdBuilder_ == null) {
          result.containerId_ = containerId_;
        } else {
          result.containerId_ = containerIdBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto.getDefaultInstance()) return this;
        if (other.hasContainerId()) {
          mergeContainerId(other.getContainerId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ContainerIdProto container_id = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> containerIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public boolean hasContainerId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto getContainerId() {
        if (containerIdBuilder_ == null) {
          return containerId_;
        } else {
          return containerIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder setContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          containerId_ = value;
          onChanged();
        } else {
          containerIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder setContainerId(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder builderForValue) {
        if (containerIdBuilder_ == null) {
          containerId_ = builderForValue.build();
          onChanged();
        } else {
          containerIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder mergeContainerId(org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto value) {
        if (containerIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              containerId_ != org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance()) {
            containerId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.newBuilder(containerId_).mergeFrom(value).buildPartial();
          } else {
            containerId_ = value;
          }
          onChanged();
        } else {
          containerIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public Builder clearContainerId() {
        if (containerIdBuilder_ == null) {
          containerId_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.getDefaultInstance();
          onChanged();
        } else {
          containerIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder getContainerIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getContainerIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder getContainerIdOrBuilder() {
        if (containerIdBuilder_ != null) {
          return containerIdBuilder_.getMessageOrBuilder();
        } else {
          return containerId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerIdProto container_id = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder> 
          getContainerIdFieldBuilder() {
        if (containerIdBuilder_ == null) {
          containerIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerIdProtoOrBuilder>(
                  containerId_,
                  getParentForChildren(),
                  isClean());
          containerId_ = null;
        }
        return containerIdBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetContainerReportRequestProto)
    }

    static {
      defaultInstance = new GetContainerReportRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetContainerReportRequestProto)
  }

  public interface GetContainerReportResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ContainerReportProto container_report = 1;
    /**
     * <code>optional .hadoop.yarn.ContainerReportProto container_report = 1;</code>
     */
    boolean hasContainerReport();
    /**
     * <code>optional .hadoop.yarn.ContainerReportProto container_report = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto getContainerReport();
    /**
     * <code>optional .hadoop.yarn.ContainerReportProto container_report = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProtoOrBuilder getContainerReportOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetContainerReportResponseProto}
   */
  public static final class GetContainerReportResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetContainerReportResponseProtoOrBuilder {
    // Use GetContainerReportResponseProto.newBuilder() to construct.
    private GetContainerReportResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetContainerReportResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetContainerReportResponseProto defaultInstance;
    public static GetContainerReportResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetContainerReportResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetContainerReportResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = containerReport_.toBuilder();
              }
              containerReport_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(containerReport_);
                containerReport_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerReportResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerReportResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetContainerReportResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetContainerReportResponseProto>() {
      public GetContainerReportResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetContainerReportResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetContainerReportResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ContainerReportProto container_report = 1;
    public static final int CONTAINER_REPORT_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto containerReport_;
    /**
     * <code>optional .hadoop.yarn.ContainerReportProto container_report = 1;</code>
     */
    public boolean hasContainerReport() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ContainerReportProto container_report = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto getContainerReport() {
      return containerReport_;
    }
    /**
     * <code>optional .hadoop.yarn.ContainerReportProto container_report = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProtoOrBuilder getContainerReportOrBuilder() {
      return containerReport_;
    }

    private void initFields() {
      containerReport_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, containerReport_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, containerReport_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto) obj;

      boolean result = true;
      result = result && (hasContainerReport() == other.hasContainerReport());
      if (hasContainerReport()) {
        result = result && getContainerReport()
            .equals(other.getContainerReport());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasContainerReport()) {
        hash = (37 * hash) + CONTAINER_REPORT_FIELD_NUMBER;
        hash = (53 * hash) + getContainerReport().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetContainerReportResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerReportResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerReportResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getContainerReportFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (containerReportBuilder_ == null) {
          containerReport_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.getDefaultInstance();
        } else {
          containerReportBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainerReportResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (containerReportBuilder_ == null) {
          result.containerReport_ = containerReport_;
        } else {
          result.containerReport_ = containerReportBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto.getDefaultInstance()) return this;
        if (other.hasContainerReport()) {
          mergeContainerReport(other.getContainerReport());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainerReportResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ContainerReportProto container_report = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto containerReport_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProtoOrBuilder> containerReportBuilder_;
      /**
       * <code>optional .hadoop.yarn.ContainerReportProto container_report = 1;</code>
       */
      public boolean hasContainerReport() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ContainerReportProto container_report = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto getContainerReport() {
        if (containerReportBuilder_ == null) {
          return containerReport_;
        } else {
          return containerReportBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerReportProto container_report = 1;</code>
       */
      public Builder setContainerReport(org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto value) {
        if (containerReportBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          containerReport_ = value;
          onChanged();
        } else {
          containerReportBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerReportProto container_report = 1;</code>
       */
      public Builder setContainerReport(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.Builder builderForValue) {
        if (containerReportBuilder_ == null) {
          containerReport_ = builderForValue.build();
          onChanged();
        } else {
          containerReportBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerReportProto container_report = 1;</code>
       */
      public Builder mergeContainerReport(org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto value) {
        if (containerReportBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              containerReport_ != org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.getDefaultInstance()) {
            containerReport_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.newBuilder(containerReport_).mergeFrom(value).buildPartial();
          } else {
            containerReport_ = value;
          }
          onChanged();
        } else {
          containerReportBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerReportProto container_report = 1;</code>
       */
      public Builder clearContainerReport() {
        if (containerReportBuilder_ == null) {
          containerReport_ = org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.getDefaultInstance();
          onChanged();
        } else {
          containerReportBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ContainerReportProto container_report = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.Builder getContainerReportBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getContainerReportFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ContainerReportProto container_report = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProtoOrBuilder getContainerReportOrBuilder() {
        if (containerReportBuilder_ != null) {
          return containerReportBuilder_.getMessageOrBuilder();
        } else {
          return containerReport_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ContainerReportProto container_report = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProtoOrBuilder> 
          getContainerReportFieldBuilder() {
        if (containerReportBuilder_ == null) {
          containerReportBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProtoOrBuilder>(
                  containerReport_,
                  getParentForChildren(),
                  isClean());
          containerReport_ = null;
        }
        return containerReportBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetContainerReportResponseProto)
    }

    static {
      defaultInstance = new GetContainerReportResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetContainerReportResponseProto)
  }

  public interface GetContainersRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
     */
    boolean hasApplicationAttemptId();
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto getApplicationAttemptId();
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder getApplicationAttemptIdOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetContainersRequestProto}
   */
  public static final class GetContainersRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetContainersRequestProtoOrBuilder {
    // Use GetContainersRequestProto.newBuilder() to construct.
    private GetContainersRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetContainersRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetContainersRequestProto defaultInstance;
    public static GetContainersRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetContainersRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetContainersRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = applicationAttemptId_.toBuilder();
              }
              applicationAttemptId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationAttemptId_);
                applicationAttemptId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainersRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainersRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetContainersRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetContainersRequestProto>() {
      public GetContainersRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetContainersRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetContainersRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;
    public static final int APPLICATION_ATTEMPT_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto applicationAttemptId_;
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
     */
    public boolean hasApplicationAttemptId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto getApplicationAttemptId() {
      return applicationAttemptId_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder getApplicationAttemptIdOrBuilder() {
      return applicationAttemptId_;
    }

    private void initFields() {
      applicationAttemptId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, applicationAttemptId_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, applicationAttemptId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto) obj;

      boolean result = true;
      result = result && (hasApplicationAttemptId() == other.hasApplicationAttemptId());
      if (hasApplicationAttemptId()) {
        result = result && getApplicationAttemptId()
            .equals(other.getApplicationAttemptId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasApplicationAttemptId()) {
        hash = (37 * hash) + APPLICATION_ATTEMPT_ID_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationAttemptId().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetContainersRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainersRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainersRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getApplicationAttemptIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (applicationAttemptIdBuilder_ == null) {
          applicationAttemptId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance();
        } else {
          applicationAttemptIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainersRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (applicationAttemptIdBuilder_ == null) {
          result.applicationAttemptId_ = applicationAttemptId_;
        } else {
          result.applicationAttemptId_ = applicationAttemptIdBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto.getDefaultInstance()) return this;
        if (other.hasApplicationAttemptId()) {
          mergeApplicationAttemptId(other.getApplicationAttemptId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto applicationAttemptId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder> applicationAttemptIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public boolean hasApplicationAttemptId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto getApplicationAttemptId() {
        if (applicationAttemptIdBuilder_ == null) {
          return applicationAttemptId_;
        } else {
          return applicationAttemptIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public Builder setApplicationAttemptId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto value) {
        if (applicationAttemptIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationAttemptId_ = value;
          onChanged();
        } else {
          applicationAttemptIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public Builder setApplicationAttemptId(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder builderForValue) {
        if (applicationAttemptIdBuilder_ == null) {
          applicationAttemptId_ = builderForValue.build();
          onChanged();
        } else {
          applicationAttemptIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public Builder mergeApplicationAttemptId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto value) {
        if (applicationAttemptIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              applicationAttemptId_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance()) {
            applicationAttemptId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.newBuilder(applicationAttemptId_).mergeFrom(value).buildPartial();
          } else {
            applicationAttemptId_ = value;
          }
          onChanged();
        } else {
          applicationAttemptIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public Builder clearApplicationAttemptId() {
        if (applicationAttemptIdBuilder_ == null) {
          applicationAttemptId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.getDefaultInstance();
          onChanged();
        } else {
          applicationAttemptIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder getApplicationAttemptIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getApplicationAttemptIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder getApplicationAttemptIdOrBuilder() {
        if (applicationAttemptIdBuilder_ != null) {
          return applicationAttemptIdBuilder_.getMessageOrBuilder();
        } else {
          return applicationAttemptId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationAttemptIdProto application_attempt_id = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder> 
          getApplicationAttemptIdFieldBuilder() {
        if (applicationAttemptIdBuilder_ == null) {
          applicationAttemptIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationAttemptIdProtoOrBuilder>(
                  applicationAttemptId_,
                  getParentForChildren(),
                  isClean());
          applicationAttemptId_ = null;
        }
        return applicationAttemptIdBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetContainersRequestProto)
    }

    static {
      defaultInstance = new GetContainersRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetContainersRequestProto)
  }

  public interface GetContainersResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.ContainerReportProto containers = 1;
    /**
     * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto> 
        getContainersList();
    /**
     * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto getContainers(int index);
    /**
     * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
     */
    int getContainersCount();
    /**
     * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProtoOrBuilder> 
        getContainersOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProtoOrBuilder getContainersOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetContainersResponseProto}
   */
  public static final class GetContainersResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetContainersResponseProtoOrBuilder {
    // Use GetContainersResponseProto.newBuilder() to construct.
    private GetContainersResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetContainersResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetContainersResponseProto defaultInstance;
    public static GetContainersResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetContainersResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetContainersResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                containers_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              containers_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          containers_ = java.util.Collections.unmodifiableList(containers_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainersResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainersResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetContainersResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetContainersResponseProto>() {
      public GetContainersResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetContainersResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetContainersResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.ContainerReportProto containers = 1;
    public static final int CONTAINERS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto> containers_;
    /**
     * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto> getContainersList() {
      return containers_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProtoOrBuilder> 
        getContainersOrBuilderList() {
      return containers_;
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
     */
    public int getContainersCount() {
      return containers_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto getContainers(int index) {
      return containers_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProtoOrBuilder getContainersOrBuilder(
        int index) {
      return containers_.get(index);
    }

    private void initFields() {
      containers_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < containers_.size(); i++) {
        output.writeMessage(1, containers_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < containers_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, containers_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto) obj;

      boolean result = true;
      result = result && getContainersList()
          .equals(other.getContainersList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getContainersCount() > 0) {
        hash = (37 * hash) + CONTAINERS_FIELD_NUMBER;
        hash = (53 * hash) + getContainersList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetContainersResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainersResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainersResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getContainersFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (containersBuilder_ == null) {
          containers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          containersBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetContainersResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (containersBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            containers_ = java.util.Collections.unmodifiableList(containers_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.containers_ = containers_;
        } else {
          result.containers_ = containersBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto.getDefaultInstance()) return this;
        if (containersBuilder_ == null) {
          if (!other.containers_.isEmpty()) {
            if (containers_.isEmpty()) {
              containers_ = other.containers_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureContainersIsMutable();
              containers_.addAll(other.containers_);
            }
            onChanged();
          }
        } else {
          if (!other.containers_.isEmpty()) {
            if (containersBuilder_.isEmpty()) {
              containersBuilder_.dispose();
              containersBuilder_ = null;
              containers_ = other.containers_;
              bitField0_ = (bitField0_ & ~0x00000001);
              containersBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getContainersFieldBuilder() : null;
            } else {
              containersBuilder_.addAllMessages(other.containers_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetContainersResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.ContainerReportProto containers = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto> containers_ =
        java.util.Collections.emptyList();
      private void ensureContainersIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          containers_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto>(containers_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProtoOrBuilder> containersBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto> getContainersList() {
        if (containersBuilder_ == null) {
          return java.util.Collections.unmodifiableList(containers_);
        } else {
          return containersBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
       */
      public int getContainersCount() {
        if (containersBuilder_ == null) {
          return containers_.size();
        } else {
          return containersBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto getContainers(int index) {
        if (containersBuilder_ == null) {
          return containers_.get(index);
        } else {
          return containersBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
       */
      public Builder setContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto value) {
        if (containersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainersIsMutable();
          containers_.set(index, value);
          onChanged();
        } else {
          containersBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
       */
      public Builder setContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.Builder builderForValue) {
        if (containersBuilder_ == null) {
          ensureContainersIsMutable();
          containers_.set(index, builderForValue.build());
          onChanged();
        } else {
          containersBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
       */
      public Builder addContainers(org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto value) {
        if (containersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainersIsMutable();
          containers_.add(value);
          onChanged();
        } else {
          containersBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
       */
      public Builder addContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto value) {
        if (containersBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureContainersIsMutable();
          containers_.add(index, value);
          onChanged();
        } else {
          containersBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
       */
      public Builder addContainers(
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.Builder builderForValue) {
        if (containersBuilder_ == null) {
          ensureContainersIsMutable();
          containers_.add(builderForValue.build());
          onChanged();
        } else {
          containersBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
       */
      public Builder addContainers(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.Builder builderForValue) {
        if (containersBuilder_ == null) {
          ensureContainersIsMutable();
          containers_.add(index, builderForValue.build());
          onChanged();
        } else {
          containersBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
       */
      public Builder addAllContainers(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto> values) {
        if (containersBuilder_ == null) {
          ensureContainersIsMutable();
          super.addAll(values, containers_);
          onChanged();
        } else {
          containersBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
       */
      public Builder clearContainers() {
        if (containersBuilder_ == null) {
          containers_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          containersBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
       */
      public Builder removeContainers(int index) {
        if (containersBuilder_ == null) {
          ensureContainersIsMutable();
          containers_.remove(index);
          onChanged();
        } else {
          containersBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.Builder getContainersBuilder(
          int index) {
        return getContainersFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProtoOrBuilder getContainersOrBuilder(
          int index) {
        if (containersBuilder_ == null) {
          return containers_.get(index);  } else {
          return containersBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProtoOrBuilder> 
           getContainersOrBuilderList() {
        if (containersBuilder_ != null) {
          return containersBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(containers_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.Builder addContainersBuilder() {
        return getContainersFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.Builder addContainersBuilder(
          int index) {
        return getContainersFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ContainerReportProto containers = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.Builder> 
           getContainersBuilderList() {
        return getContainersFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProtoOrBuilder> 
          getContainersFieldBuilder() {
        if (containersBuilder_ == null) {
          containersBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto, org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ContainerReportProtoOrBuilder>(
                  containers_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          containers_ = null;
        }
        return containersBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetContainersResponseProto)
    }

    static {
      defaultInstance = new GetContainersResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetContainersResponseProto)
  }

  public interface UseSharedCacheResourceRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ApplicationIdProto applicationId = 1;
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
     */
    boolean hasApplicationId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder();

    // optional string resourceKey = 2;
    /**
     * <code>optional string resourceKey = 2;</code>
     */
    boolean hasResourceKey();
    /**
     * <code>optional string resourceKey = 2;</code>
     */
    java.lang.String getResourceKey();
    /**
     * <code>optional string resourceKey = 2;</code>
     */
    com.google.protobuf.ByteString
        getResourceKeyBytes();
  }
  /**
   * Protobuf type {@code hadoop.yarn.UseSharedCacheResourceRequestProto}
   */
  public static final class UseSharedCacheResourceRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements UseSharedCacheResourceRequestProtoOrBuilder {
    // Use UseSharedCacheResourceRequestProto.newBuilder() to construct.
    private UseSharedCacheResourceRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private UseSharedCacheResourceRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final UseSharedCacheResourceRequestProto defaultInstance;
    public static UseSharedCacheResourceRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public UseSharedCacheResourceRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private UseSharedCacheResourceRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = applicationId_.toBuilder();
              }
              applicationId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationId_);
                applicationId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              resourceKey_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UseSharedCacheResourceRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UseSharedCacheResourceRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<UseSharedCacheResourceRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<UseSharedCacheResourceRequestProto>() {
      public UseSharedCacheResourceRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new UseSharedCacheResourceRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<UseSharedCacheResourceRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ApplicationIdProto applicationId = 1;
    public static final int APPLICATIONID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto applicationId_;
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
     */
    public boolean hasApplicationId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId() {
      return applicationId_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder() {
      return applicationId_;
    }

    // optional string resourceKey = 2;
    public static final int RESOURCEKEY_FIELD_NUMBER = 2;
    private java.lang.Object resourceKey_;
    /**
     * <code>optional string resourceKey = 2;</code>
     */
    public boolean hasResourceKey() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional string resourceKey = 2;</code>
     */
    public java.lang.String getResourceKey() {
      java.lang.Object ref = resourceKey_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          resourceKey_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string resourceKey = 2;</code>
     */
    public com.google.protobuf.ByteString
        getResourceKeyBytes() {
      java.lang.Object ref = resourceKey_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        resourceKey_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private void initFields() {
      applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
      resourceKey_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, applicationId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, getResourceKeyBytes());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, applicationId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, getResourceKeyBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto) obj;

      boolean result = true;
      result = result && (hasApplicationId() == other.hasApplicationId());
      if (hasApplicationId()) {
        result = result && getApplicationId()
            .equals(other.getApplicationId());
      }
      result = result && (hasResourceKey() == other.hasResourceKey());
      if (hasResourceKey()) {
        result = result && getResourceKey()
            .equals(other.getResourceKey());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasApplicationId()) {
        hash = (37 * hash) + APPLICATIONID_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationId().hashCode();
      }
      if (hasResourceKey()) {
        hash = (37 * hash) + RESOURCEKEY_FIELD_NUMBER;
        hash = (53 * hash) + getResourceKey().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.UseSharedCacheResourceRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UseSharedCacheResourceRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UseSharedCacheResourceRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getApplicationIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (applicationIdBuilder_ == null) {
          applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
        } else {
          applicationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        resourceKey_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UseSharedCacheResourceRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (applicationIdBuilder_ == null) {
          result.applicationId_ = applicationId_;
        } else {
          result.applicationId_ = applicationIdBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.resourceKey_ = resourceKey_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto.getDefaultInstance()) return this;
        if (other.hasApplicationId()) {
          mergeApplicationId(other.getApplicationId());
        }
        if (other.hasResourceKey()) {
          bitField0_ |= 0x00000002;
          resourceKey_ = other.resourceKey_;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ApplicationIdProto applicationId = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> applicationIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      public boolean hasApplicationId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId() {
        if (applicationIdBuilder_ == null) {
          return applicationId_;
        } else {
          return applicationIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      public Builder setApplicationId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationId_ = value;
          onChanged();
        } else {
          applicationIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      public Builder setApplicationId(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder builderForValue) {
        if (applicationIdBuilder_ == null) {
          applicationId_ = builderForValue.build();
          onChanged();
        } else {
          applicationIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      public Builder mergeApplicationId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              applicationId_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance()) {
            applicationId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.newBuilder(applicationId_).mergeFrom(value).buildPartial();
          } else {
            applicationId_ = value;
          }
          onChanged();
        } else {
          applicationIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      public Builder clearApplicationId() {
        if (applicationIdBuilder_ == null) {
          applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
          onChanged();
        } else {
          applicationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder getApplicationIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getApplicationIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder() {
        if (applicationIdBuilder_ != null) {
          return applicationIdBuilder_.getMessageOrBuilder();
        } else {
          return applicationId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> 
          getApplicationIdFieldBuilder() {
        if (applicationIdBuilder_ == null) {
          applicationIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder>(
                  applicationId_,
                  getParentForChildren(),
                  isClean());
          applicationId_ = null;
        }
        return applicationIdBuilder_;
      }

      // optional string resourceKey = 2;
      private java.lang.Object resourceKey_ = "";
      /**
       * <code>optional string resourceKey = 2;</code>
       */
      public boolean hasResourceKey() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional string resourceKey = 2;</code>
       */
      public java.lang.String getResourceKey() {
        java.lang.Object ref = resourceKey_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          resourceKey_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string resourceKey = 2;</code>
       */
      public com.google.protobuf.ByteString
          getResourceKeyBytes() {
        java.lang.Object ref = resourceKey_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          resourceKey_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string resourceKey = 2;</code>
       */
      public Builder setResourceKey(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        resourceKey_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string resourceKey = 2;</code>
       */
      public Builder clearResourceKey() {
        bitField0_ = (bitField0_ & ~0x00000002);
        resourceKey_ = getDefaultInstance().getResourceKey();
        onChanged();
        return this;
      }
      /**
       * <code>optional string resourceKey = 2;</code>
       */
      public Builder setResourceKeyBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        resourceKey_ = value;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.UseSharedCacheResourceRequestProto)
    }

    static {
      defaultInstance = new UseSharedCacheResourceRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.UseSharedCacheResourceRequestProto)
  }

  public interface UseSharedCacheResourceResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional string path = 1;
    /**
     * <code>optional string path = 1;</code>
     */
    boolean hasPath();
    /**
     * <code>optional string path = 1;</code>
     */
    java.lang.String getPath();
    /**
     * <code>optional string path = 1;</code>
     */
    com.google.protobuf.ByteString
        getPathBytes();
  }
  /**
   * Protobuf type {@code hadoop.yarn.UseSharedCacheResourceResponseProto}
   */
  public static final class UseSharedCacheResourceResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements UseSharedCacheResourceResponseProtoOrBuilder {
    // Use UseSharedCacheResourceResponseProto.newBuilder() to construct.
    private UseSharedCacheResourceResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private UseSharedCacheResourceResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final UseSharedCacheResourceResponseProto defaultInstance;
    public static UseSharedCacheResourceResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public UseSharedCacheResourceResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private UseSharedCacheResourceResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              path_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UseSharedCacheResourceResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UseSharedCacheResourceResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<UseSharedCacheResourceResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<UseSharedCacheResourceResponseProto>() {
      public UseSharedCacheResourceResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new UseSharedCacheResourceResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<UseSharedCacheResourceResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional string path = 1;
    public static final int PATH_FIELD_NUMBER = 1;
    private java.lang.Object path_;
    /**
     * <code>optional string path = 1;</code>
     */
    public boolean hasPath() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional string path = 1;</code>
     */
    public java.lang.String getPath() {
      java.lang.Object ref = path_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          path_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string path = 1;</code>
     */
    public com.google.protobuf.ByteString
        getPathBytes() {
      java.lang.Object ref = path_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        path_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private void initFields() {
      path_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getPathBytes());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getPathBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto) obj;

      boolean result = true;
      result = result && (hasPath() == other.hasPath());
      if (hasPath()) {
        result = result && getPath()
            .equals(other.getPath());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasPath()) {
        hash = (37 * hash) + PATH_FIELD_NUMBER;
        hash = (53 * hash) + getPath().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.UseSharedCacheResourceResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UseSharedCacheResourceResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UseSharedCacheResourceResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        path_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_UseSharedCacheResourceResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.path_ = path_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto.getDefaultInstance()) return this;
        if (other.hasPath()) {
          bitField0_ |= 0x00000001;
          path_ = other.path_;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.UseSharedCacheResourceResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional string path = 1;
      private java.lang.Object path_ = "";
      /**
       * <code>optional string path = 1;</code>
       */
      public boolean hasPath() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional string path = 1;</code>
       */
      public java.lang.String getPath() {
        java.lang.Object ref = path_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          path_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string path = 1;</code>
       */
      public com.google.protobuf.ByteString
          getPathBytes() {
        java.lang.Object ref = path_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          path_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string path = 1;</code>
       */
      public Builder setPath(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        path_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string path = 1;</code>
       */
      public Builder clearPath() {
        bitField0_ = (bitField0_ & ~0x00000001);
        path_ = getDefaultInstance().getPath();
        onChanged();
        return this;
      }
      /**
       * <code>optional string path = 1;</code>
       */
      public Builder setPathBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        path_ = value;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.UseSharedCacheResourceResponseProto)
    }

    static {
      defaultInstance = new UseSharedCacheResourceResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.UseSharedCacheResourceResponseProto)
  }

  public interface ReleaseSharedCacheResourceRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ApplicationIdProto applicationId = 1;
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
     */
    boolean hasApplicationId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId();
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder();

    // optional string resourceKey = 2;
    /**
     * <code>optional string resourceKey = 2;</code>
     */
    boolean hasResourceKey();
    /**
     * <code>optional string resourceKey = 2;</code>
     */
    java.lang.String getResourceKey();
    /**
     * <code>optional string resourceKey = 2;</code>
     */
    com.google.protobuf.ByteString
        getResourceKeyBytes();
  }
  /**
   * Protobuf type {@code hadoop.yarn.ReleaseSharedCacheResourceRequestProto}
   */
  public static final class ReleaseSharedCacheResourceRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements ReleaseSharedCacheResourceRequestProtoOrBuilder {
    // Use ReleaseSharedCacheResourceRequestProto.newBuilder() to construct.
    private ReleaseSharedCacheResourceRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private ReleaseSharedCacheResourceRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final ReleaseSharedCacheResourceRequestProto defaultInstance;
    public static ReleaseSharedCacheResourceRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public ReleaseSharedCacheResourceRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private ReleaseSharedCacheResourceRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = applicationId_.toBuilder();
              }
              applicationId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(applicationId_);
                applicationId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              resourceKey_ = input.readBytes();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReleaseSharedCacheResourceRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReleaseSharedCacheResourceRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<ReleaseSharedCacheResourceRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<ReleaseSharedCacheResourceRequestProto>() {
      public ReleaseSharedCacheResourceRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ReleaseSharedCacheResourceRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<ReleaseSharedCacheResourceRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ApplicationIdProto applicationId = 1;
    public static final int APPLICATIONID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto applicationId_;
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
     */
    public boolean hasApplicationId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId() {
      return applicationId_;
    }
    /**
     * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder() {
      return applicationId_;
    }

    // optional string resourceKey = 2;
    public static final int RESOURCEKEY_FIELD_NUMBER = 2;
    private java.lang.Object resourceKey_;
    /**
     * <code>optional string resourceKey = 2;</code>
     */
    public boolean hasResourceKey() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional string resourceKey = 2;</code>
     */
    public java.lang.String getResourceKey() {
      java.lang.Object ref = resourceKey_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          resourceKey_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string resourceKey = 2;</code>
     */
    public com.google.protobuf.ByteString
        getResourceKeyBytes() {
      java.lang.Object ref = resourceKey_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        resourceKey_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private void initFields() {
      applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
      resourceKey_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, applicationId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, getResourceKeyBytes());
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, applicationId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, getResourceKeyBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto) obj;

      boolean result = true;
      result = result && (hasApplicationId() == other.hasApplicationId());
      if (hasApplicationId()) {
        result = result && getApplicationId()
            .equals(other.getApplicationId());
      }
      result = result && (hasResourceKey() == other.hasResourceKey());
      if (hasResourceKey()) {
        result = result && getResourceKey()
            .equals(other.getResourceKey());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasApplicationId()) {
        hash = (37 * hash) + APPLICATIONID_FIELD_NUMBER;
        hash = (53 * hash) + getApplicationId().hashCode();
      }
      if (hasResourceKey()) {
        hash = (37 * hash) + RESOURCEKEY_FIELD_NUMBER;
        hash = (53 * hash) + getResourceKey().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.ReleaseSharedCacheResourceRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReleaseSharedCacheResourceRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReleaseSharedCacheResourceRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getApplicationIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (applicationIdBuilder_ == null) {
          applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
        } else {
          applicationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        resourceKey_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReleaseSharedCacheResourceRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (applicationIdBuilder_ == null) {
          result.applicationId_ = applicationId_;
        } else {
          result.applicationId_ = applicationIdBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.resourceKey_ = resourceKey_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto.getDefaultInstance()) return this;
        if (other.hasApplicationId()) {
          mergeApplicationId(other.getApplicationId());
        }
        if (other.hasResourceKey()) {
          bitField0_ |= 0x00000002;
          resourceKey_ = other.resourceKey_;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ApplicationIdProto applicationId = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> applicationIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      public boolean hasApplicationId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto getApplicationId() {
        if (applicationIdBuilder_ == null) {
          return applicationId_;
        } else {
          return applicationIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      public Builder setApplicationId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          applicationId_ = value;
          onChanged();
        } else {
          applicationIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      public Builder setApplicationId(
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder builderForValue) {
        if (applicationIdBuilder_ == null) {
          applicationId_ = builderForValue.build();
          onChanged();
        } else {
          applicationIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      public Builder mergeApplicationId(org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto value) {
        if (applicationIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              applicationId_ != org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance()) {
            applicationId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.newBuilder(applicationId_).mergeFrom(value).buildPartial();
          } else {
            applicationId_ = value;
          }
          onChanged();
        } else {
          applicationIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      public Builder clearApplicationId() {
        if (applicationIdBuilder_ == null) {
          applicationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.getDefaultInstance();
          onChanged();
        } else {
          applicationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder getApplicationIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getApplicationIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder getApplicationIdOrBuilder() {
        if (applicationIdBuilder_ != null) {
          return applicationIdBuilder_.getMessageOrBuilder();
        } else {
          return applicationId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ApplicationIdProto applicationId = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder> 
          getApplicationIdFieldBuilder() {
        if (applicationIdBuilder_ == null) {
          applicationIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ApplicationIdProtoOrBuilder>(
                  applicationId_,
                  getParentForChildren(),
                  isClean());
          applicationId_ = null;
        }
        return applicationIdBuilder_;
      }

      // optional string resourceKey = 2;
      private java.lang.Object resourceKey_ = "";
      /**
       * <code>optional string resourceKey = 2;</code>
       */
      public boolean hasResourceKey() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional string resourceKey = 2;</code>
       */
      public java.lang.String getResourceKey() {
        java.lang.Object ref = resourceKey_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          resourceKey_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string resourceKey = 2;</code>
       */
      public com.google.protobuf.ByteString
          getResourceKeyBytes() {
        java.lang.Object ref = resourceKey_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          resourceKey_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string resourceKey = 2;</code>
       */
      public Builder setResourceKey(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        resourceKey_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string resourceKey = 2;</code>
       */
      public Builder clearResourceKey() {
        bitField0_ = (bitField0_ & ~0x00000002);
        resourceKey_ = getDefaultInstance().getResourceKey();
        onChanged();
        return this;
      }
      /**
       * <code>optional string resourceKey = 2;</code>
       */
      public Builder setResourceKeyBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        resourceKey_ = value;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.ReleaseSharedCacheResourceRequestProto)
    }

    static {
      defaultInstance = new ReleaseSharedCacheResourceRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.ReleaseSharedCacheResourceRequestProto)
  }

  public interface ReleaseSharedCacheResourceResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.yarn.ReleaseSharedCacheResourceResponseProto}
   */
  public static final class ReleaseSharedCacheResourceResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements ReleaseSharedCacheResourceResponseProtoOrBuilder {
    // Use ReleaseSharedCacheResourceResponseProto.newBuilder() to construct.
    private ReleaseSharedCacheResourceResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private ReleaseSharedCacheResourceResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final ReleaseSharedCacheResourceResponseProto defaultInstance;
    public static ReleaseSharedCacheResourceResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public ReleaseSharedCacheResourceResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private ReleaseSharedCacheResourceResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReleaseSharedCacheResourceResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReleaseSharedCacheResourceResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<ReleaseSharedCacheResourceResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<ReleaseSharedCacheResourceResponseProto>() {
      public ReleaseSharedCacheResourceResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ReleaseSharedCacheResourceResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<ReleaseSharedCacheResourceResponseProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.ReleaseSharedCacheResourceResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReleaseSharedCacheResourceResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReleaseSharedCacheResourceResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReleaseSharedCacheResourceResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.ReleaseSharedCacheResourceResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.ReleaseSharedCacheResourceResponseProto)
    }

    static {
      defaultInstance = new ReleaseSharedCacheResourceResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.ReleaseSharedCacheResourceResponseProto)
  }

  public interface GetNewReservationRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetNewReservationRequestProto}
   */
  public static final class GetNewReservationRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements GetNewReservationRequestProtoOrBuilder {
    // Use GetNewReservationRequestProto.newBuilder() to construct.
    private GetNewReservationRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetNewReservationRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetNewReservationRequestProto defaultInstance;
    public static GetNewReservationRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetNewReservationRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetNewReservationRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewReservationRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewReservationRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetNewReservationRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<GetNewReservationRequestProto>() {
      public GetNewReservationRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetNewReservationRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetNewReservationRequestProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetNewReservationRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewReservationRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewReservationRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewReservationRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetNewReservationRequestProto)
    }

    static {
      defaultInstance = new GetNewReservationRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetNewReservationRequestProto)
  }

  public interface GetNewReservationResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ReservationIdProto reservation_id = 1;
    /**
     * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
     */
    boolean hasReservationId();
    /**
     * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto getReservationId();
    /**
     * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProtoOrBuilder getReservationIdOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.GetNewReservationResponseProto}
   */
  public static final class GetNewReservationResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements GetNewReservationResponseProtoOrBuilder {
    // Use GetNewReservationResponseProto.newBuilder() to construct.
    private GetNewReservationResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private GetNewReservationResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final GetNewReservationResponseProto defaultInstance;
    public static GetNewReservationResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public GetNewReservationResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private GetNewReservationResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = reservationId_.toBuilder();
              }
              reservationId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(reservationId_);
                reservationId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewReservationResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewReservationResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<GetNewReservationResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<GetNewReservationResponseProto>() {
      public GetNewReservationResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new GetNewReservationResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<GetNewReservationResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ReservationIdProto reservation_id = 1;
    public static final int RESERVATION_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto reservationId_;
    /**
     * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
     */
    public boolean hasReservationId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto getReservationId() {
      return reservationId_;
    }
    /**
     * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProtoOrBuilder getReservationIdOrBuilder() {
      return reservationId_;
    }

    private void initFields() {
      reservationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, reservationId_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, reservationId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto) obj;

      boolean result = true;
      result = result && (hasReservationId() == other.hasReservationId());
      if (hasReservationId()) {
        result = result && getReservationId()
            .equals(other.getReservationId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasReservationId()) {
        hash = (37 * hash) + RESERVATION_ID_FIELD_NUMBER;
        hash = (53 * hash) + getReservationId().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.GetNewReservationResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewReservationResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewReservationResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getReservationIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (reservationIdBuilder_ == null) {
          reservationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.getDefaultInstance();
        } else {
          reservationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_GetNewReservationResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (reservationIdBuilder_ == null) {
          result.reservationId_ = reservationId_;
        } else {
          result.reservationId_ = reservationIdBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto.getDefaultInstance()) return this;
        if (other.hasReservationId()) {
          mergeReservationId(other.getReservationId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.GetNewReservationResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ReservationIdProto reservation_id = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto reservationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProtoOrBuilder> reservationIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
       */
      public boolean hasReservationId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto getReservationId() {
        if (reservationIdBuilder_ == null) {
          return reservationId_;
        } else {
          return reservationIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
       */
      public Builder setReservationId(org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto value) {
        if (reservationIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          reservationId_ = value;
          onChanged();
        } else {
          reservationIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
       */
      public Builder setReservationId(
          org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.Builder builderForValue) {
        if (reservationIdBuilder_ == null) {
          reservationId_ = builderForValue.build();
          onChanged();
        } else {
          reservationIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
       */
      public Builder mergeReservationId(org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto value) {
        if (reservationIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              reservationId_ != org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.getDefaultInstance()) {
            reservationId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.newBuilder(reservationId_).mergeFrom(value).buildPartial();
          } else {
            reservationId_ = value;
          }
          onChanged();
        } else {
          reservationIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
       */
      public Builder clearReservationId() {
        if (reservationIdBuilder_ == null) {
          reservationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.getDefaultInstance();
          onChanged();
        } else {
          reservationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.Builder getReservationIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getReservationIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProtoOrBuilder getReservationIdOrBuilder() {
        if (reservationIdBuilder_ != null) {
          return reservationIdBuilder_.getMessageOrBuilder();
        } else {
          return reservationId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProtoOrBuilder> 
          getReservationIdFieldBuilder() {
        if (reservationIdBuilder_ == null) {
          reservationIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProtoOrBuilder>(
                  reservationId_,
                  getParentForChildren(),
                  isClean());
          reservationId_ = null;
        }
        return reservationIdBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.GetNewReservationResponseProto)
    }

    static {
      defaultInstance = new GetNewReservationResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.GetNewReservationResponseProto)
  }

  public interface ReservationSubmissionRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional string queue = 1;
    /**
     * <code>optional string queue = 1;</code>
     */
    boolean hasQueue();
    /**
     * <code>optional string queue = 1;</code>
     */
    java.lang.String getQueue();
    /**
     * <code>optional string queue = 1;</code>
     */
    com.google.protobuf.ByteString
        getQueueBytes();

    // optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 2;
    /**
     * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 2;</code>
     */
    boolean hasReservationDefinition();
    /**
     * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto getReservationDefinition();
    /**
     * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProtoOrBuilder getReservationDefinitionOrBuilder();

    // optional .hadoop.yarn.ReservationIdProto reservation_id = 3;
    /**
     * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 3;</code>
     */
    boolean hasReservationId();
    /**
     * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto getReservationId();
    /**
     * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 3;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProtoOrBuilder getReservationIdOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.ReservationSubmissionRequestProto}
   */
  public static final class ReservationSubmissionRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements ReservationSubmissionRequestProtoOrBuilder {
    // Use ReservationSubmissionRequestProto.newBuilder() to construct.
    private ReservationSubmissionRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private ReservationSubmissionRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final ReservationSubmissionRequestProto defaultInstance;
    public static ReservationSubmissionRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public ReservationSubmissionRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private ReservationSubmissionRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              queue_ = input.readBytes();
              break;
            }
            case 18: {
              org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) == 0x00000002)) {
                subBuilder = reservationDefinition_.toBuilder();
              }
              reservationDefinition_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(reservationDefinition_);
                reservationDefinition_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
            case 26: {
              org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000004) == 0x00000004)) {
                subBuilder = reservationId_.toBuilder();
              }
              reservationId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(reservationId_);
                reservationId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000004;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationSubmissionRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationSubmissionRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<ReservationSubmissionRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<ReservationSubmissionRequestProto>() {
      public ReservationSubmissionRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ReservationSubmissionRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<ReservationSubmissionRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional string queue = 1;
    public static final int QUEUE_FIELD_NUMBER = 1;
    private java.lang.Object queue_;
    /**
     * <code>optional string queue = 1;</code>
     */
    public boolean hasQueue() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional string queue = 1;</code>
     */
    public java.lang.String getQueue() {
      java.lang.Object ref = queue_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          queue_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string queue = 1;</code>
     */
    public com.google.protobuf.ByteString
        getQueueBytes() {
      java.lang.Object ref = queue_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        queue_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 2;
    public static final int RESERVATION_DEFINITION_FIELD_NUMBER = 2;
    private org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto reservationDefinition_;
    /**
     * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 2;</code>
     */
    public boolean hasReservationDefinition() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto getReservationDefinition() {
      return reservationDefinition_;
    }
    /**
     * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProtoOrBuilder getReservationDefinitionOrBuilder() {
      return reservationDefinition_;
    }

    // optional .hadoop.yarn.ReservationIdProto reservation_id = 3;
    public static final int RESERVATION_ID_FIELD_NUMBER = 3;
    private org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto reservationId_;
    /**
     * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 3;</code>
     */
    public boolean hasReservationId() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto getReservationId() {
      return reservationId_;
    }
    /**
     * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 3;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProtoOrBuilder getReservationIdOrBuilder() {
      return reservationId_;
    }

    private void initFields() {
      queue_ = "";
      reservationDefinition_ = org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.getDefaultInstance();
      reservationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getQueueBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeMessage(2, reservationDefinition_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeMessage(3, reservationId_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getQueueBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, reservationDefinition_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, reservationId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto) obj;

      boolean result = true;
      result = result && (hasQueue() == other.hasQueue());
      if (hasQueue()) {
        result = result && getQueue()
            .equals(other.getQueue());
      }
      result = result && (hasReservationDefinition() == other.hasReservationDefinition());
      if (hasReservationDefinition()) {
        result = result && getReservationDefinition()
            .equals(other.getReservationDefinition());
      }
      result = result && (hasReservationId() == other.hasReservationId());
      if (hasReservationId()) {
        result = result && getReservationId()
            .equals(other.getReservationId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasQueue()) {
        hash = (37 * hash) + QUEUE_FIELD_NUMBER;
        hash = (53 * hash) + getQueue().hashCode();
      }
      if (hasReservationDefinition()) {
        hash = (37 * hash) + RESERVATION_DEFINITION_FIELD_NUMBER;
        hash = (53 * hash) + getReservationDefinition().hashCode();
      }
      if (hasReservationId()) {
        hash = (37 * hash) + RESERVATION_ID_FIELD_NUMBER;
        hash = (53 * hash) + getReservationId().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.ReservationSubmissionRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationSubmissionRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationSubmissionRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getReservationDefinitionFieldBuilder();
          getReservationIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        queue_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        if (reservationDefinitionBuilder_ == null) {
          reservationDefinition_ = org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.getDefaultInstance();
        } else {
          reservationDefinitionBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        if (reservationIdBuilder_ == null) {
          reservationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.getDefaultInstance();
        } else {
          reservationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationSubmissionRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.queue_ = queue_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        if (reservationDefinitionBuilder_ == null) {
          result.reservationDefinition_ = reservationDefinition_;
        } else {
          result.reservationDefinition_ = reservationDefinitionBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        if (reservationIdBuilder_ == null) {
          result.reservationId_ = reservationId_;
        } else {
          result.reservationId_ = reservationIdBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto.getDefaultInstance()) return this;
        if (other.hasQueue()) {
          bitField0_ |= 0x00000001;
          queue_ = other.queue_;
          onChanged();
        }
        if (other.hasReservationDefinition()) {
          mergeReservationDefinition(other.getReservationDefinition());
        }
        if (other.hasReservationId()) {
          mergeReservationId(other.getReservationId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional string queue = 1;
      private java.lang.Object queue_ = "";
      /**
       * <code>optional string queue = 1;</code>
       */
      public boolean hasQueue() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional string queue = 1;</code>
       */
      public java.lang.String getQueue() {
        java.lang.Object ref = queue_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          queue_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string queue = 1;</code>
       */
      public com.google.protobuf.ByteString
          getQueueBytes() {
        java.lang.Object ref = queue_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          queue_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string queue = 1;</code>
       */
      public Builder setQueue(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        queue_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string queue = 1;</code>
       */
      public Builder clearQueue() {
        bitField0_ = (bitField0_ & ~0x00000001);
        queue_ = getDefaultInstance().getQueue();
        onChanged();
        return this;
      }
      /**
       * <code>optional string queue = 1;</code>
       */
      public Builder setQueueBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        queue_ = value;
        onChanged();
        return this;
      }

      // optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 2;
      private org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto reservationDefinition_ = org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto, org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProtoOrBuilder> reservationDefinitionBuilder_;
      /**
       * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 2;</code>
       */
      public boolean hasReservationDefinition() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto getReservationDefinition() {
        if (reservationDefinitionBuilder_ == null) {
          return reservationDefinition_;
        } else {
          return reservationDefinitionBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 2;</code>
       */
      public Builder setReservationDefinition(org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto value) {
        if (reservationDefinitionBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          reservationDefinition_ = value;
          onChanged();
        } else {
          reservationDefinitionBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 2;</code>
       */
      public Builder setReservationDefinition(
          org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.Builder builderForValue) {
        if (reservationDefinitionBuilder_ == null) {
          reservationDefinition_ = builderForValue.build();
          onChanged();
        } else {
          reservationDefinitionBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 2;</code>
       */
      public Builder mergeReservationDefinition(org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto value) {
        if (reservationDefinitionBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002) &&
              reservationDefinition_ != org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.getDefaultInstance()) {
            reservationDefinition_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.newBuilder(reservationDefinition_).mergeFrom(value).buildPartial();
          } else {
            reservationDefinition_ = value;
          }
          onChanged();
        } else {
          reservationDefinitionBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 2;</code>
       */
      public Builder clearReservationDefinition() {
        if (reservationDefinitionBuilder_ == null) {
          reservationDefinition_ = org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.getDefaultInstance();
          onChanged();
        } else {
          reservationDefinitionBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.Builder getReservationDefinitionBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getReservationDefinitionFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProtoOrBuilder getReservationDefinitionOrBuilder() {
        if (reservationDefinitionBuilder_ != null) {
          return reservationDefinitionBuilder_.getMessageOrBuilder();
        } else {
          return reservationDefinition_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto, org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProtoOrBuilder> 
          getReservationDefinitionFieldBuilder() {
        if (reservationDefinitionBuilder_ == null) {
          reservationDefinitionBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto, org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProtoOrBuilder>(
                  reservationDefinition_,
                  getParentForChildren(),
                  isClean());
          reservationDefinition_ = null;
        }
        return reservationDefinitionBuilder_;
      }

      // optional .hadoop.yarn.ReservationIdProto reservation_id = 3;
      private org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto reservationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProtoOrBuilder> reservationIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 3;</code>
       */
      public boolean hasReservationId() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto getReservationId() {
        if (reservationIdBuilder_ == null) {
          return reservationId_;
        } else {
          return reservationIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 3;</code>
       */
      public Builder setReservationId(org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto value) {
        if (reservationIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          reservationId_ = value;
          onChanged();
        } else {
          reservationIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 3;</code>
       */
      public Builder setReservationId(
          org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.Builder builderForValue) {
        if (reservationIdBuilder_ == null) {
          reservationId_ = builderForValue.build();
          onChanged();
        } else {
          reservationIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 3;</code>
       */
      public Builder mergeReservationId(org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto value) {
        if (reservationIdBuilder_ == null) {
          if (((bitField0_ & 0x00000004) == 0x00000004) &&
              reservationId_ != org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.getDefaultInstance()) {
            reservationId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.newBuilder(reservationId_).mergeFrom(value).buildPartial();
          } else {
            reservationId_ = value;
          }
          onChanged();
        } else {
          reservationIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 3;</code>
       */
      public Builder clearReservationId() {
        if (reservationIdBuilder_ == null) {
          reservationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.getDefaultInstance();
          onChanged();
        } else {
          reservationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.Builder getReservationIdBuilder() {
        bitField0_ |= 0x00000004;
        onChanged();
        return getReservationIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 3;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProtoOrBuilder getReservationIdOrBuilder() {
        if (reservationIdBuilder_ != null) {
          return reservationIdBuilder_.getMessageOrBuilder();
        } else {
          return reservationId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProtoOrBuilder> 
          getReservationIdFieldBuilder() {
        if (reservationIdBuilder_ == null) {
          reservationIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProtoOrBuilder>(
                  reservationId_,
                  getParentForChildren(),
                  isClean());
          reservationId_ = null;
        }
        return reservationIdBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.ReservationSubmissionRequestProto)
    }

    static {
      defaultInstance = new ReservationSubmissionRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.ReservationSubmissionRequestProto)
  }

  public interface ReservationSubmissionResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.yarn.ReservationSubmissionResponseProto}
   */
  public static final class ReservationSubmissionResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements ReservationSubmissionResponseProtoOrBuilder {
    // Use ReservationSubmissionResponseProto.newBuilder() to construct.
    private ReservationSubmissionResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private ReservationSubmissionResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final ReservationSubmissionResponseProto defaultInstance;
    public static ReservationSubmissionResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public ReservationSubmissionResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private ReservationSubmissionResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationSubmissionResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationSubmissionResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<ReservationSubmissionResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<ReservationSubmissionResponseProto>() {
      public ReservationSubmissionResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ReservationSubmissionResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<ReservationSubmissionResponseProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.ReservationSubmissionResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationSubmissionResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationSubmissionResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationSubmissionResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationSubmissionResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.ReservationSubmissionResponseProto)
    }

    static {
      defaultInstance = new ReservationSubmissionResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.ReservationSubmissionResponseProto)
  }

  public interface ReservationUpdateRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 1;
    /**
     * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 1;</code>
     */
    boolean hasReservationDefinition();
    /**
     * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto getReservationDefinition();
    /**
     * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProtoOrBuilder getReservationDefinitionOrBuilder();

    // optional .hadoop.yarn.ReservationIdProto reservation_id = 2;
    /**
     * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 2;</code>
     */
    boolean hasReservationId();
    /**
     * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto getReservationId();
    /**
     * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 2;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProtoOrBuilder getReservationIdOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.ReservationUpdateRequestProto}
   */
  public static final class ReservationUpdateRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements ReservationUpdateRequestProtoOrBuilder {
    // Use ReservationUpdateRequestProto.newBuilder() to construct.
    private ReservationUpdateRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private ReservationUpdateRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final ReservationUpdateRequestProto defaultInstance;
    public static ReservationUpdateRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public ReservationUpdateRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private ReservationUpdateRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = reservationDefinition_.toBuilder();
              }
              reservationDefinition_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(reservationDefinition_);
                reservationDefinition_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
            case 18: {
              org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000002) == 0x00000002)) {
                subBuilder = reservationId_.toBuilder();
              }
              reservationId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(reservationId_);
                reservationId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000002;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationUpdateRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationUpdateRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<ReservationUpdateRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<ReservationUpdateRequestProto>() {
      public ReservationUpdateRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ReservationUpdateRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<ReservationUpdateRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 1;
    public static final int RESERVATION_DEFINITION_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto reservationDefinition_;
    /**
     * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 1;</code>
     */
    public boolean hasReservationDefinition() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto getReservationDefinition() {
      return reservationDefinition_;
    }
    /**
     * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProtoOrBuilder getReservationDefinitionOrBuilder() {
      return reservationDefinition_;
    }

    // optional .hadoop.yarn.ReservationIdProto reservation_id = 2;
    public static final int RESERVATION_ID_FIELD_NUMBER = 2;
    private org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto reservationId_;
    /**
     * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 2;</code>
     */
    public boolean hasReservationId() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto getReservationId() {
      return reservationId_;
    }
    /**
     * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 2;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProtoOrBuilder getReservationIdOrBuilder() {
      return reservationId_;
    }

    private void initFields() {
      reservationDefinition_ = org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.getDefaultInstance();
      reservationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, reservationDefinition_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeMessage(2, reservationId_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, reservationDefinition_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, reservationId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto) obj;

      boolean result = true;
      result = result && (hasReservationDefinition() == other.hasReservationDefinition());
      if (hasReservationDefinition()) {
        result = result && getReservationDefinition()
            .equals(other.getReservationDefinition());
      }
      result = result && (hasReservationId() == other.hasReservationId());
      if (hasReservationId()) {
        result = result && getReservationId()
            .equals(other.getReservationId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasReservationDefinition()) {
        hash = (37 * hash) + RESERVATION_DEFINITION_FIELD_NUMBER;
        hash = (53 * hash) + getReservationDefinition().hashCode();
      }
      if (hasReservationId()) {
        hash = (37 * hash) + RESERVATION_ID_FIELD_NUMBER;
        hash = (53 * hash) + getReservationId().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.ReservationUpdateRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationUpdateRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationUpdateRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getReservationDefinitionFieldBuilder();
          getReservationIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (reservationDefinitionBuilder_ == null) {
          reservationDefinition_ = org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.getDefaultInstance();
        } else {
          reservationDefinitionBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (reservationIdBuilder_ == null) {
          reservationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.getDefaultInstance();
        } else {
          reservationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationUpdateRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (reservationDefinitionBuilder_ == null) {
          result.reservationDefinition_ = reservationDefinition_;
        } else {
          result.reservationDefinition_ = reservationDefinitionBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        if (reservationIdBuilder_ == null) {
          result.reservationId_ = reservationId_;
        } else {
          result.reservationId_ = reservationIdBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto.getDefaultInstance()) return this;
        if (other.hasReservationDefinition()) {
          mergeReservationDefinition(other.getReservationDefinition());
        }
        if (other.hasReservationId()) {
          mergeReservationId(other.getReservationId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto reservationDefinition_ = org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto, org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProtoOrBuilder> reservationDefinitionBuilder_;
      /**
       * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 1;</code>
       */
      public boolean hasReservationDefinition() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto getReservationDefinition() {
        if (reservationDefinitionBuilder_ == null) {
          return reservationDefinition_;
        } else {
          return reservationDefinitionBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 1;</code>
       */
      public Builder setReservationDefinition(org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto value) {
        if (reservationDefinitionBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          reservationDefinition_ = value;
          onChanged();
        } else {
          reservationDefinitionBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 1;</code>
       */
      public Builder setReservationDefinition(
          org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.Builder builderForValue) {
        if (reservationDefinitionBuilder_ == null) {
          reservationDefinition_ = builderForValue.build();
          onChanged();
        } else {
          reservationDefinitionBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 1;</code>
       */
      public Builder mergeReservationDefinition(org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto value) {
        if (reservationDefinitionBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              reservationDefinition_ != org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.getDefaultInstance()) {
            reservationDefinition_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.newBuilder(reservationDefinition_).mergeFrom(value).buildPartial();
          } else {
            reservationDefinition_ = value;
          }
          onChanged();
        } else {
          reservationDefinitionBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 1;</code>
       */
      public Builder clearReservationDefinition() {
        if (reservationDefinitionBuilder_ == null) {
          reservationDefinition_ = org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.getDefaultInstance();
          onChanged();
        } else {
          reservationDefinitionBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.Builder getReservationDefinitionBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getReservationDefinitionFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProtoOrBuilder getReservationDefinitionOrBuilder() {
        if (reservationDefinitionBuilder_ != null) {
          return reservationDefinitionBuilder_.getMessageOrBuilder();
        } else {
          return reservationDefinition_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ReservationDefinitionProto reservation_definition = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto, org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProtoOrBuilder> 
          getReservationDefinitionFieldBuilder() {
        if (reservationDefinitionBuilder_ == null) {
          reservationDefinitionBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto, org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ReservationDefinitionProtoOrBuilder>(
                  reservationDefinition_,
                  getParentForChildren(),
                  isClean());
          reservationDefinition_ = null;
        }
        return reservationDefinitionBuilder_;
      }

      // optional .hadoop.yarn.ReservationIdProto reservation_id = 2;
      private org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto reservationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProtoOrBuilder> reservationIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 2;</code>
       */
      public boolean hasReservationId() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto getReservationId() {
        if (reservationIdBuilder_ == null) {
          return reservationId_;
        } else {
          return reservationIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 2;</code>
       */
      public Builder setReservationId(org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto value) {
        if (reservationIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          reservationId_ = value;
          onChanged();
        } else {
          reservationIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 2;</code>
       */
      public Builder setReservationId(
          org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.Builder builderForValue) {
        if (reservationIdBuilder_ == null) {
          reservationId_ = builderForValue.build();
          onChanged();
        } else {
          reservationIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 2;</code>
       */
      public Builder mergeReservationId(org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto value) {
        if (reservationIdBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002) &&
              reservationId_ != org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.getDefaultInstance()) {
            reservationId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.newBuilder(reservationId_).mergeFrom(value).buildPartial();
          } else {
            reservationId_ = value;
          }
          onChanged();
        } else {
          reservationIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 2;</code>
       */
      public Builder clearReservationId() {
        if (reservationIdBuilder_ == null) {
          reservationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.getDefaultInstance();
          onChanged();
        } else {
          reservationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.Builder getReservationIdBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getReservationIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 2;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProtoOrBuilder getReservationIdOrBuilder() {
        if (reservationIdBuilder_ != null) {
          return reservationIdBuilder_.getMessageOrBuilder();
        } else {
          return reservationId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProtoOrBuilder> 
          getReservationIdFieldBuilder() {
        if (reservationIdBuilder_ == null) {
          reservationIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProtoOrBuilder>(
                  reservationId_,
                  getParentForChildren(),
                  isClean());
          reservationId_ = null;
        }
        return reservationIdBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.ReservationUpdateRequestProto)
    }

    static {
      defaultInstance = new ReservationUpdateRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.ReservationUpdateRequestProto)
  }

  public interface ReservationUpdateResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.yarn.ReservationUpdateResponseProto}
   */
  public static final class ReservationUpdateResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements ReservationUpdateResponseProtoOrBuilder {
    // Use ReservationUpdateResponseProto.newBuilder() to construct.
    private ReservationUpdateResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private ReservationUpdateResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final ReservationUpdateResponseProto defaultInstance;
    public static ReservationUpdateResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public ReservationUpdateResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private ReservationUpdateResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationUpdateResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationUpdateResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<ReservationUpdateResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<ReservationUpdateResponseProto>() {
      public ReservationUpdateResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ReservationUpdateResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<ReservationUpdateResponseProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.ReservationUpdateResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationUpdateResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationUpdateResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationUpdateResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationUpdateResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.ReservationUpdateResponseProto)
    }

    static {
      defaultInstance = new ReservationUpdateResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.ReservationUpdateResponseProto)
  }

  public interface ReservationDeleteRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional .hadoop.yarn.ReservationIdProto reservation_id = 1;
    /**
     * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
     */
    boolean hasReservationId();
    /**
     * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto getReservationId();
    /**
     * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProtoOrBuilder getReservationIdOrBuilder();
  }
  /**
   * Protobuf type {@code hadoop.yarn.ReservationDeleteRequestProto}
   */
  public static final class ReservationDeleteRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements ReservationDeleteRequestProtoOrBuilder {
    // Use ReservationDeleteRequestProto.newBuilder() to construct.
    private ReservationDeleteRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private ReservationDeleteRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final ReservationDeleteRequestProto defaultInstance;
    public static ReservationDeleteRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public ReservationDeleteRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private ReservationDeleteRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.Builder subBuilder = null;
              if (((bitField0_ & 0x00000001) == 0x00000001)) {
                subBuilder = reservationId_.toBuilder();
              }
              reservationId_ = input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.PARSER, extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(reservationId_);
                reservationId_ = subBuilder.buildPartial();
              }
              bitField0_ |= 0x00000001;
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationDeleteRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationDeleteRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<ReservationDeleteRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<ReservationDeleteRequestProto>() {
      public ReservationDeleteRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ReservationDeleteRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<ReservationDeleteRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional .hadoop.yarn.ReservationIdProto reservation_id = 1;
    public static final int RESERVATION_ID_FIELD_NUMBER = 1;
    private org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto reservationId_;
    /**
     * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
     */
    public boolean hasReservationId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto getReservationId() {
      return reservationId_;
    }
    /**
     * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProtoOrBuilder getReservationIdOrBuilder() {
      return reservationId_;
    }

    private void initFields() {
      reservationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, reservationId_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, reservationId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto) obj;

      boolean result = true;
      result = result && (hasReservationId() == other.hasReservationId());
      if (hasReservationId()) {
        result = result && getReservationId()
            .equals(other.getReservationId());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasReservationId()) {
        hash = (37 * hash) + RESERVATION_ID_FIELD_NUMBER;
        hash = (53 * hash) + getReservationId().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.ReservationDeleteRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationDeleteRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationDeleteRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getReservationIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (reservationIdBuilder_ == null) {
          reservationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.getDefaultInstance();
        } else {
          reservationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationDeleteRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (reservationIdBuilder_ == null) {
          result.reservationId_ = reservationId_;
        } else {
          result.reservationId_ = reservationIdBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto.getDefaultInstance()) return this;
        if (other.hasReservationId()) {
          mergeReservationId(other.getReservationId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional .hadoop.yarn.ReservationIdProto reservation_id = 1;
      private org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto reservationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProtoOrBuilder> reservationIdBuilder_;
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
       */
      public boolean hasReservationId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto getReservationId() {
        if (reservationIdBuilder_ == null) {
          return reservationId_;
        } else {
          return reservationIdBuilder_.getMessage();
        }
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
       */
      public Builder setReservationId(org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto value) {
        if (reservationIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          reservationId_ = value;
          onChanged();
        } else {
          reservationIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
       */
      public Builder setReservationId(
          org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.Builder builderForValue) {
        if (reservationIdBuilder_ == null) {
          reservationId_ = builderForValue.build();
          onChanged();
        } else {
          reservationIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
       */
      public Builder mergeReservationId(org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto value) {
        if (reservationIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              reservationId_ != org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.getDefaultInstance()) {
            reservationId_ =
              org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.newBuilder(reservationId_).mergeFrom(value).buildPartial();
          } else {
            reservationId_ = value;
          }
          onChanged();
        } else {
          reservationIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
       */
      public Builder clearReservationId() {
        if (reservationIdBuilder_ == null) {
          reservationId_ = org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.getDefaultInstance();
          onChanged();
        } else {
          reservationIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.Builder getReservationIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getReservationIdFieldBuilder().getBuilder();
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProtoOrBuilder getReservationIdOrBuilder() {
        if (reservationIdBuilder_ != null) {
          return reservationIdBuilder_.getMessageOrBuilder();
        } else {
          return reservationId_;
        }
      }
      /**
       * <code>optional .hadoop.yarn.ReservationIdProto reservation_id = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProtoOrBuilder> 
          getReservationIdFieldBuilder() {
        if (reservationIdBuilder_ == null) {
          reservationIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto, org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ReservationIdProtoOrBuilder>(
                  reservationId_,
                  getParentForChildren(),
                  isClean());
          reservationId_ = null;
        }
        return reservationIdBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.ReservationDeleteRequestProto)
    }

    static {
      defaultInstance = new ReservationDeleteRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.ReservationDeleteRequestProto)
  }

  public interface ReservationDeleteResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.yarn.ReservationDeleteResponseProto}
   */
  public static final class ReservationDeleteResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements ReservationDeleteResponseProtoOrBuilder {
    // Use ReservationDeleteResponseProto.newBuilder() to construct.
    private ReservationDeleteResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private ReservationDeleteResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final ReservationDeleteResponseProto defaultInstance;
    public static ReservationDeleteResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public ReservationDeleteResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private ReservationDeleteResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationDeleteResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationDeleteResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<ReservationDeleteResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<ReservationDeleteResponseProto>() {
      public ReservationDeleteResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ReservationDeleteResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<ReservationDeleteResponseProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.ReservationDeleteResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationDeleteResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationDeleteResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationDeleteResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationDeleteResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.ReservationDeleteResponseProto)
    }

    static {
      defaultInstance = new ReservationDeleteResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.ReservationDeleteResponseProto)
  }

  public interface ReservationListRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional string queue = 1;
    /**
     * <code>optional string queue = 1;</code>
     */
    boolean hasQueue();
    /**
     * <code>optional string queue = 1;</code>
     */
    java.lang.String getQueue();
    /**
     * <code>optional string queue = 1;</code>
     */
    com.google.protobuf.ByteString
        getQueueBytes();

    // optional string reservation_id = 3;
    /**
     * <code>optional string reservation_id = 3;</code>
     */
    boolean hasReservationId();
    /**
     * <code>optional string reservation_id = 3;</code>
     */
    java.lang.String getReservationId();
    /**
     * <code>optional string reservation_id = 3;</code>
     */
    com.google.protobuf.ByteString
        getReservationIdBytes();

    // optional int64 start_time = 4;
    /**
     * <code>optional int64 start_time = 4;</code>
     */
    boolean hasStartTime();
    /**
     * <code>optional int64 start_time = 4;</code>
     */
    long getStartTime();

    // optional int64 end_time = 5;
    /**
     * <code>optional int64 end_time = 5;</code>
     */
    boolean hasEndTime();
    /**
     * <code>optional int64 end_time = 5;</code>
     */
    long getEndTime();

    // optional bool include_resource_allocations = 6;
    /**
     * <code>optional bool include_resource_allocations = 6;</code>
     */
    boolean hasIncludeResourceAllocations();
    /**
     * <code>optional bool include_resource_allocations = 6;</code>
     */
    boolean getIncludeResourceAllocations();
  }
  /**
   * Protobuf type {@code hadoop.yarn.ReservationListRequestProto}
   */
  public static final class ReservationListRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements ReservationListRequestProtoOrBuilder {
    // Use ReservationListRequestProto.newBuilder() to construct.
    private ReservationListRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private ReservationListRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final ReservationListRequestProto defaultInstance;
    public static ReservationListRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public ReservationListRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private ReservationListRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              queue_ = input.readBytes();
              break;
            }
            case 26: {
              bitField0_ |= 0x00000002;
              reservationId_ = input.readBytes();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000004;
              startTime_ = input.readInt64();
              break;
            }
            case 40: {
              bitField0_ |= 0x00000008;
              endTime_ = input.readInt64();
              break;
            }
            case 48: {
              bitField0_ |= 0x00000010;
              includeResourceAllocations_ = input.readBool();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationListRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationListRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<ReservationListRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<ReservationListRequestProto>() {
      public ReservationListRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ReservationListRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<ReservationListRequestProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional string queue = 1;
    public static final int QUEUE_FIELD_NUMBER = 1;
    private java.lang.Object queue_;
    /**
     * <code>optional string queue = 1;</code>
     */
    public boolean hasQueue() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional string queue = 1;</code>
     */
    public java.lang.String getQueue() {
      java.lang.Object ref = queue_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          queue_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string queue = 1;</code>
     */
    public com.google.protobuf.ByteString
        getQueueBytes() {
      java.lang.Object ref = queue_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        queue_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional string reservation_id = 3;
    public static final int RESERVATION_ID_FIELD_NUMBER = 3;
    private java.lang.Object reservationId_;
    /**
     * <code>optional string reservation_id = 3;</code>
     */
    public boolean hasReservationId() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    /**
     * <code>optional string reservation_id = 3;</code>
     */
    public java.lang.String getReservationId() {
      java.lang.Object ref = reservationId_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        if (bs.isValidUtf8()) {
          reservationId_ = s;
        }
        return s;
      }
    }
    /**
     * <code>optional string reservation_id = 3;</code>
     */
    public com.google.protobuf.ByteString
        getReservationIdBytes() {
      java.lang.Object ref = reservationId_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        reservationId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    // optional int64 start_time = 4;
    public static final int START_TIME_FIELD_NUMBER = 4;
    private long startTime_;
    /**
     * <code>optional int64 start_time = 4;</code>
     */
    public boolean hasStartTime() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    /**
     * <code>optional int64 start_time = 4;</code>
     */
    public long getStartTime() {
      return startTime_;
    }

    // optional int64 end_time = 5;
    public static final int END_TIME_FIELD_NUMBER = 5;
    private long endTime_;
    /**
     * <code>optional int64 end_time = 5;</code>
     */
    public boolean hasEndTime() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    /**
     * <code>optional int64 end_time = 5;</code>
     */
    public long getEndTime() {
      return endTime_;
    }

    // optional bool include_resource_allocations = 6;
    public static final int INCLUDE_RESOURCE_ALLOCATIONS_FIELD_NUMBER = 6;
    private boolean includeResourceAllocations_;
    /**
     * <code>optional bool include_resource_allocations = 6;</code>
     */
    public boolean hasIncludeResourceAllocations() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    /**
     * <code>optional bool include_resource_allocations = 6;</code>
     */
    public boolean getIncludeResourceAllocations() {
      return includeResourceAllocations_;
    }

    private void initFields() {
      queue_ = "";
      reservationId_ = "";
      startTime_ = 0L;
      endTime_ = 0L;
      includeResourceAllocations_ = false;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getQueueBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(3, getReservationIdBytes());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeInt64(4, startTime_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeInt64(5, endTime_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeBool(6, includeResourceAllocations_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getQueueBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(3, getReservationIdBytes());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(4, startTime_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(5, endTime_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(6, includeResourceAllocations_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto) obj;

      boolean result = true;
      result = result && (hasQueue() == other.hasQueue());
      if (hasQueue()) {
        result = result && getQueue()
            .equals(other.getQueue());
      }
      result = result && (hasReservationId() == other.hasReservationId());
      if (hasReservationId()) {
        result = result && getReservationId()
            .equals(other.getReservationId());
      }
      result = result && (hasStartTime() == other.hasStartTime());
      if (hasStartTime()) {
        result = result && (getStartTime()
            == other.getStartTime());
      }
      result = result && (hasEndTime() == other.hasEndTime());
      if (hasEndTime()) {
        result = result && (getEndTime()
            == other.getEndTime());
      }
      result = result && (hasIncludeResourceAllocations() == other.hasIncludeResourceAllocations());
      if (hasIncludeResourceAllocations()) {
        result = result && (getIncludeResourceAllocations()
            == other.getIncludeResourceAllocations());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasQueue()) {
        hash = (37 * hash) + QUEUE_FIELD_NUMBER;
        hash = (53 * hash) + getQueue().hashCode();
      }
      if (hasReservationId()) {
        hash = (37 * hash) + RESERVATION_ID_FIELD_NUMBER;
        hash = (53 * hash) + getReservationId().hashCode();
      }
      if (hasStartTime()) {
        hash = (37 * hash) + START_TIME_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getStartTime());
      }
      if (hasEndTime()) {
        hash = (37 * hash) + END_TIME_FIELD_NUMBER;
        hash = (53 * hash) + hashLong(getEndTime());
      }
      if (hasIncludeResourceAllocations()) {
        hash = (37 * hash) + INCLUDE_RESOURCE_ALLOCATIONS_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getIncludeResourceAllocations());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.ReservationListRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationListRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationListRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        queue_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        reservationId_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        startTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        endTime_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        includeResourceAllocations_ = false;
        bitField0_ = (bitField0_ & ~0x00000010);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationListRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.queue_ = queue_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.reservationId_ = reservationId_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.startTime_ = startTime_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.endTime_ = endTime_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000010;
        }
        result.includeResourceAllocations_ = includeResourceAllocations_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto.getDefaultInstance()) return this;
        if (other.hasQueue()) {
          bitField0_ |= 0x00000001;
          queue_ = other.queue_;
          onChanged();
        }
        if (other.hasReservationId()) {
          bitField0_ |= 0x00000002;
          reservationId_ = other.reservationId_;
          onChanged();
        }
        if (other.hasStartTime()) {
          setStartTime(other.getStartTime());
        }
        if (other.hasEndTime()) {
          setEndTime(other.getEndTime());
        }
        if (other.hasIncludeResourceAllocations()) {
          setIncludeResourceAllocations(other.getIncludeResourceAllocations());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional string queue = 1;
      private java.lang.Object queue_ = "";
      /**
       * <code>optional string queue = 1;</code>
       */
      public boolean hasQueue() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional string queue = 1;</code>
       */
      public java.lang.String getQueue() {
        java.lang.Object ref = queue_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          queue_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string queue = 1;</code>
       */
      public com.google.protobuf.ByteString
          getQueueBytes() {
        java.lang.Object ref = queue_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          queue_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string queue = 1;</code>
       */
      public Builder setQueue(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        queue_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string queue = 1;</code>
       */
      public Builder clearQueue() {
        bitField0_ = (bitField0_ & ~0x00000001);
        queue_ = getDefaultInstance().getQueue();
        onChanged();
        return this;
      }
      /**
       * <code>optional string queue = 1;</code>
       */
      public Builder setQueueBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        queue_ = value;
        onChanged();
        return this;
      }

      // optional string reservation_id = 3;
      private java.lang.Object reservationId_ = "";
      /**
       * <code>optional string reservation_id = 3;</code>
       */
      public boolean hasReservationId() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      /**
       * <code>optional string reservation_id = 3;</code>
       */
      public java.lang.String getReservationId() {
        java.lang.Object ref = reservationId_;
        if (!(ref instanceof java.lang.String)) {
          java.lang.String s = ((com.google.protobuf.ByteString) ref)
              .toStringUtf8();
          reservationId_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <code>optional string reservation_id = 3;</code>
       */
      public com.google.protobuf.ByteString
          getReservationIdBytes() {
        java.lang.Object ref = reservationId_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          reservationId_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>optional string reservation_id = 3;</code>
       */
      public Builder setReservationId(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        reservationId_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional string reservation_id = 3;</code>
       */
      public Builder clearReservationId() {
        bitField0_ = (bitField0_ & ~0x00000002);
        reservationId_ = getDefaultInstance().getReservationId();
        onChanged();
        return this;
      }
      /**
       * <code>optional string reservation_id = 3;</code>
       */
      public Builder setReservationIdBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        reservationId_ = value;
        onChanged();
        return this;
      }

      // optional int64 start_time = 4;
      private long startTime_ ;
      /**
       * <code>optional int64 start_time = 4;</code>
       */
      public boolean hasStartTime() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      /**
       * <code>optional int64 start_time = 4;</code>
       */
      public long getStartTime() {
        return startTime_;
      }
      /**
       * <code>optional int64 start_time = 4;</code>
       */
      public Builder setStartTime(long value) {
        bitField0_ |= 0x00000004;
        startTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 start_time = 4;</code>
       */
      public Builder clearStartTime() {
        bitField0_ = (bitField0_ & ~0x00000004);
        startTime_ = 0L;
        onChanged();
        return this;
      }

      // optional int64 end_time = 5;
      private long endTime_ ;
      /**
       * <code>optional int64 end_time = 5;</code>
       */
      public boolean hasEndTime() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      /**
       * <code>optional int64 end_time = 5;</code>
       */
      public long getEndTime() {
        return endTime_;
      }
      /**
       * <code>optional int64 end_time = 5;</code>
       */
      public Builder setEndTime(long value) {
        bitField0_ |= 0x00000008;
        endTime_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional int64 end_time = 5;</code>
       */
      public Builder clearEndTime() {
        bitField0_ = (bitField0_ & ~0x00000008);
        endTime_ = 0L;
        onChanged();
        return this;
      }

      // optional bool include_resource_allocations = 6;
      private boolean includeResourceAllocations_ ;
      /**
       * <code>optional bool include_resource_allocations = 6;</code>
       */
      public boolean hasIncludeResourceAllocations() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      /**
       * <code>optional bool include_resource_allocations = 6;</code>
       */
      public boolean getIncludeResourceAllocations() {
        return includeResourceAllocations_;
      }
      /**
       * <code>optional bool include_resource_allocations = 6;</code>
       */
      public Builder setIncludeResourceAllocations(boolean value) {
        bitField0_ |= 0x00000010;
        includeResourceAllocations_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool include_resource_allocations = 6;</code>
       */
      public Builder clearIncludeResourceAllocations() {
        bitField0_ = (bitField0_ & ~0x00000010);
        includeResourceAllocations_ = false;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.ReservationListRequestProto)
    }

    static {
      defaultInstance = new ReservationListRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.ReservationListRequestProto)
  }

  public interface ReservationListResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;
    /**
     * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
     */
    java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto> 
        getReservationsList();
    /**
     * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto getReservations(int index);
    /**
     * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
     */
    int getReservationsCount();
    /**
     * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
     */
    java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProtoOrBuilder> 
        getReservationsOrBuilderList();
    /**
     * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
     */
    org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProtoOrBuilder getReservationsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code hadoop.yarn.ReservationListResponseProto}
   */
  public static final class ReservationListResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements ReservationListResponseProtoOrBuilder {
    // Use ReservationListResponseProto.newBuilder() to construct.
    private ReservationListResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private ReservationListResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final ReservationListResponseProto defaultInstance;
    public static ReservationListResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public ReservationListResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private ReservationListResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                reservations_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto>();
                mutable_bitField0_ |= 0x00000001;
              }
              reservations_.add(input.readMessage(org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto.PARSER, extensionRegistry));
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          reservations_ = java.util.Collections.unmodifiableList(reservations_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationListResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationListResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<ReservationListResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<ReservationListResponseProto>() {
      public ReservationListResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ReservationListResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<ReservationListResponseProto> getParserForType() {
      return PARSER;
    }

    // repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;
    public static final int RESERVATIONS_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto> reservations_;
    /**
     * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
     */
    public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto> getReservationsList() {
      return reservations_;
    }
    /**
     * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
     */
    public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProtoOrBuilder> 
        getReservationsOrBuilderList() {
      return reservations_;
    }
    /**
     * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
     */
    public int getReservationsCount() {
      return reservations_.size();
    }
    /**
     * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto getReservations(int index) {
      return reservations_.get(index);
    }
    /**
     * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
     */
    public org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProtoOrBuilder getReservationsOrBuilder(
        int index) {
      return reservations_.get(index);
    }

    private void initFields() {
      reservations_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < reservations_.size(); i++) {
        output.writeMessage(1, reservations_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < reservations_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, reservations_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto) obj;

      boolean result = true;
      result = result && getReservationsList()
          .equals(other.getReservationsList());
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (getReservationsCount() > 0) {
        hash = (37 * hash) + RESERVATIONS_FIELD_NUMBER;
        hash = (53 * hash) + getReservationsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.ReservationListResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationListResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationListResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getReservationsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        if (reservationsBuilder_ == null) {
          reservations_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          reservationsBuilder_.clear();
        }
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_ReservationListResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto(this);
        int from_bitField0_ = bitField0_;
        if (reservationsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            reservations_ = java.util.Collections.unmodifiableList(reservations_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.reservations_ = reservations_;
        } else {
          result.reservations_ = reservationsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto.getDefaultInstance()) return this;
        if (reservationsBuilder_ == null) {
          if (!other.reservations_.isEmpty()) {
            if (reservations_.isEmpty()) {
              reservations_ = other.reservations_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureReservationsIsMutable();
              reservations_.addAll(other.reservations_);
            }
            onChanged();
          }
        } else {
          if (!other.reservations_.isEmpty()) {
            if (reservationsBuilder_.isEmpty()) {
              reservationsBuilder_.dispose();
              reservationsBuilder_ = null;
              reservations_ = other.reservations_;
              bitField0_ = (bitField0_ & ~0x00000001);
              reservationsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getReservationsFieldBuilder() : null;
            } else {
              reservationsBuilder_.addAllMessages(other.reservations_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.ReservationListResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;
      private java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto> reservations_ =
        java.util.Collections.emptyList();
      private void ensureReservationsIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          reservations_ = new java.util.ArrayList<org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto>(reservations_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto, org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProtoOrBuilder> reservationsBuilder_;

      /**
       * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto> getReservationsList() {
        if (reservationsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(reservations_);
        } else {
          return reservationsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
       */
      public int getReservationsCount() {
        if (reservationsBuilder_ == null) {
          return reservations_.size();
        } else {
          return reservationsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto getReservations(int index) {
        if (reservationsBuilder_ == null) {
          return reservations_.get(index);
        } else {
          return reservationsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
       */
      public Builder setReservations(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto value) {
        if (reservationsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureReservationsIsMutable();
          reservations_.set(index, value);
          onChanged();
        } else {
          reservationsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
       */
      public Builder setReservations(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto.Builder builderForValue) {
        if (reservationsBuilder_ == null) {
          ensureReservationsIsMutable();
          reservations_.set(index, builderForValue.build());
          onChanged();
        } else {
          reservationsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
       */
      public Builder addReservations(org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto value) {
        if (reservationsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureReservationsIsMutable();
          reservations_.add(value);
          onChanged();
        } else {
          reservationsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
       */
      public Builder addReservations(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto value) {
        if (reservationsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureReservationsIsMutable();
          reservations_.add(index, value);
          onChanged();
        } else {
          reservationsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
       */
      public Builder addReservations(
          org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto.Builder builderForValue) {
        if (reservationsBuilder_ == null) {
          ensureReservationsIsMutable();
          reservations_.add(builderForValue.build());
          onChanged();
        } else {
          reservationsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
       */
      public Builder addReservations(
          int index, org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto.Builder builderForValue) {
        if (reservationsBuilder_ == null) {
          ensureReservationsIsMutable();
          reservations_.add(index, builderForValue.build());
          onChanged();
        } else {
          reservationsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
       */
      public Builder addAllReservations(
          java.lang.Iterable<? extends org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto> values) {
        if (reservationsBuilder_ == null) {
          ensureReservationsIsMutable();
          super.addAll(values, reservations_);
          onChanged();
        } else {
          reservationsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
       */
      public Builder clearReservations() {
        if (reservationsBuilder_ == null) {
          reservations_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          reservationsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
       */
      public Builder removeReservations(int index) {
        if (reservationsBuilder_ == null) {
          ensureReservationsIsMutable();
          reservations_.remove(index);
          onChanged();
        } else {
          reservationsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto.Builder getReservationsBuilder(
          int index) {
        return getReservationsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProtoOrBuilder getReservationsOrBuilder(
          int index) {
        if (reservationsBuilder_ == null) {
          return reservations_.get(index);  } else {
          return reservationsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
       */
      public java.util.List<? extends org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProtoOrBuilder> 
           getReservationsOrBuilderList() {
        if (reservationsBuilder_ != null) {
          return reservationsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(reservations_);
        }
      }
      /**
       * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto.Builder addReservationsBuilder() {
        return getReservationsFieldBuilder().addBuilder(
            org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
       */
      public org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto.Builder addReservationsBuilder(
          int index) {
        return getReservationsFieldBuilder().addBuilder(
            index, org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto.getDefaultInstance());
      }
      /**
       * <code>repeated .hadoop.yarn.ReservationAllocationStateProto reservations = 1;</code>
       */
      public java.util.List<org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto.Builder> 
           getReservationsBuilderList() {
        return getReservationsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto, org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProtoOrBuilder> 
          getReservationsFieldBuilder() {
        if (reservationsBuilder_ == null) {
          reservationsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto, org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProto.Builder, org.apache.hadoop.yarn.proto.YarnProtos.ReservationAllocationStateProtoOrBuilder>(
                  reservations_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          reservations_ = null;
        }
        return reservationsBuilder_;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.ReservationListResponseProto)
    }

    static {
      defaultInstance = new ReservationListResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.ReservationListResponseProto)
  }

  public interface RunSharedCacheCleanerTaskRequestProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code hadoop.yarn.RunSharedCacheCleanerTaskRequestProto}
   */
  public static final class RunSharedCacheCleanerTaskRequestProto extends
      com.google.protobuf.GeneratedMessage
      implements RunSharedCacheCleanerTaskRequestProtoOrBuilder {
    // Use RunSharedCacheCleanerTaskRequestProto.newBuilder() to construct.
    private RunSharedCacheCleanerTaskRequestProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private RunSharedCacheCleanerTaskRequestProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final RunSharedCacheCleanerTaskRequestProto defaultInstance;
    public static RunSharedCacheCleanerTaskRequestProto getDefaultInstance() {
      return defaultInstance;
    }

    public RunSharedCacheCleanerTaskRequestProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private RunSharedCacheCleanerTaskRequestProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RunSharedCacheCleanerTaskRequestProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RunSharedCacheCleanerTaskRequestProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto.Builder.class);
    }

    public static com.google.protobuf.Parser<RunSharedCacheCleanerTaskRequestProto> PARSER =
        new com.google.protobuf.AbstractParser<RunSharedCacheCleanerTaskRequestProto>() {
      public RunSharedCacheCleanerTaskRequestProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RunSharedCacheCleanerTaskRequestProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<RunSharedCacheCleanerTaskRequestProto> getParserForType() {
      return PARSER;
    }

    private void initFields() {
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto) obj;

      boolean result = true;
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.RunSharedCacheCleanerTaskRequestProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RunSharedCacheCleanerTaskRequestProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RunSharedCacheCleanerTaskRequestProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RunSharedCacheCleanerTaskRequestProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto(this);
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskRequestProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.RunSharedCacheCleanerTaskRequestProto)
    }

    static {
      defaultInstance = new RunSharedCacheCleanerTaskRequestProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.RunSharedCacheCleanerTaskRequestProto)
  }

  public interface RunSharedCacheCleanerTaskResponseProtoOrBuilder
      extends com.google.protobuf.MessageOrBuilder {

    // optional bool accepted = 1;
    /**
     * <code>optional bool accepted = 1;</code>
     */
    boolean hasAccepted();
    /**
     * <code>optional bool accepted = 1;</code>
     */
    boolean getAccepted();
  }
  /**
   * Protobuf type {@code hadoop.yarn.RunSharedCacheCleanerTaskResponseProto}
   */
  public static final class RunSharedCacheCleanerTaskResponseProto extends
      com.google.protobuf.GeneratedMessage
      implements RunSharedCacheCleanerTaskResponseProtoOrBuilder {
    // Use RunSharedCacheCleanerTaskResponseProto.newBuilder() to construct.
    private RunSharedCacheCleanerTaskResponseProto(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
      super(builder);
      this.unknownFields = builder.getUnknownFields();
    }
    private RunSharedCacheCleanerTaskResponseProto(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }

    private static final RunSharedCacheCleanerTaskResponseProto defaultInstance;
    public static RunSharedCacheCleanerTaskResponseProto getDefaultInstance() {
      return defaultInstance;
    }

    public RunSharedCacheCleanerTaskResponseProto getDefaultInstanceForType() {
      return defaultInstance;
    }

    private final com.google.protobuf.UnknownFieldSet unknownFields;
    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
      return this.unknownFields;
    }
    private RunSharedCacheCleanerTaskResponseProto(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      initFields();
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              accepted_ = input.readBool();
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e.getMessage()).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RunSharedCacheCleanerTaskResponseProto_descriptor;
    }

    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RunSharedCacheCleanerTaskResponseProto_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto.Builder.class);
    }

    public static com.google.protobuf.Parser<RunSharedCacheCleanerTaskResponseProto> PARSER =
        new com.google.protobuf.AbstractParser<RunSharedCacheCleanerTaskResponseProto>() {
      public RunSharedCacheCleanerTaskResponseProto parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new RunSharedCacheCleanerTaskResponseProto(input, extensionRegistry);
      }
    };

    @java.lang.Override
    public com.google.protobuf.Parser<RunSharedCacheCleanerTaskResponseProto> getParserForType() {
      return PARSER;
    }

    private int bitField0_;
    // optional bool accepted = 1;
    public static final int ACCEPTED_FIELD_NUMBER = 1;
    private boolean accepted_;
    /**
     * <code>optional bool accepted = 1;</code>
     */
    public boolean hasAccepted() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    /**
     * <code>optional bool accepted = 1;</code>
     */
    public boolean getAccepted() {
      return accepted_;
    }

    private void initFields() {
      accepted_ = false;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;

      memoizedIsInitialized = 1;
      return true;
    }

    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBool(1, accepted_);
      }
      getUnknownFields().writeTo(output);
    }

    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;

      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(1, accepted_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }

    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto)) {
        return super.equals(obj);
      }
      org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto other = (org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto) obj;

      boolean result = true;
      result = result && (hasAccepted() == other.hasAccepted());
      if (hasAccepted()) {
        result = result && (getAccepted()
            == other.getAccepted());
      }
      result = result &&
          getUnknownFields().equals(other.getUnknownFields());
      return result;
    }

    private int memoizedHashCode = 0;
    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptorForType().hashCode();
      if (hasAccepted()) {
        hash = (37 * hash) + ACCEPTED_FIELD_NUMBER;
        hash = (53 * hash) + hashBoolean(getAccepted());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseDelimitedFrom(input, extensionRegistry);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return PARSER.parseFrom(input);
    }
    public static org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return PARSER.parseFrom(input, extensionRegistry);
    }

    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code hadoop.yarn.RunSharedCacheCleanerTaskResponseProto}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProtoOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RunSharedCacheCleanerTaskResponseProto_descriptor;
      }

      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RunSharedCacheCleanerTaskResponseProto_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto.class, org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto.Builder.class);
      }

      // Construct using org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }

      public Builder clear() {
        super.clear();
        accepted_ = false;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }

      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.internal_static_hadoop_yarn_RunSharedCacheCleanerTaskResponseProto_descriptor;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto getDefaultInstanceForType() {
        return org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto.getDefaultInstance();
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto build() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      public org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto buildPartial() {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto result = new org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.accepted_ = accepted_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto) {
          return mergeFrom((org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto other) {
        if (other == org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto.getDefaultInstance()) return this;
        if (other.hasAccepted()) {
          setAccepted(other.getAccepted());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }

      public final boolean isInitialized() {
        return true;
      }

      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (org.apache.hadoop.yarn.proto.YarnServiceProtos.RunSharedCacheCleanerTaskResponseProto) e.getUnfinishedMessage();
          throw e;
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      // optional bool accepted = 1;
      private boolean accepted_ ;
      /**
       * <code>optional bool accepted = 1;</code>
       */
      public boolean hasAccepted() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      /**
       * <code>optional bool accepted = 1;</code>
       */
      public boolean getAccepted() {
        return accepted_;
      }
      /**
       * <code>optional bool accepted = 1;</code>
       */
      public Builder setAccepted(boolean value) {
        bitField0_ |= 0x00000001;
        accepted_ = value;
        onChanged();
        return this;
      }
      /**
       * <code>optional bool accepted = 1;</code>
       */
      public Builder clearAccepted() {
        bitField0_ = (bitField0_ & ~0x00000001);
        accepted_ = false;
        onChanged();
        return this;
      }

      // @@protoc_insertion_point(builder_scope:hadoop.yarn.RunSharedCacheCleanerTaskResponseProto)
    }

    static {
      defaultInstance = new RunSharedCacheCleanerTaskResponseProto(true);
      defaultInstance.initFields();
    }

    // @@protoc_insertion_point(class_scope:hadoop.yarn.RunSharedCacheCleanerTaskResponseProto)
  }

  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_RegisterApplicationMasterRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_RegisterApplicationMasterRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_RegisterApplicationMasterResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_RegisterApplicationMasterResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_FinishApplicationMasterRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_FinishApplicationMasterRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_FinishApplicationMasterResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_FinishApplicationMasterResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_UpdateContainerRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_UpdateContainerRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_UpdateContainerErrorProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_UpdateContainerErrorProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_AllocateRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_AllocateRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_NMTokenProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_NMTokenProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_UpdatedContainerProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_UpdatedContainerProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_AllocateResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_AllocateResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetNewApplicationRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetNewApplicationRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetNewApplicationResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetNewApplicationResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetApplicationReportRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetApplicationReportRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetApplicationReportResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetApplicationReportResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_SubmitApplicationRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_SubmitApplicationRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_SubmitApplicationResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_SubmitApplicationResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_FailApplicationAttemptRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_FailApplicationAttemptRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_FailApplicationAttemptResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_FailApplicationAttemptResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_KillApplicationRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_KillApplicationRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_KillApplicationResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_KillApplicationResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetClusterMetricsRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetClusterMetricsRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetClusterMetricsResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetClusterMetricsResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_MoveApplicationAcrossQueuesRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_MoveApplicationAcrossQueuesRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_MoveApplicationAcrossQueuesResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_MoveApplicationAcrossQueuesResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetApplicationsRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetApplicationsRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetApplicationsResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetApplicationsResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetClusterNodesRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetClusterNodesRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetClusterNodesResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetClusterNodesResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetQueueInfoRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetQueueInfoRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetQueueInfoResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetQueueInfoResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetQueueUserAclsInfoRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetQueueUserAclsInfoRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetQueueUserAclsInfoResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetQueueUserAclsInfoResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetNodesToLabelsRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetNodesToLabelsRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetNodesToLabelsResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetNodesToLabelsResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetLabelsToNodesRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetLabelsToNodesRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetLabelsToNodesResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetLabelsToNodesResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetClusterNodeLabelsRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetClusterNodeLabelsRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetClusterNodeLabelsResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetClusterNodeLabelsResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_UpdateApplicationPriorityRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_UpdateApplicationPriorityRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_UpdateApplicationPriorityResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_UpdateApplicationPriorityResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_SignalContainerRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_SignalContainerRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_SignalContainerResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_SignalContainerResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_StartContainerRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_StartContainerRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_StartContainerResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_StartContainerResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_StopContainerRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_StopContainerRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_StopContainerResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_StopContainerResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_StartContainersRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_StartContainersRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_ContainerExceptionMapProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_ContainerExceptionMapProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_StartContainersResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_StartContainersResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_StopContainersRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_StopContainersRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_StopContainersResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_StopContainersResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetContainerStatusesRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetContainerStatusesRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetContainerStatusesResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetContainerStatusesResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_IncreaseContainersResourceRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_IncreaseContainersResourceRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_IncreaseContainersResourceResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_IncreaseContainersResourceResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetApplicationAttemptReportRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetApplicationAttemptReportRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetApplicationAttemptReportResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetApplicationAttemptReportResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetApplicationAttemptsRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetApplicationAttemptsRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetApplicationAttemptsResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetApplicationAttemptsResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetContainerReportRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetContainerReportRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetContainerReportResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetContainerReportResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetContainersRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetContainersRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetContainersResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetContainersResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_UseSharedCacheResourceRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_UseSharedCacheResourceRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_UseSharedCacheResourceResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_UseSharedCacheResourceResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_ReleaseSharedCacheResourceRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_ReleaseSharedCacheResourceRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_ReleaseSharedCacheResourceResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_ReleaseSharedCacheResourceResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetNewReservationRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetNewReservationRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_GetNewReservationResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_GetNewReservationResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_ReservationSubmissionRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_ReservationSubmissionRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_ReservationSubmissionResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_ReservationSubmissionResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_ReservationUpdateRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_ReservationUpdateRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_ReservationUpdateResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_ReservationUpdateResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_ReservationDeleteRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_ReservationDeleteRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_ReservationDeleteResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_ReservationDeleteResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_ReservationListRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_ReservationListRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_ReservationListResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_ReservationListResponseProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_RunSharedCacheCleanerTaskRequestProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_RunSharedCacheCleanerTaskRequestProto_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_hadoop_yarn_RunSharedCacheCleanerTaskResponseProto_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_hadoop_yarn_RunSharedCacheCleanerTaskResponseProto_fieldAccessorTable;

  public static com.google.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static com.google.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n\031yarn_service_protos.proto\022\013hadoop.yarn" +
      "\032\016Security.proto\032\021yarn_protos.proto\"]\n%R" +
      "egisterApplicationMasterRequestProto\022\014\n\004" +
      "host\030\001 \001(\t\022\020\n\010rpc_port\030\002 \001(\005\022\024\n\014tracking" +
      "_url\030\003 \001(\t\"\250\003\n&RegisterApplicationMaster" +
      "ResponseProto\0225\n\021maximumCapability\030\001 \001(\013" +
      "2\032.hadoop.yarn.ResourceProto\022%\n\035client_t" +
      "o_am_token_master_key\030\002 \001(\014\022=\n\020applicati" +
      "on_ACLs\030\003 \003(\0132#.hadoop.yarn.ApplicationA" +
      "CLMapProto\022F\n!containers_from_previous_a",
      "ttempts\030\004 \003(\0132\033.hadoop.yarn.ContainerPro" +
      "to\022\r\n\005queue\030\005 \001(\t\022C\n nm_tokens_from_prev" +
      "ious_attempts\030\006 \003(\0132\031.hadoop.yarn.NMToke" +
      "nProto\022E\n\030scheduler_resource_types\030\007 \003(\016" +
      "2#.hadoop.yarn.SchedulerResourceTypes\"\234\001" +
      "\n#FinishApplicationMasterRequestProto\022\023\n" +
      "\013diagnostics\030\001 \001(\t\022\024\n\014tracking_url\030\002 \001(\t" +
      "\022J\n\030final_application_status\030\003 \001(\0162(.had" +
      "oop.yarn.FinalApplicationStatusProto\"E\n$" +
      "FinishApplicationMasterResponseProto\022\035\n\016",
      "isUnregistered\030\001 \001(\010:\005false\"\331\001\n\033UpdateCo" +
      "ntainerRequestProto\022\031\n\021container_version" +
      "\030\001 \002(\005\0223\n\014container_id\030\002 \002(\0132\035.hadoop.ya" +
      "rn.ContainerIdProto\022:\n\013update_type\030\003 \002(\016" +
      "2%.hadoop.yarn.ContainerUpdateTypeProto\022" +
      ".\n\ncapability\030\004 \001(\0132\032.hadoop.yarn.Resour" +
      "ceProto\"m\n\031UpdateContainerErrorProto\022\016\n\006" +
      "reason\030\001 \001(\t\022@\n\016update_request\030\002 \001(\0132(.h" +
      "adoop.yarn.UpdateContainerRequestProto\"\365" +
      "\002\n\024AllocateRequestProto\022.\n\003ask\030\001 \003(\0132!.h",
      "adoop.yarn.ResourceRequestProto\022.\n\007relea" +
      "se\030\002 \003(\0132\035.hadoop.yarn.ContainerIdProto\022" +
      "E\n\021blacklist_request\030\003 \001(\0132*.hadoop.yarn" +
      ".ResourceBlacklistRequestProto\022\023\n\013respon" +
      "se_id\030\004 \001(\005\022\020\n\010progress\030\005 \001(\002\022L\n\020increas" +
      "e_request\030\006 \003(\01322.hadoop.yarn.ContainerR" +
      "esourceIncreaseRequestProto\022A\n\017update_re" +
      "quests\030\007 \003(\0132(.hadoop.yarn.UpdateContain" +
      "erRequestProto\"b\n\014NMTokenProto\022(\n\006nodeId" +
      "\030\001 \001(\0132\030.hadoop.yarn.NodeIdProto\022(\n\005toke",
      "n\030\002 \001(\0132\031.hadoop.common.TokenProto\"\203\001\n\025U" +
      "pdatedContainerProto\022:\n\013update_type\030\001 \002(" +
      "\0162%.hadoop.yarn.ContainerUpdateTypeProto" +
      "\022.\n\tcontainer\030\002 \002(\0132\033.hadoop.yarn.Contai" +
      "nerProto\"\300\006\n\025AllocateResponseProto\0220\n\013a_" +
      "m_command\030\001 \001(\0162\033.hadoop.yarn.AMCommandP" +
      "roto\022\023\n\013response_id\030\002 \001(\005\0229\n\024allocated_c" +
      "ontainers\030\003 \003(\0132\033.hadoop.yarn.ContainerP" +
      "roto\022G\n\034completed_container_statuses\030\004 \003" +
      "(\0132!.hadoop.yarn.ContainerStatusProto\022)\n",
      "\005limit\030\005 \001(\0132\032.hadoop.yarn.ResourceProto" +
      "\0223\n\rupdated_nodes\030\006 \003(\0132\034.hadoop.yarn.No" +
      "deReportProto\022\031\n\021num_cluster_nodes\030\007 \001(\005" +
      "\0224\n\007preempt\030\010 \001(\0132#.hadoop.yarn.Preempti" +
      "onMessageProto\022,\n\tnm_tokens\030\t \003(\0132\031.hado" +
      "op.yarn.NMTokenProto\022I\n\024increased_contai" +
      "ners\030\n \003(\0132+.hadoop.yarn.ContainerResour" +
      "ceIncreaseProto\022I\n\024decreased_containers\030" +
      "\013 \003(\0132+.hadoop.yarn.ContainerResourceDec" +
      "reaseProto\022.\n\013am_rm_token\030\014 \001(\0132\031.hadoop",
      ".common.TokenProto\0228\n\024application_priori" +
      "ty\030\r \001(\0132\032.hadoop.yarn.PriorityProto\022=\n\r" +
      "update_errors\030\017 \003(\0132&.hadoop.yarn.Update" +
      "ContainerErrorProto\022>\n\022updated_container" +
      "s\030\020 \003(\0132\".hadoop.yarn.UpdatedContainerPr" +
      "oto\"\037\n\035GetNewApplicationRequestProto\"\220\001\n" +
      "\036GetNewApplicationResponseProto\0227\n\016appli" +
      "cation_id\030\001 \001(\0132\037.hadoop.yarn.Applicatio" +
      "nIdProto\0225\n\021maximumCapability\030\002 \001(\0132\032.ha" +
      "doop.yarn.ResourceProto\"[\n GetApplicatio",
      "nReportRequestProto\0227\n\016application_id\030\001 " +
      "\001(\0132\037.hadoop.yarn.ApplicationIdProto\"d\n!" +
      "GetApplicationReportResponseProto\022?\n\022app" +
      "lication_report\030\001 \001(\0132#.hadoop.yarn.Appl" +
      "icationReportProto\"w\n\035SubmitApplicationR" +
      "equestProto\022V\n\036application_submission_co" +
      "ntext\030\001 \001(\0132..hadoop.yarn.ApplicationSub" +
      "missionContextProto\" \n\036SubmitApplication" +
      "ResponseProto\"l\n\"FailApplicationAttemptR" +
      "equestProto\022F\n\026application_attempt_id\030\001 ",
      "\001(\0132&.hadoop.yarn.ApplicationAttemptIdPr" +
      "oto\"%\n#FailApplicationAttemptResponsePro" +
      "to\"k\n\033KillApplicationRequestProto\0227\n\016app" +
      "lication_id\030\001 \001(\0132\037.hadoop.yarn.Applicat" +
      "ionIdProto\022\023\n\013diagnostics\030\002 \001(\t\"@\n\034KillA" +
      "pplicationResponseProto\022 \n\021is_kill_compl" +
      "eted\030\001 \001(\010:\005false\"\037\n\035GetClusterMetricsRe" +
      "questProto\"_\n\036GetClusterMetricsResponseP" +
      "roto\022=\n\017cluster_metrics\030\001 \001(\0132$.hadoop.y" +
      "arn.YarnClusterMetricsProto\"x\n\'MoveAppli",
      "cationAcrossQueuesRequestProto\0227\n\016applic" +
      "ation_id\030\001 \002(\0132\037.hadoop.yarn.Application" +
      "IdProto\022\024\n\014target_queue\030\002 \002(\t\"*\n(MoveApp" +
      "licationAcrossQueuesResponseProto\"\325\002\n\033Ge" +
      "tApplicationsRequestProto\022\031\n\021application" +
      "_types\030\001 \003(\t\022B\n\022application_states\030\002 \003(\016" +
      "2&.hadoop.yarn.YarnApplicationStateProto" +
      "\022\r\n\005users\030\003 \003(\t\022\016\n\006queues\030\004 \003(\t\022\r\n\005limit" +
      "\030\005 \001(\003\022\023\n\013start_begin\030\006 \001(\003\022\021\n\tstart_end" +
      "\030\007 \001(\003\022\024\n\014finish_begin\030\010 \001(\003\022\022\n\nfinish_e",
      "nd\030\t \001(\003\022\027\n\017applicationTags\030\n \003(\t\022>\n\005sco" +
      "pe\030\013 \001(\0162*.hadoop.yarn.ApplicationsReque" +
      "stScopeProto:\003ALL\"Y\n\034GetApplicationsResp" +
      "onseProto\0229\n\014applications\030\001 \003(\0132#.hadoop" +
      ".yarn.ApplicationReportProto\"N\n\033GetClust" +
      "erNodesRequestProto\022/\n\nnodeStates\030\001 \003(\0162" +
      "\033.hadoop.yarn.NodeStateProto\"Q\n\034GetClust" +
      "erNodesResponseProto\0221\n\013nodeReports\030\001 \003(" +
      "\0132\034.hadoop.yarn.NodeReportProto\"y\n\030GetQu" +
      "eueInfoRequestProto\022\021\n\tqueueName\030\001 \001(\t\022\033",
      "\n\023includeApplications\030\002 \001(\010\022\032\n\022includeCh" +
      "ildQueues\030\003 \001(\010\022\021\n\trecursive\030\004 \001(\010\"K\n\031Ge" +
      "tQueueInfoResponseProto\022.\n\tqueueInfo\030\001 \001" +
      "(\0132\033.hadoop.yarn.QueueInfoProto\"\"\n GetQu" +
      "eueUserAclsInfoRequestProto\"^\n!GetQueueU" +
      "serAclsInfoResponseProto\0229\n\rqueueUserAcl" +
      "s\030\001 \003(\0132\".hadoop.yarn.QueueUserACLInfoPr" +
      "oto\"\036\n\034GetNodesToLabelsRequestProto\"W\n\035G" +
      "etNodesToLabelsResponseProto\0226\n\014nodeToLa" +
      "bels\030\001 \003(\0132 .hadoop.yarn.NodeIdToLabelsP",
      "roto\"2\n\034GetLabelsToNodesRequestProto\022\022\n\n" +
      "nodeLabels\030\001 \003(\t\"Y\n\035GetLabelsToNodesResp" +
      "onseProto\0228\n\rlabelsToNodes\030\001 \003(\0132!.hadoo" +
      "p.yarn.LabelsToNodeIdsProto\"\"\n GetCluste" +
      "rNodeLabelsRequestProto\"r\n!GetClusterNod" +
      "eLabelsResponseProto\022\034\n\024deprecatedNodeLa" +
      "bels\030\001 \003(\t\022/\n\nnodeLabels\030\002 \003(\0132\033.hadoop." +
      "yarn.NodeLabelProto\"\230\001\n%UpdateApplicatio" +
      "nPriorityRequestProto\0226\n\rapplicationId\030\001" +
      " \002(\0132\037.hadoop.yarn.ApplicationIdProto\0227\n",
      "\023applicationPriority\030\002 \002(\0132\032.hadoop.yarn" +
      ".PriorityProto\"a\n&UpdateApplicationPrior" +
      "ityResponseProto\0227\n\023applicationPriority\030" +
      "\001 \001(\0132\032.hadoop.yarn.PriorityProto\"\215\001\n\033Si" +
      "gnalContainerRequestProto\0223\n\014container_i" +
      "d\030\001 \002(\0132\035.hadoop.yarn.ContainerIdProto\0229" +
      "\n\007command\030\002 \002(\0162(.hadoop.yarn.SignalCont" +
      "ainerCommandProto\"\036\n\034SignalContainerResp" +
      "onseProto\"\234\001\n\032StartContainerRequestProto" +
      "\022J\n\030container_launch_context\030\001 \001(\0132(.had",
      "oop.yarn.ContainerLaunchContextProto\0222\n\017" +
      "container_token\030\002 \001(\0132\031.hadoop.common.To" +
      "kenProto\"[\n\033StartContainerResponseProto\022" +
      "<\n\022services_meta_data\030\001 \003(\0132 .hadoop.yar" +
      "n.StringBytesMapProto\"P\n\031StopContainerRe" +
      "questProto\0223\n\014container_id\030\001 \001(\0132\035.hadoo" +
      "p.yarn.ContainerIdProto\"\034\n\032StopContainer" +
      "ResponseProto\"g\n\033StartContainersRequestP" +
      "roto\022H\n\027start_container_request\030\001 \003(\0132\'." +
      "hadoop.yarn.StartContainerRequestProto\"\213",
      "\001\n\032ContainerExceptionMapProto\0223\n\014contain" +
      "er_id\030\001 \001(\0132\035.hadoop.yarn.ContainerIdPro" +
      "to\0228\n\texception\030\002 \001(\0132%.hadoop.yarn.Seri" +
      "alizedExceptionProto\"\331\001\n\034StartContainers" +
      "ResponseProto\022<\n\022services_meta_data\030\001 \003(" +
      "\0132 .hadoop.yarn.StringBytesMapProto\0229\n\022s" +
      "ucceeded_requests\030\002 \003(\0132\035.hadoop.yarn.Co" +
      "ntainerIdProto\022@\n\017failed_requests\030\003 \003(\0132" +
      "\'.hadoop.yarn.ContainerExceptionMapProto" +
      "\"Q\n\032StopContainersRequestProto\0223\n\014contai",
      "ner_id\030\001 \003(\0132\035.hadoop.yarn.ContainerIdPr" +
      "oto\"\232\001\n\033StopContainersResponseProto\0229\n\022s" +
      "ucceeded_requests\030\001 \003(\0132\035.hadoop.yarn.Co" +
      "ntainerIdProto\022@\n\017failed_requests\030\002 \003(\0132" +
      "\'.hadoop.yarn.ContainerExceptionMapProto" +
      "\"W\n GetContainerStatusesRequestProto\0223\n\014" +
      "container_id\030\001 \003(\0132\035.hadoop.yarn.Contain" +
      "erIdProto\"\230\001\n!GetContainerStatusesRespon" +
      "seProto\0221\n\006status\030\001 \003(\0132!.hadoop.yarn.Co" +
      "ntainerStatusProto\022@\n\017failed_requests\030\002 ",
      "\003(\0132\'.hadoop.yarn.ContainerExceptionMapP" +
      "roto\"`\n&IncreaseContainersResourceReques" +
      "tProto\0226\n\023increase_containers\030\001 \003(\0132\031.ha" +
      "doop.common.TokenProto\"\246\001\n\'IncreaseConta" +
      "inersResourceResponseProto\0229\n\022succeeded_" +
      "requests\030\001 \003(\0132\035.hadoop.yarn.ContainerId" +
      "Proto\022@\n\017failed_requests\030\002 \003(\0132\'.hadoop." +
      "yarn.ContainerExceptionMapProto\"q\n\'GetAp" +
      "plicationAttemptReportRequestProto\022F\n\026ap" +
      "plication_attempt_id\030\001 \001(\0132&.hadoop.yarn",
      ".ApplicationAttemptIdProto\"z\n(GetApplica" +
      "tionAttemptReportResponseProto\022N\n\032applic" +
      "ation_attempt_report\030\001 \001(\0132*.hadoop.yarn" +
      ".ApplicationAttemptReportProto\"]\n\"GetApp" +
      "licationAttemptsRequestProto\0227\n\016applicat" +
      "ion_id\030\001 \001(\0132\037.hadoop.yarn.ApplicationId" +
      "Proto\"o\n#GetApplicationAttemptsResponseP" +
      "roto\022H\n\024application_attempts\030\001 \003(\0132*.had" +
      "oop.yarn.ApplicationAttemptReportProto\"U" +
      "\n\036GetContainerReportRequestProto\0223\n\014cont",
      "ainer_id\030\001 \001(\0132\035.hadoop.yarn.ContainerId" +
      "Proto\"^\n\037GetContainerReportResponseProto" +
      "\022;\n\020container_report\030\001 \001(\0132!.hadoop.yarn" +
      ".ContainerReportProto\"c\n\031GetContainersRe" +
      "questProto\022F\n\026application_attempt_id\030\001 \001" +
      "(\0132&.hadoop.yarn.ApplicationAttemptIdPro" +
      "to\"S\n\032GetContainersResponseProto\0225\n\ncont" +
      "ainers\030\001 \003(\0132!.hadoop.yarn.ContainerRepo" +
      "rtProto\"q\n\"UseSharedCacheResourceRequest" +
      "Proto\0226\n\rapplicationId\030\001 \001(\0132\037.hadoop.ya",
      "rn.ApplicationIdProto\022\023\n\013resourceKey\030\002 \001" +
      "(\t\"3\n#UseSharedCacheResourceResponseProt" +
      "o\022\014\n\004path\030\001 \001(\t\"u\n&ReleaseSharedCacheRes" +
      "ourceRequestProto\0226\n\rapplicationId\030\001 \001(\013" +
      "2\037.hadoop.yarn.ApplicationIdProto\022\023\n\013res" +
      "ourceKey\030\002 \001(\t\")\n\'ReleaseSharedCacheReso" +
      "urceResponseProto\"\037\n\035GetNewReservationRe" +
      "questProto\"Y\n\036GetNewReservationResponseP" +
      "roto\0227\n\016reservation_id\030\001 \001(\0132\037.hadoop.ya" +
      "rn.ReservationIdProto\"\264\001\n!ReservationSub",
      "missionRequestProto\022\r\n\005queue\030\001 \001(\t\022G\n\026re" +
      "servation_definition\030\002 \001(\0132\'.hadoop.yarn" +
      ".ReservationDefinitionProto\0227\n\016reservati" +
      "on_id\030\003 \001(\0132\037.hadoop.yarn.ReservationIdP" +
      "roto\"$\n\"ReservationSubmissionResponsePro" +
      "to\"\241\001\n\035ReservationUpdateRequestProto\022G\n\026" +
      "reservation_definition\030\001 \001(\0132\'.hadoop.ya" +
      "rn.ReservationDefinitionProto\0227\n\016reserva" +
      "tion_id\030\002 \001(\0132\037.hadoop.yarn.ReservationI" +
      "dProto\" \n\036ReservationUpdateResponseProto",
      "\"X\n\035ReservationDeleteRequestProto\0227\n\016res" +
      "ervation_id\030\001 \001(\0132\037.hadoop.yarn.Reservat" +
      "ionIdProto\" \n\036ReservationDeleteResponseP" +
      "roto\"\220\001\n\033ReservationListRequestProto\022\r\n\005" +
      "queue\030\001 \001(\t\022\026\n\016reservation_id\030\003 \001(\t\022\022\n\ns" +
      "tart_time\030\004 \001(\003\022\020\n\010end_time\030\005 \001(\003\022$\n\034inc" +
      "lude_resource_allocations\030\006 \001(\010\"b\n\034Reser" +
      "vationListResponseProto\022B\n\014reservations\030" +
      "\001 \003(\0132,.hadoop.yarn.ReservationAllocatio" +
      "nStateProto\"\'\n%RunSharedCacheCleanerTask",
      "RequestProto\":\n&RunSharedCacheCleanerTas" +
      "kResponseProto\022\020\n\010accepted\030\001 \001(\010*H\n\030Cont" +
      "ainerUpdateTypeProto\022\025\n\021INCREASE_RESOURC" +
      "E\020\000\022\025\n\021DECREASE_RESOURCE\020\001*-\n\026SchedulerR" +
      "esourceTypes\022\n\n\006MEMORY\020\000\022\007\n\003CPU\020\001*?\n\035App" +
      "licationsRequestScopeProto\022\007\n\003ALL\020\000\022\014\n\010V" +
      "IEWABLE\020\001\022\007\n\003OWN\020\002B7\n\034org.apache.hadoop." +
      "yarn.protoB\021YarnServiceProtos\210\001\001\240\001\001"
    };
    com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
      new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
        public com.google.protobuf.ExtensionRegistry assignDescriptors(
            com.google.protobuf.Descriptors.FileDescriptor root) {
          descriptor = root;
          internal_static_hadoop_yarn_RegisterApplicationMasterRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(0);
          internal_static_hadoop_yarn_RegisterApplicationMasterRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_RegisterApplicationMasterRequestProto_descriptor,
              new java.lang.String[] { "Host", "RpcPort", "TrackingUrl", });
          internal_static_hadoop_yarn_RegisterApplicationMasterResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(1);
          internal_static_hadoop_yarn_RegisterApplicationMasterResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_RegisterApplicationMasterResponseProto_descriptor,
              new java.lang.String[] { "MaximumCapability", "ClientToAmTokenMasterKey", "ApplicationACLs", "ContainersFromPreviousAttempts", "Queue", "NmTokensFromPreviousAttempts", "SchedulerResourceTypes", });
          internal_static_hadoop_yarn_FinishApplicationMasterRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(2);
          internal_static_hadoop_yarn_FinishApplicationMasterRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_FinishApplicationMasterRequestProto_descriptor,
              new java.lang.String[] { "Diagnostics", "TrackingUrl", "FinalApplicationStatus", });
          internal_static_hadoop_yarn_FinishApplicationMasterResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(3);
          internal_static_hadoop_yarn_FinishApplicationMasterResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_FinishApplicationMasterResponseProto_descriptor,
              new java.lang.String[] { "IsUnregistered", });
          internal_static_hadoop_yarn_UpdateContainerRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(4);
          internal_static_hadoop_yarn_UpdateContainerRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_UpdateContainerRequestProto_descriptor,
              new java.lang.String[] { "ContainerVersion", "ContainerId", "UpdateType", "Capability", });
          internal_static_hadoop_yarn_UpdateContainerErrorProto_descriptor =
            getDescriptor().getMessageTypes().get(5);
          internal_static_hadoop_yarn_UpdateContainerErrorProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_UpdateContainerErrorProto_descriptor,
              new java.lang.String[] { "Reason", "UpdateRequest", });
          internal_static_hadoop_yarn_AllocateRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(6);
          internal_static_hadoop_yarn_AllocateRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_AllocateRequestProto_descriptor,
              new java.lang.String[] { "Ask", "Release", "BlacklistRequest", "ResponseId", "Progress", "IncreaseRequest", "UpdateRequests", });
          internal_static_hadoop_yarn_NMTokenProto_descriptor =
            getDescriptor().getMessageTypes().get(7);
          internal_static_hadoop_yarn_NMTokenProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_NMTokenProto_descriptor,
              new java.lang.String[] { "NodeId", "Token", });
          internal_static_hadoop_yarn_UpdatedContainerProto_descriptor =
            getDescriptor().getMessageTypes().get(8);
          internal_static_hadoop_yarn_UpdatedContainerProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_UpdatedContainerProto_descriptor,
              new java.lang.String[] { "UpdateType", "Container", });
          internal_static_hadoop_yarn_AllocateResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(9);
          internal_static_hadoop_yarn_AllocateResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_AllocateResponseProto_descriptor,
              new java.lang.String[] { "AMCommand", "ResponseId", "AllocatedContainers", "CompletedContainerStatuses", "Limit", "UpdatedNodes", "NumClusterNodes", "Preempt", "NmTokens", "IncreasedContainers", "DecreasedContainers", "AmRmToken", "ApplicationPriority", "UpdateErrors", "UpdatedContainers", });
          internal_static_hadoop_yarn_GetNewApplicationRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(10);
          internal_static_hadoop_yarn_GetNewApplicationRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetNewApplicationRequestProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_yarn_GetNewApplicationResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(11);
          internal_static_hadoop_yarn_GetNewApplicationResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetNewApplicationResponseProto_descriptor,
              new java.lang.String[] { "ApplicationId", "MaximumCapability", });
          internal_static_hadoop_yarn_GetApplicationReportRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(12);
          internal_static_hadoop_yarn_GetApplicationReportRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetApplicationReportRequestProto_descriptor,
              new java.lang.String[] { "ApplicationId", });
          internal_static_hadoop_yarn_GetApplicationReportResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(13);
          internal_static_hadoop_yarn_GetApplicationReportResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetApplicationReportResponseProto_descriptor,
              new java.lang.String[] { "ApplicationReport", });
          internal_static_hadoop_yarn_SubmitApplicationRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(14);
          internal_static_hadoop_yarn_SubmitApplicationRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_SubmitApplicationRequestProto_descriptor,
              new java.lang.String[] { "ApplicationSubmissionContext", });
          internal_static_hadoop_yarn_SubmitApplicationResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(15);
          internal_static_hadoop_yarn_SubmitApplicationResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_SubmitApplicationResponseProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_yarn_FailApplicationAttemptRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(16);
          internal_static_hadoop_yarn_FailApplicationAttemptRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_FailApplicationAttemptRequestProto_descriptor,
              new java.lang.String[] { "ApplicationAttemptId", });
          internal_static_hadoop_yarn_FailApplicationAttemptResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(17);
          internal_static_hadoop_yarn_FailApplicationAttemptResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_FailApplicationAttemptResponseProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_yarn_KillApplicationRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(18);
          internal_static_hadoop_yarn_KillApplicationRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_KillApplicationRequestProto_descriptor,
              new java.lang.String[] { "ApplicationId", "Diagnostics", });
          internal_static_hadoop_yarn_KillApplicationResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(19);
          internal_static_hadoop_yarn_KillApplicationResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_KillApplicationResponseProto_descriptor,
              new java.lang.String[] { "IsKillCompleted", });
          internal_static_hadoop_yarn_GetClusterMetricsRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(20);
          internal_static_hadoop_yarn_GetClusterMetricsRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetClusterMetricsRequestProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_yarn_GetClusterMetricsResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(21);
          internal_static_hadoop_yarn_GetClusterMetricsResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetClusterMetricsResponseProto_descriptor,
              new java.lang.String[] { "ClusterMetrics", });
          internal_static_hadoop_yarn_MoveApplicationAcrossQueuesRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(22);
          internal_static_hadoop_yarn_MoveApplicationAcrossQueuesRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_MoveApplicationAcrossQueuesRequestProto_descriptor,
              new java.lang.String[] { "ApplicationId", "TargetQueue", });
          internal_static_hadoop_yarn_MoveApplicationAcrossQueuesResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(23);
          internal_static_hadoop_yarn_MoveApplicationAcrossQueuesResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_MoveApplicationAcrossQueuesResponseProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_yarn_GetApplicationsRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(24);
          internal_static_hadoop_yarn_GetApplicationsRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetApplicationsRequestProto_descriptor,
              new java.lang.String[] { "ApplicationTypes", "ApplicationStates", "Users", "Queues", "Limit", "StartBegin", "StartEnd", "FinishBegin", "FinishEnd", "ApplicationTags", "Scope", });
          internal_static_hadoop_yarn_GetApplicationsResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(25);
          internal_static_hadoop_yarn_GetApplicationsResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetApplicationsResponseProto_descriptor,
              new java.lang.String[] { "Applications", });
          internal_static_hadoop_yarn_GetClusterNodesRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(26);
          internal_static_hadoop_yarn_GetClusterNodesRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetClusterNodesRequestProto_descriptor,
              new java.lang.String[] { "NodeStates", });
          internal_static_hadoop_yarn_GetClusterNodesResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(27);
          internal_static_hadoop_yarn_GetClusterNodesResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetClusterNodesResponseProto_descriptor,
              new java.lang.String[] { "NodeReports", });
          internal_static_hadoop_yarn_GetQueueInfoRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(28);
          internal_static_hadoop_yarn_GetQueueInfoRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetQueueInfoRequestProto_descriptor,
              new java.lang.String[] { "QueueName", "IncludeApplications", "IncludeChildQueues", "Recursive", });
          internal_static_hadoop_yarn_GetQueueInfoResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(29);
          internal_static_hadoop_yarn_GetQueueInfoResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetQueueInfoResponseProto_descriptor,
              new java.lang.String[] { "QueueInfo", });
          internal_static_hadoop_yarn_GetQueueUserAclsInfoRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(30);
          internal_static_hadoop_yarn_GetQueueUserAclsInfoRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetQueueUserAclsInfoRequestProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_yarn_GetQueueUserAclsInfoResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(31);
          internal_static_hadoop_yarn_GetQueueUserAclsInfoResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetQueueUserAclsInfoResponseProto_descriptor,
              new java.lang.String[] { "QueueUserAcls", });
          internal_static_hadoop_yarn_GetNodesToLabelsRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(32);
          internal_static_hadoop_yarn_GetNodesToLabelsRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetNodesToLabelsRequestProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_yarn_GetNodesToLabelsResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(33);
          internal_static_hadoop_yarn_GetNodesToLabelsResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetNodesToLabelsResponseProto_descriptor,
              new java.lang.String[] { "NodeToLabels", });
          internal_static_hadoop_yarn_GetLabelsToNodesRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(34);
          internal_static_hadoop_yarn_GetLabelsToNodesRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetLabelsToNodesRequestProto_descriptor,
              new java.lang.String[] { "NodeLabels", });
          internal_static_hadoop_yarn_GetLabelsToNodesResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(35);
          internal_static_hadoop_yarn_GetLabelsToNodesResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetLabelsToNodesResponseProto_descriptor,
              new java.lang.String[] { "LabelsToNodes", });
          internal_static_hadoop_yarn_GetClusterNodeLabelsRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(36);
          internal_static_hadoop_yarn_GetClusterNodeLabelsRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetClusterNodeLabelsRequestProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_yarn_GetClusterNodeLabelsResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(37);
          internal_static_hadoop_yarn_GetClusterNodeLabelsResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetClusterNodeLabelsResponseProto_descriptor,
              new java.lang.String[] { "DeprecatedNodeLabels", "NodeLabels", });
          internal_static_hadoop_yarn_UpdateApplicationPriorityRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(38);
          internal_static_hadoop_yarn_UpdateApplicationPriorityRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_UpdateApplicationPriorityRequestProto_descriptor,
              new java.lang.String[] { "ApplicationId", "ApplicationPriority", });
          internal_static_hadoop_yarn_UpdateApplicationPriorityResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(39);
          internal_static_hadoop_yarn_UpdateApplicationPriorityResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_UpdateApplicationPriorityResponseProto_descriptor,
              new java.lang.String[] { "ApplicationPriority", });
          internal_static_hadoop_yarn_SignalContainerRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(40);
          internal_static_hadoop_yarn_SignalContainerRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_SignalContainerRequestProto_descriptor,
              new java.lang.String[] { "ContainerId", "Command", });
          internal_static_hadoop_yarn_SignalContainerResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(41);
          internal_static_hadoop_yarn_SignalContainerResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_SignalContainerResponseProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_yarn_StartContainerRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(42);
          internal_static_hadoop_yarn_StartContainerRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_StartContainerRequestProto_descriptor,
              new java.lang.String[] { "ContainerLaunchContext", "ContainerToken", });
          internal_static_hadoop_yarn_StartContainerResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(43);
          internal_static_hadoop_yarn_StartContainerResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_StartContainerResponseProto_descriptor,
              new java.lang.String[] { "ServicesMetaData", });
          internal_static_hadoop_yarn_StopContainerRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(44);
          internal_static_hadoop_yarn_StopContainerRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_StopContainerRequestProto_descriptor,
              new java.lang.String[] { "ContainerId", });
          internal_static_hadoop_yarn_StopContainerResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(45);
          internal_static_hadoop_yarn_StopContainerResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_StopContainerResponseProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_yarn_StartContainersRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(46);
          internal_static_hadoop_yarn_StartContainersRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_StartContainersRequestProto_descriptor,
              new java.lang.String[] { "StartContainerRequest", });
          internal_static_hadoop_yarn_ContainerExceptionMapProto_descriptor =
            getDescriptor().getMessageTypes().get(47);
          internal_static_hadoop_yarn_ContainerExceptionMapProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_ContainerExceptionMapProto_descriptor,
              new java.lang.String[] { "ContainerId", "Exception", });
          internal_static_hadoop_yarn_StartContainersResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(48);
          internal_static_hadoop_yarn_StartContainersResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_StartContainersResponseProto_descriptor,
              new java.lang.String[] { "ServicesMetaData", "SucceededRequests", "FailedRequests", });
          internal_static_hadoop_yarn_StopContainersRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(49);
          internal_static_hadoop_yarn_StopContainersRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_StopContainersRequestProto_descriptor,
              new java.lang.String[] { "ContainerId", });
          internal_static_hadoop_yarn_StopContainersResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(50);
          internal_static_hadoop_yarn_StopContainersResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_StopContainersResponseProto_descriptor,
              new java.lang.String[] { "SucceededRequests", "FailedRequests", });
          internal_static_hadoop_yarn_GetContainerStatusesRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(51);
          internal_static_hadoop_yarn_GetContainerStatusesRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetContainerStatusesRequestProto_descriptor,
              new java.lang.String[] { "ContainerId", });
          internal_static_hadoop_yarn_GetContainerStatusesResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(52);
          internal_static_hadoop_yarn_GetContainerStatusesResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetContainerStatusesResponseProto_descriptor,
              new java.lang.String[] { "Status", "FailedRequests", });
          internal_static_hadoop_yarn_IncreaseContainersResourceRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(53);
          internal_static_hadoop_yarn_IncreaseContainersResourceRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_IncreaseContainersResourceRequestProto_descriptor,
              new java.lang.String[] { "IncreaseContainers", });
          internal_static_hadoop_yarn_IncreaseContainersResourceResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(54);
          internal_static_hadoop_yarn_IncreaseContainersResourceResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_IncreaseContainersResourceResponseProto_descriptor,
              new java.lang.String[] { "SucceededRequests", "FailedRequests", });
          internal_static_hadoop_yarn_GetApplicationAttemptReportRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(55);
          internal_static_hadoop_yarn_GetApplicationAttemptReportRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetApplicationAttemptReportRequestProto_descriptor,
              new java.lang.String[] { "ApplicationAttemptId", });
          internal_static_hadoop_yarn_GetApplicationAttemptReportResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(56);
          internal_static_hadoop_yarn_GetApplicationAttemptReportResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetApplicationAttemptReportResponseProto_descriptor,
              new java.lang.String[] { "ApplicationAttemptReport", });
          internal_static_hadoop_yarn_GetApplicationAttemptsRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(57);
          internal_static_hadoop_yarn_GetApplicationAttemptsRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetApplicationAttemptsRequestProto_descriptor,
              new java.lang.String[] { "ApplicationId", });
          internal_static_hadoop_yarn_GetApplicationAttemptsResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(58);
          internal_static_hadoop_yarn_GetApplicationAttemptsResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetApplicationAttemptsResponseProto_descriptor,
              new java.lang.String[] { "ApplicationAttempts", });
          internal_static_hadoop_yarn_GetContainerReportRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(59);
          internal_static_hadoop_yarn_GetContainerReportRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetContainerReportRequestProto_descriptor,
              new java.lang.String[] { "ContainerId", });
          internal_static_hadoop_yarn_GetContainerReportResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(60);
          internal_static_hadoop_yarn_GetContainerReportResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetContainerReportResponseProto_descriptor,
              new java.lang.String[] { "ContainerReport", });
          internal_static_hadoop_yarn_GetContainersRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(61);
          internal_static_hadoop_yarn_GetContainersRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetContainersRequestProto_descriptor,
              new java.lang.String[] { "ApplicationAttemptId", });
          internal_static_hadoop_yarn_GetContainersResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(62);
          internal_static_hadoop_yarn_GetContainersResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetContainersResponseProto_descriptor,
              new java.lang.String[] { "Containers", });
          internal_static_hadoop_yarn_UseSharedCacheResourceRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(63);
          internal_static_hadoop_yarn_UseSharedCacheResourceRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_UseSharedCacheResourceRequestProto_descriptor,
              new java.lang.String[] { "ApplicationId", "ResourceKey", });
          internal_static_hadoop_yarn_UseSharedCacheResourceResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(64);
          internal_static_hadoop_yarn_UseSharedCacheResourceResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_UseSharedCacheResourceResponseProto_descriptor,
              new java.lang.String[] { "Path", });
          internal_static_hadoop_yarn_ReleaseSharedCacheResourceRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(65);
          internal_static_hadoop_yarn_ReleaseSharedCacheResourceRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_ReleaseSharedCacheResourceRequestProto_descriptor,
              new java.lang.String[] { "ApplicationId", "ResourceKey", });
          internal_static_hadoop_yarn_ReleaseSharedCacheResourceResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(66);
          internal_static_hadoop_yarn_ReleaseSharedCacheResourceResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_ReleaseSharedCacheResourceResponseProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_yarn_GetNewReservationRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(67);
          internal_static_hadoop_yarn_GetNewReservationRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetNewReservationRequestProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_yarn_GetNewReservationResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(68);
          internal_static_hadoop_yarn_GetNewReservationResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_GetNewReservationResponseProto_descriptor,
              new java.lang.String[] { "ReservationId", });
          internal_static_hadoop_yarn_ReservationSubmissionRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(69);
          internal_static_hadoop_yarn_ReservationSubmissionRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_ReservationSubmissionRequestProto_descriptor,
              new java.lang.String[] { "Queue", "ReservationDefinition", "ReservationId", });
          internal_static_hadoop_yarn_ReservationSubmissionResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(70);
          internal_static_hadoop_yarn_ReservationSubmissionResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_ReservationSubmissionResponseProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_yarn_ReservationUpdateRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(71);
          internal_static_hadoop_yarn_ReservationUpdateRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_ReservationUpdateRequestProto_descriptor,
              new java.lang.String[] { "ReservationDefinition", "ReservationId", });
          internal_static_hadoop_yarn_ReservationUpdateResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(72);
          internal_static_hadoop_yarn_ReservationUpdateResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_ReservationUpdateResponseProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_yarn_ReservationDeleteRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(73);
          internal_static_hadoop_yarn_ReservationDeleteRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_ReservationDeleteRequestProto_descriptor,
              new java.lang.String[] { "ReservationId", });
          internal_static_hadoop_yarn_ReservationDeleteResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(74);
          internal_static_hadoop_yarn_ReservationDeleteResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_ReservationDeleteResponseProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_yarn_ReservationListRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(75);
          internal_static_hadoop_yarn_ReservationListRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_ReservationListRequestProto_descriptor,
              new java.lang.String[] { "Queue", "ReservationId", "StartTime", "EndTime", "IncludeResourceAllocations", });
          internal_static_hadoop_yarn_ReservationListResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(76);
          internal_static_hadoop_yarn_ReservationListResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_ReservationListResponseProto_descriptor,
              new java.lang.String[] { "Reservations", });
          internal_static_hadoop_yarn_RunSharedCacheCleanerTaskRequestProto_descriptor =
            getDescriptor().getMessageTypes().get(77);
          internal_static_hadoop_yarn_RunSharedCacheCleanerTaskRequestProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_RunSharedCacheCleanerTaskRequestProto_descriptor,
              new java.lang.String[] { });
          internal_static_hadoop_yarn_RunSharedCacheCleanerTaskResponseProto_descriptor =
            getDescriptor().getMessageTypes().get(78);
          internal_static_hadoop_yarn_RunSharedCacheCleanerTaskResponseProto_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_hadoop_yarn_RunSharedCacheCleanerTaskResponseProto_descriptor,
              new java.lang.String[] { "Accepted", });
          return null;
        }
      };
    com.google.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new com.google.protobuf.Descriptors.FileDescriptor[] {
          org.apache.hadoop.security.proto.SecurityProtos.getDescriptor(),
          org.apache.hadoop.yarn.proto.YarnProtos.getDescriptor(),
        }, assigner);
  }

  // @@protoc_insertion_point(outer_class_scope)
}
